{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Dueling Deep Q Learning with Doom (+ double DQNs and Prioritized Experience Replay).ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYulVstwzoPZ",
        "colab_type": "text"
      },
      "source": [
        "# DDDQN  (Double Dueling Deep Q Learning with Prioritized Experience Replay)  DoomüïπÔ∏è\n",
        "In this notebook we'll implement an agent <b>that plays Doom by using a Dueling Double Deep Q learning architecture with Prioritized Experience Replay.</b> <br>\n",
        "\n",
        "Our agent playing Doom after 3 hours of training of **CPU**, remember that our agent needs about 2 days of **GPU** to have optimal score, we'll train from beginning to end the most important architectures (PPO with transfer):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/projects/doomdeathmatc.gif\" alt=\"Doom Deathmatch\"/>\n",
        "\n",
        "But we can see that our agent **understand that he needs to kill enemies before being able to move forward (if he moves forward without killing ennemies he will be killed before getting the vest)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDDBHa97zoPe",
        "colab_type": "text"
      },
      "source": [
        "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
        "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
        "<br>\n",
        "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
        "<br><br>\n",
        "    \n",
        "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
        "<br>\n",
        "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
        "<br>\n",
        "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
        "<br><br>\n",
        "</p> \n",
        "\n",
        "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
        "\n",
        "\n",
        "## Any questions üë®‚Äçüíª\n",
        "<p> If you have any questions, feel free to ask me: </p>\n",
        "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
        "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
        "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
        "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
        "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
        "    \n",
        "## How to help  üôå\n",
        "3 ways:\n",
        "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
        "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
        "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
        "<br>\n",
        "\n",
        "## Important note ü§î\n",
        "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
        "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
        "<br>\n",
        "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
        "\n",
        "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0NKfF8rzoPg",
        "colab_type": "text"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving on the notebook **you need to understand**:\n",
        "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
        "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
        "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
        "- Improvments in Deep Q-learning [Article]()\n",
        "- You can follow this notebook using my [video tutorial](https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHCw9ZS5zoPk",
        "colab_type": "code",
        "outputId": "94a36be7-a69f-489d-a903-6cc24e446cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SdvkupazoPv",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Import the libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlxwpgSv1mRK",
        "colab_type": "code",
        "outputId": "f29df4d4-f3fc-45b8-97df-2fe745417f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "# Install deps from \n",
        "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
        "\n",
        "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
        "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
        "libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# Boost libraries\n",
        "apt-get install libboost-all-dev\n",
        "\n",
        "# Lua binding dependencies\n",
        "apt-get install liblua5.1-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "libjpeg-dev set to manually installed.\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.5).\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "libbz2-dev set to manually installed.\n",
            "tar is already the newest version (1.29b-2ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file freepats gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-ibus-1.0\n",
            "  gir1.2-pango-1.0 intltool-debian libarchive-cpio-perl libarchive-zip-perl\n",
            "  libatk1.0-dev libaudio2 libcairo-script-interpreter2 libcairo2-dev\n",
            "  libcapnp-0.6.1 libdbus-1-dev libfile-stripnondeterminism-perl libfluidsynth1\n",
            "  libgdk-pixbuf2.0-dev libibus-1.0-5 libibus-1.0-dev libmagic-mgc libmagic1\n",
            "  libmail-sendmail-perl libmirclient-dev libmirclient9 libmircommon-dev\n",
            "  libmircommon7 libmircookie-dev libmircookie2 libmircore-dev libmircore1\n",
            "  libmirprotobuf3 libpango1.0-dev libpangoxft-1.0-0 libpixman-1-dev\n",
            "  libprotobuf-dev libprotobuf-lite10 libpulse-dev libpulse-mainloop-glib0\n",
            "  libsigsegv2 libsndio-dev libsys-hostname-long-perl libtimedate-perl libtool\n",
            "  libudev-dev libwildmidi-config libwildmidi2 libxcb-shm0-dev\n",
            "  libxcomposite-dev libxcursor-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxml2-utils libxrandr-dev libxv-dev m4 po-debconf timidity-daemon\n",
            "  x11proto-composite-dev x11proto-randr-dev x11proto-xinerama-dev\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc dh-make dwz gettext-doc\n",
            "  libasprintf-dev libgettextpo-dev nas libcairo2-doc fluidr3mono-gm-soundfont\n",
            "  | timgm6mb-soundfont | fluid-soundfont-gm libgtk2.0-doc imagemagick\n",
            "  libpango1.0-doc libtool-doc gcj-jdk m4-doc libmail-box-perl\n",
            "  fluid-soundfont-gm fluid-soundfont-gs pmidi\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file freepats gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-ibus-1.0\n",
            "  gir1.2-pango-1.0 intltool-debian libarchive-cpio-perl libarchive-zip-perl\n",
            "  libatk1.0-dev libaudio2 libcairo-script-interpreter2 libcairo2-dev\n",
            "  libcapnp-0.6.1 libdbus-1-dev libfile-stripnondeterminism-perl\n",
            "  libfluidsynth-dev libfluidsynth1 libgdk-pixbuf2.0-dev libgme-dev\n",
            "  libgtk2.0-dev libibus-1.0-5 libibus-1.0-dev libmagic-mgc libmagic1\n",
            "  libmail-sendmail-perl libmirclient-dev libmirclient9 libmircommon-dev\n",
            "  libmircommon7 libmircookie-dev libmircookie2 libmircore-dev libmircore1\n",
            "  libmirprotobuf3 libopenal-dev libpango1.0-dev libpangoxft-1.0-0\n",
            "  libpixman-1-dev libprotobuf-dev libprotobuf-lite10 libpulse-dev\n",
            "  libpulse-mainloop-glib0 libsdl2-dev libsigsegv2 libsndio-dev\n",
            "  libsys-hostname-long-perl libtimedate-perl libtool libudev-dev\n",
            "  libwildmidi-config libwildmidi-dev libwildmidi2 libxcb-shm0-dev\n",
            "  libxcomposite-dev libxcursor-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxml2-utils libxrandr-dev libxv-dev m4 nasm po-debconf timidity\n",
            "  timidity-daemon x11proto-composite-dev x11proto-randr-dev\n",
            "  x11proto-xinerama-dev\n",
            "0 upgraded, 80 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 43.2 MB of archives.\n",
            "After this operation, 114 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.3 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.3 [68.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.3 [22.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext-base amd64 0.19.8.1-6ubuntu0.3 [113 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 autopoint all 0.19.8.1-6ubuntu0.3 [426 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-autoreconf all 17 [15.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libarchive-zip-perl all 1.60-1ubuntu0.1 [84.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-stripnondeterminism-perl all 0.040-1.1~build1 [13.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-strip-nondeterminism all 0.040-1.1~build1 [5,208 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext amd64 0.19.8.1-6ubuntu0.3 [1,293 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 intltool-debian all 0.35.0+20060710.4 [24.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 po-debconf all 1.0.20 [232 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 debhelper all 11.1.6ubuntu2 [902 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freepats all 20060219-1 [29.0 MB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-atk-1.0 amd64 2.28.1-1 [17.8 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-freedesktop amd64 1.56.1-1 [9,080 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gdkpixbuf-2.0 amd64 2.36.11-2 [7,748 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangoxft-1.0-0 amd64 1.40.14-1ubuntu0.1 [15.0 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-pango-1.0 amd64 1.40.14-1ubuntu0.1 [21.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gtk-2.0 amd64 2.24.32-1ubuntu1 [172 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-5 amd64 1.5.17-3ubuntu5.2 [134 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-ibus-1.0 amd64 1.5.17-3ubuntu5.2 [66.5 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libarchive-cpio-perl all 0.10-1 [9,644 B]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk1.0-dev amd64 2.28.1-1 [79.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libaudio2 amd64 1.9.4-6 [50.3 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo-script-interpreter2 amd64 1.15.10-2ubuntu0.1 [53.5 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-dev amd64 0.34.0-2 [244 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-shm0-dev amd64 1.13-2~ubuntu18.04 [6,684 B]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo2-dev amd64 1.15.10-2ubuntu0.1 [626 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcapnp-0.6.1 amd64 0.6.1-1ubuntu1 [658 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdbus-1-dev amd64 1.12.2-1ubuntu1.1 [165 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfluidsynth1 amd64 1.1.9-1 [137 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfluidsynth-dev amd64 1.1.9-1 [19.7 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgdk-pixbuf2.0-dev amd64 2.36.11-2 [46.8 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgme-dev amd64 0.6.2-1 [5,796 B]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpango1.0-dev amd64 1.40.14-1ubuntu0.1 [288 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xinerama-dev all 2018.4-4 [2,628 B]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxinerama-dev amd64 2:1.1.3-1 [8,404 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-randr-dev all 2018.4-4 [2,620 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrandr-dev amd64 2:1.5.1-1 [24.0 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcursor-dev amd64 1:1.1.15-1 [26.5 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-composite-dev all 1:2018.4-4 [2,620 B]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcomposite-dev amd64 1:0.4.4-2 [9,136 B]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxml2-utils amd64 2.9.4+dfsg1-6.1ubuntu1.2 [35.8 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-dev amd64 2.24.32-1ubuntu1 [2,652 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-dev amd64 1.5.17-3ubuntu5.2 [145 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsys-hostname-long-perl all 1.5-1 [11.7 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmail-sendmail-perl all 0.80-1 [22.6 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore1 amd64 0.31.1-0ubuntu1 [26.5 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon7 amd64 0.31.1-0ubuntu1 [73.9 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-lite10 amd64 3.0.0-9.1ubuntu1 [97.7 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirprotobuf3 amd64 0.31.1-0ubuntu1 [127 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient9 amd64 0.31.1-0ubuntu1 [199 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore-dev amd64 0.31.1-0ubuntu1 [21.7 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-dev amd64 3.0.0-9.1ubuntu1 [959 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxkbcommon-dev amd64 0.8.2-1~ubuntu18.04.1 [150 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon-dev amd64 0.31.1-0ubuntu1 [13.9 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie2 amd64 0.31.1-0ubuntu1 [19.7 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie-dev amd64 0.31.1-0ubuntu1 [4,392 B]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient-dev amd64 0.31.1-0ubuntu1 [47.8 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenal-dev amd64 1:1.18.2-2 [20.9 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.4 [22.1 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.4 [81.5 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsndio-dev amd64 1.1.0-3 [13.3 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev-dev amd64 237-3ubuntu10.33 [19.1 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxv-dev amd64 2:1.0.11-1 [32.5 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsdl2-dev amd64 2.0.8+dfsg1-1ubuntu1.18.04.4 [683 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libwildmidi-config all 0.4.2-1 [7,212 B]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libwildmidi2 amd64 0.4.2-1 [55.8 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libwildmidi-dev amd64 0.4.2-1 [86.4 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu bionic/universe amd64 nasm amd64 2.13.02-0.1 [359 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu bionic/universe amd64 timidity amd64 2.13.2-41 [585 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu bionic/universe amd64 timidity-daemon all 2.13.2-41 [5,984 B]\n",
            "Fetched 43.2 MB in 6s (7,559 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 134485 files and directories currently installed.)\r\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.3_amd64.deb ...\r\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.3) ...\r\n",
            "Selecting previously unselected package libmagic1:amd64.\r\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.3_amd64.deb ...\r\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\r\n",
            "Selecting previously unselected package file.\r\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.3_amd64.deb ...\r\n",
            "Unpacking file (1:5.32-2ubuntu0.3) ...\r\n",
            "Selecting previously unselected package gettext-base.\r\n",
            "Preparing to unpack .../03-gettext-base_0.19.8.1-6ubuntu0.3_amd64.deb ...\r\n",
            "Unpacking gettext-base (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Selecting previously unselected package libsigsegv2:amd64.\r\n",
            "Preparing to unpack .../04-libsigsegv2_2.12-1_amd64.deb ...\r\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\r\n",
            "Selecting previously unselected package m4.\r\n",
            "Preparing to unpack .../05-m4_1.4.18-1_amd64.deb ...\r\n",
            "Unpacking m4 (1.4.18-1) ...\r\n",
            "Selecting previously unselected package autoconf.\r\n",
            "Preparing to unpack .../06-autoconf_2.69-11_all.deb ...\r\n",
            "Unpacking autoconf (2.69-11) ...\r\n",
            "Selecting previously unselected package autotools-dev.\r\n",
            "Preparing to unpack .../07-autotools-dev_20180224.1_all.deb ...\r\n",
            "Unpacking autotools-dev (20180224.1) ...\r\n",
            "Selecting previously unselected package automake.\r\n",
            "Preparing to unpack .../08-automake_1%3a1.15.1-3ubuntu2_all.deb ...\r\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\r\n",
            "Selecting previously unselected package autopoint.\r\n",
            "Preparing to unpack .../09-autopoint_0.19.8.1-6ubuntu0.3_all.deb ...\r\n",
            "Unpacking autopoint (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Selecting previously unselected package libtool.\r\n",
            "Preparing to unpack .../10-libtool_2.4.6-2_all.deb ...\r\n",
            "Unpacking libtool (2.4.6-2) ...\r\n",
            "Selecting previously unselected package dh-autoreconf.\r\n",
            "Preparing to unpack .../11-dh-autoreconf_17_all.deb ...\r\n",
            "Unpacking dh-autoreconf (17) ...\r\n",
            "Selecting previously unselected package libarchive-zip-perl.\r\n",
            "Preparing to unpack .../12-libarchive-zip-perl_1.60-1ubuntu0.1_all.deb ...\r\n",
            "Unpacking libarchive-zip-perl (1.60-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libfile-stripnondeterminism-perl.\r\n",
            "Preparing to unpack .../13-libfile-stripnondeterminism-perl_0.040-1.1~build1_all.deb ...\r\n",
            "Unpacking libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\r\n",
            "Selecting previously unselected package libtimedate-perl.\r\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\r\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\r\n",
            "Selecting previously unselected package dh-strip-nondeterminism.\r\n",
            "Preparing to unpack .../15-dh-strip-nondeterminism_0.040-1.1~build1_all.deb ...\r\n",
            "Unpacking dh-strip-nondeterminism (0.040-1.1~build1) ...\r\n",
            "Selecting previously unselected package gettext.\r\n",
            "Preparing to unpack .../16-gettext_0.19.8.1-6ubuntu0.3_amd64.deb ...\r\n",
            "Unpacking gettext (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Selecting previously unselected package intltool-debian.\r\n",
            "Preparing to unpack .../17-intltool-debian_0.35.0+20060710.4_all.deb ...\r\n",
            "Unpacking intltool-debian (0.35.0+20060710.4) ...\r\n",
            "Selecting previously unselected package po-debconf.\r\n",
            "Preparing to unpack .../18-po-debconf_1.0.20_all.deb ...\r\n",
            "Unpacking po-debconf (1.0.20) ...\r\n",
            "Selecting previously unselected package debhelper.\r\n",
            "Preparing to unpack .../19-debhelper_11.1.6ubuntu2_all.deb ...\r\n",
            "Unpacking debhelper (11.1.6ubuntu2) ...\r\n",
            "Selecting previously unselected package freepats.\r\n",
            "Preparing to unpack .../20-freepats_20060219-1_all.deb ...\r\n",
            "Unpacking freepats (20060219-1) ...\r\n",
            "Selecting previously unselected package gir1.2-atk-1.0:amd64.\r\n",
            "Preparing to unpack .../21-gir1.2-atk-1.0_2.28.1-1_amd64.deb ...\r\n",
            "Unpacking gir1.2-atk-1.0:amd64 (2.28.1-1) ...\r\n",
            "Selecting previously unselected package gir1.2-freedesktop:amd64.\r\n",
            "Preparing to unpack .../22-gir1.2-freedesktop_1.56.1-1_amd64.deb ...\r\n",
            "Unpacking gir1.2-freedesktop:amd64 (1.56.1-1) ...\r\n",
            "Selecting previously unselected package gir1.2-gdkpixbuf-2.0:amd64.\r\n",
            "Preparing to unpack .../23-gir1.2-gdkpixbuf-2.0_2.36.11-2_amd64.deb ...\r\n",
            "Unpacking gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\r\n",
            "Selecting previously unselected package libpangoxft-1.0-0:amd64.\r\n",
            "Preparing to unpack .../24-libpangoxft-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package gir1.2-pango-1.0:amd64.\r\n",
            "Preparing to unpack .../25-gir1.2-pango-1.0_1.40.14-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package gir1.2-gtk-2.0.\r\n",
            "Preparing to unpack .../26-gir1.2-gtk-2.0_2.24.32-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\r\n",
            "Preparing to unpack .../27-libibus-1.0-5_1.5.17-3ubuntu5.2_amd64.deb ...\r\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.17-3ubuntu5.2) ...\r\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\r\n",
            "Preparing to unpack .../28-gir1.2-ibus-1.0_1.5.17-3ubuntu5.2_amd64.deb ...\r\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu5.2) ...\r\n",
            "Selecting previously unselected package libarchive-cpio-perl.\r\n",
            "Preparing to unpack .../29-libarchive-cpio-perl_0.10-1_all.deb ...\r\n",
            "Unpacking libarchive-cpio-perl (0.10-1) ...\r\n",
            "Selecting previously unselected package libatk1.0-dev:amd64.\r\n",
            "Preparing to unpack .../30-libatk1.0-dev_2.28.1-1_amd64.deb ...\r\n",
            "Unpacking libatk1.0-dev:amd64 (2.28.1-1) ...\r\n",
            "Selecting previously unselected package libaudio2:amd64.\r\n",
            "Preparing to unpack .../31-libaudio2_1.9.4-6_amd64.deb ...\r\n",
            "Unpacking libaudio2:amd64 (1.9.4-6) ...\r\n",
            "Selecting previously unselected package libcairo-script-interpreter2:amd64.\r\n",
            "Preparing to unpack .../32-libcairo-script-interpreter2_1.15.10-2ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libpixman-1-dev:amd64.\r\n",
            "Preparing to unpack .../33-libpixman-1-dev_0.34.0-2_amd64.deb ...\r\n",
            "Unpacking libpixman-1-dev:amd64 (0.34.0-2) ...\r\n",
            "Selecting previously unselected package libxcb-shm0-dev:amd64.\r\n",
            "Preparing to unpack .../34-libxcb-shm0-dev_1.13-2~ubuntu18.04_amd64.deb ...\r\n",
            "Unpacking libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\r\n",
            "Selecting previously unselected package libcairo2-dev:amd64.\r\n",
            "Preparing to unpack .../35-libcairo2-dev_1.15.10-2ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libcapnp-0.6.1:amd64.\r\n",
            "Preparing to unpack .../36-libcapnp-0.6.1_0.6.1-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\r\n",
            "Preparing to unpack .../37-libdbus-1-dev_1.12.2-1ubuntu1.1_amd64.deb ...\r\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.2-1ubuntu1.1) ...\r\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\r\n",
            "Preparing to unpack .../38-libfluidsynth1_1.1.9-1_amd64.deb ...\r\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\r\n",
            "Selecting previously unselected package libfluidsynth-dev:amd64.\r\n",
            "Preparing to unpack .../39-libfluidsynth-dev_1.1.9-1_amd64.deb ...\r\n",
            "Unpacking libfluidsynth-dev:amd64 (1.1.9-1) ...\r\n",
            "Selecting previously unselected package libgdk-pixbuf2.0-dev.\r\n",
            "Preparing to unpack .../40-libgdk-pixbuf2.0-dev_2.36.11-2_amd64.deb ...\r\n",
            "Unpacking libgdk-pixbuf2.0-dev (2.36.11-2) ...\r\n",
            "Selecting previously unselected package libgme-dev:amd64.\r\n",
            "Preparing to unpack .../41-libgme-dev_0.6.2-1_amd64.deb ...\r\n",
            "Unpacking libgme-dev:amd64 (0.6.2-1) ...\r\n",
            "Selecting previously unselected package libpango1.0-dev.\r\n",
            "Preparing to unpack .../42-libpango1.0-dev_1.40.14-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libpango1.0-dev (1.40.14-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package x11proto-xinerama-dev.\r\n",
            "Preparing to unpack .../43-x11proto-xinerama-dev_2018.4-4_all.deb ...\r\n",
            "Unpacking x11proto-xinerama-dev (2018.4-4) ...\r\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\r\n",
            "Preparing to unpack .../44-libxinerama-dev_2%3a1.1.3-1_amd64.deb ...\r\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.3-1) ...\r\n",
            "Selecting previously unselected package x11proto-randr-dev.\r\n",
            "Preparing to unpack .../45-x11proto-randr-dev_2018.4-4_all.deb ...\r\n",
            "Unpacking x11proto-randr-dev (2018.4-4) ...\r\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\r\n",
            "Preparing to unpack .../46-libxrandr-dev_2%3a1.5.1-1_amd64.deb ...\r\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.1-1) ...\r\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\r\n",
            "Preparing to unpack .../47-libxcursor-dev_1%3a1.1.15-1_amd64.deb ...\r\n",
            "Unpacking libxcursor-dev:amd64 (1:1.1.15-1) ...\r\n",
            "Selecting previously unselected package x11proto-composite-dev.\r\n",
            "Preparing to unpack .../48-x11proto-composite-dev_1%3a2018.4-4_all.deb ...\r\n",
            "Unpacking x11proto-composite-dev (1:2018.4-4) ...\r\n",
            "Selecting previously unselected package libxcomposite-dev:amd64.\r\n",
            "Preparing to unpack .../49-libxcomposite-dev_1%3a0.4.4-2_amd64.deb ...\r\n",
            "Unpacking libxcomposite-dev:amd64 (1:0.4.4-2) ...\r\n",
            "Selecting previously unselected package libxml2-utils.\r\n",
            "Preparing to unpack .../50-libxml2-utils_2.9.4+dfsg1-6.1ubuntu1.2_amd64.deb ...\r\n",
            "Unpacking libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.2) ...\r\n",
            "Selecting previously unselected package libgtk2.0-dev.\r\n",
            "Preparing to unpack .../51-libgtk2.0-dev_2.24.32-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libgtk2.0-dev (2.24.32-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\r\n",
            "Preparing to unpack .../52-libibus-1.0-dev_1.5.17-3ubuntu5.2_amd64.deb ...\r\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.17-3ubuntu5.2) ...\r\n",
            "Selecting previously unselected package libsys-hostname-long-perl.\r\n",
            "Preparing to unpack .../53-libsys-hostname-long-perl_1.5-1_all.deb ...\r\n",
            "Unpacking libsys-hostname-long-perl (1.5-1) ...\r\n",
            "Selecting previously unselected package libmail-sendmail-perl.\r\n",
            "Preparing to unpack .../54-libmail-sendmail-perl_0.80-1_all.deb ...\r\n",
            "Unpacking libmail-sendmail-perl (0.80-1) ...\r\n",
            "Selecting previously unselected package libmircore1:amd64.\r\n",
            "Preparing to unpack .../55-libmircore1_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircore1:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircommon7:amd64.\r\n",
            "Preparing to unpack .../56-libmircommon7_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircommon7:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libprotobuf-lite10:amd64.\r\n",
            "Preparing to unpack .../57-libprotobuf-lite10_3.0.0-9.1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Selecting previously unselected package libmirprotobuf3:amd64.\r\n",
            "Preparing to unpack .../58-libmirprotobuf3_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmirclient9:amd64.\r\n",
            "Preparing to unpack .../59-libmirclient9_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmirclient9:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircore-dev:amd64.\r\n",
            "Preparing to unpack .../60-libmircore-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libprotobuf-dev:amd64.\r\n",
            "Preparing to unpack .../61-libprotobuf-dev_3.0.0-9.1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Selecting previously unselected package libxkbcommon-dev:amd64.\r\n",
            "Preparing to unpack .../62-libxkbcommon-dev_0.8.2-1~ubuntu18.04.1_amd64.deb ...\r\n",
            "Unpacking libxkbcommon-dev:amd64 (0.8.2-1~ubuntu18.04.1) ...\r\n",
            "Selecting previously unselected package libmircommon-dev:amd64.\r\n",
            "Preparing to unpack .../63-libmircommon-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircookie2:amd64.\r\n",
            "Preparing to unpack .../64-libmircookie2_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircookie2:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircookie-dev:amd64.\r\n",
            "Preparing to unpack .../65-libmircookie-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmirclient-dev:amd64.\r\n",
            "Preparing to unpack .../66-libmirclient-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libopenal-dev:amd64.\r\n",
            "Preparing to unpack .../67-libopenal-dev_1%3a1.18.2-2_amd64.deb ...\r\n",
            "Unpacking libopenal-dev:amd64 (1:1.18.2-2) ...\r\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\r\n",
            "Preparing to unpack .../68-libpulse-mainloop-glib0_1%3a11.1-1ubuntu7.4_amd64.deb ...\r\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.4) ...\r\n",
            "Selecting previously unselected package libpulse-dev:amd64.\r\n",
            "Preparing to unpack .../69-libpulse-dev_1%3a11.1-1ubuntu7.4_amd64.deb ...\r\n",
            "Unpacking libpulse-dev:amd64 (1:11.1-1ubuntu7.4) ...\r\n",
            "Selecting previously unselected package libsndio-dev:amd64.\r\n",
            "Preparing to unpack .../70-libsndio-dev_1.1.0-3_amd64.deb ...\r\n",
            "Unpacking libsndio-dev:amd64 (1.1.0-3) ...\r\n",
            "Selecting previously unselected package libudev-dev:amd64.\r\n",
            "Preparing to unpack .../71-libudev-dev_237-3ubuntu10.33_amd64.deb ...\r\n",
            "Unpacking libudev-dev:amd64 (237-3ubuntu10.33) ...\r\n",
            "Selecting previously unselected package libxv-dev:amd64.\r\n",
            "Preparing to unpack .../72-libxv-dev_2%3a1.0.11-1_amd64.deb ...\r\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1) ...\r\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\r\n",
            "Preparing to unpack .../73-libsdl2-dev_2.0.8+dfsg1-1ubuntu1.18.04.4_amd64.deb ...\r\n",
            "Unpacking libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\r\n",
            "Selecting previously unselected package libwildmidi-config.\r\n",
            "Preparing to unpack .../74-libwildmidi-config_0.4.2-1_all.deb ...\r\n",
            "Unpacking libwildmidi-config (0.4.2-1) ...\r\n",
            "Selecting previously unselected package libwildmidi2:amd64.\r\n",
            "Preparing to unpack .../75-libwildmidi2_0.4.2-1_amd64.deb ...\r\n",
            "Unpacking libwildmidi2:amd64 (0.4.2-1) ...\r\n",
            "Selecting previously unselected package libwildmidi-dev.\r\n",
            "Preparing to unpack .../76-libwildmidi-dev_0.4.2-1_amd64.deb ...\r\n",
            "Unpacking libwildmidi-dev (0.4.2-1) ...\r\n",
            "Selecting previously unselected package nasm.\r\n",
            "Preparing to unpack .../77-nasm_2.13.02-0.1_amd64.deb ...\r\n",
            "Unpacking nasm (2.13.02-0.1) ...\r\n",
            "Selecting previously unselected package timidity.\r\n",
            "Preparing to unpack .../78-timidity_2.13.2-41_amd64.deb ...\r\n",
            "Unpacking timidity (2.13.2-41) ...\r\n",
            "Selecting previously unselected package timidity-daemon.\r\n",
            "Preparing to unpack .../79-timidity-daemon_2.13.2-41_all.deb ...\r\n",
            "Unpacking timidity-daemon (2.13.2-41) ...\r\n",
            "Setting up libdbus-1-dev:amd64 (1.12.2-1ubuntu1.1) ...\r\n",
            "Setting up libxcursor-dev:amd64 (1:1.1.15-1) ...\r\n",
            "Setting up gir1.2-atk-1.0:amd64 (2.28.1-1) ...\r\n",
            "Setting up libxkbcommon-dev:amd64 (0.8.2-1~ubuntu18.04.1) ...\r\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.4) ...\r\n",
            "Setting up libpulse-dev:amd64 (1:11.1-1ubuntu7.4) ...\r\n",
            "Setting up libarchive-zip-perl (1.60-1ubuntu0.1) ...\r\n",
            "Setting up libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libtimedate-perl (2.3000-2) ...\r\n",
            "Setting up libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\r\n",
            "Setting up libgme-dev:amd64 (0.6.2-1) ...\r\n",
            "Setting up gir1.2-freedesktop:amd64 (1.56.1-1) ...\r\n",
            "Setting up libsndio-dev:amd64 (1.1.0-3) ...\r\n",
            "Setting up libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\r\n",
            "Setting up libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Setting up libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.2) ...\r\n",
            "Setting up libarchive-cpio-perl (0.10-1) ...\r\n",
            "Setting up gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\r\n",
            "Setting up libatk1.0-dev:amd64 (2.28.1-1) ...\r\n",
            "Setting up gettext-base (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Setting up m4 (1.4.18-1) ...\r\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.3) ...\r\n",
            "Setting up gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\r\n",
            "Setting up libopenal-dev:amd64 (1:1.18.2-2) ...\r\n",
            "Setting up libsys-hostname-long-perl (1.5-1) ...\r\n",
            "Setting up libwildmidi-config (0.4.2-1) ...\r\n",
            "Setting up libmircookie2:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libgdk-pixbuf2.0-dev (2.36.11-2) ...\r\n",
            "Setting up libmail-sendmail-perl (0.80-1) ...\r\n",
            "Setting up x11proto-xinerama-dev (2018.4-4) ...\r\n",
            "Setting up autotools-dev (20180224.1) ...\r\n",
            "Setting up libpixman-1-dev:amd64 (0.34.0-2) ...\r\n",
            "Setting up x11proto-randr-dev (2018.4-4) ...\r\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.3-1) ...\r\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1) ...\r\n",
            "Setting up nasm (2.13.02-0.1) ...\r\n",
            "Setting up libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\r\n",
            "Setting up libibus-1.0-5:amd64 (1.5.17-3ubuntu5.2) ...\r\n",
            "Setting up libmircore1:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up freepats (20060219-1) ...\r\n",
            "Setting up libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Setting up libudev-dev:amd64 (237-3ubuntu10.33) ...\r\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\r\n",
            "Setting up x11proto-composite-dev (1:2018.4-4) ...\r\n",
            "Setting up autopoint (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Setting up libaudio2:amd64 (1.9.4-6) ...\r\n",
            "Setting up libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\r\n",
            "Setting up gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\r\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu5.2) ...\r\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.1-1) ...\r\n",
            "Setting up libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Setting up gettext (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Setting up libxcomposite-dev:amd64 (1:0.4.4-2) ...\r\n",
            "Setting up libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Setting up autoconf (2.69-11) ...\r\n",
            "Setting up libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libwildmidi2:amd64 (0.4.2-1) ...\r\n",
            "Setting up file (1:5.32-2ubuntu0.3) ...\r\n",
            "Setting up intltool-debian (0.35.0+20060710.4) ...\r\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.17-3ubuntu5.2) ...\r\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\r\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\r\n",
            "Setting up libmircommon7:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libpango1.0-dev (1.40.14-1ubuntu0.1) ...\r\n",
            "Setting up libfluidsynth-dev:amd64 (1.1.9-1) ...\r\n",
            "Setting up timidity (2.13.2-41) ...\r\n",
            "Setting up libtool (2.4.6-2) ...\r\n",
            "Setting up po-debconf (1.0.20) ...\r\n",
            "Setting up libwildmidi-dev (0.4.2-1) ...\r\n",
            "Setting up libgtk2.0-dev (2.24.32-1ubuntu1) ...\r\n",
            "Setting up libmirclient9:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up timidity-daemon (2.13.2-41) ...\r\n",
            "Adding group timidity....done\r\n",
            "Adding system user timidity....done\r\n",
            "Adding user `timidity' to group `audio' ...\r\n",
            "Adding user timidity to group audio\r\n",
            "Done.\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of stop.\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "Setting up libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\r\n",
            "Setting up debhelper (11.1.6ubuntu2) ...\r\n",
            "Setting up dh-autoreconf (17) ...\r\n",
            "Setting up dh-strip-nondeterminism (0.040-1.1~build1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Processing triggers for systemd (237-3ubuntu10.33) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libtool-bin\n",
            "The following NEW packages will be installed:\n",
            "  liblua5.1-0-dev libtool-bin\n",
            "0 upgraded, 2 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 198 kB of archives.\n",
            "After this operation, 1,188 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 liblua5.1-0-dev amd64 5.1.5-8.1build2 [119 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool-bin amd64 2.4.6-2 [79.5 kB]\n",
            "Fetched 198 kB in 1s (287 kB/s)\n",
            "Selecting previously unselected package liblua5.1-0-dev:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 137511 files and directories currently installed.)\r\n",
            "Preparing to unpack .../liblua5.1-0-dev_5.1.5-8.1build2_amd64.deb ...\r\n",
            "Unpacking liblua5.1-0-dev:amd64 (5.1.5-8.1build2) ...\r\n",
            "Selecting previously unselected package libtool-bin.\r\n",
            "Preparing to unpack .../libtool-bin_2.4.6-2_amd64.deb ...\r\n",
            "Unpacking libtool-bin (2.4.6-2) ...\r\n",
            "Setting up libtool-bin (2.4.6-2) ...\r\n",
            "Setting up liblua5.1-0-dev:amd64 (5.1.5-8.1build2) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCmLVSzP1Ttz",
        "colab_type": "code",
        "outputId": "3ed0ec8c-51b9-406b-8556-b9832a055c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "!pip install vizdoom"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vizdoom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/6c/23565c09387173423883e7881fce53541ff89b5209ca0904c67e577dd6ac/vizdoom-1.1.7.tar.gz (4.9MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.9MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from vizdoom) (1.17.5)\n",
            "Building wheels for collected packages: vizdoom\n",
            "  Building wheel for vizdoom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vizdoom: filename=vizdoom-1.1.7-cp36-none-any.whl size=14285876 sha256=0b306226b7ae48dcfa26f9eb65f5c3e4e8956e451f101280fd8139a0bd88813d\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/98/04/d96d2c8edb8d1c008d926716257b407e56fb3ee0c81e51d25e\n",
            "Successfully built vizdoom\n",
            "Installing collected packages: vizdoom\n",
            "Successfully installed vizdoom-1.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhP_7xhgzoPx",
        "colab_type": "code",
        "outputId": "f5b7ec2d-05ab-40bc-d559-edb51f75bec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf      # Deep Learning library\n",
        "import numpy as np           # Handle matrices\n",
        "from vizdoom import *        # Doom Environment\n",
        "\n",
        "import random                # Handling random number generation\n",
        "import time                  # Handling time calculation\n",
        "from skimage import transform# Help us to preprocess the frames\n",
        "\n",
        "from collections import deque# Ordered collection with ends\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVk3TljjzoP4",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Create our environment üéÆ\n",
        "- Now that we imported the libraries/dependencies, we will create our environment.\n",
        "- Doom environment takes:\n",
        "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
        "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
        "- Note: We have 7 possible actions: turn left, turn right, move left, move right, shoot (attack)...`[[0,0,0,0,1]...]` so we don't need to do one hot encoding (thanks to <a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out). \n",
        "\n",
        "### Our environment\n",
        "<img src=\"https://simoninithomas.github.io/Deep_reinforcement_learning_Course/assets/img/video%20projects/deadlycorridor.png\" style=\"max-width:500px;\" alt=\"Vizdoom deadly corridor\"/>\n",
        "\n",
        "The purpose of this scenario is to teach the agent to navigate towards his fundamental goal (the vest) and make sure he survives at the same time.\n",
        "\n",
        "- Map is a corridor with shooting monsters on both sides (6 monsters in total). \n",
        "- A green vest is placed at the oposite end of the corridor. \n",
        "- **Reward is proportional (negative or positive) to change of the distance between the player and the vest.** \n",
        "- If player ignores monsters on the sides and runs straight for the vest he will be killed somewhere along the way. \n",
        "- To ensure this behavior doom_skill = 5 (config) is needed.\n",
        "\n",
        "<br>\n",
        "REWARDS:\n",
        "\n",
        "- +dX for getting closer to the vest. -dX for getting further from the vest.\n",
        "- death penalty = 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ues9-9ms3cRI",
        "colab_type": "code",
        "outputId": "c251fdb7-9192-48a4-8052-28550a5278a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mbloem/Deep_reinforcement_learning_Course/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.cfg\n",
        "!wget https://github.com/mbloem/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.wad?raw=true"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-04 11:36:12--  https://raw.githubusercontent.com/mbloem/Deep_reinforcement_learning_Course/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 892 [text/plain]\n",
            "Saving to: ‚Äòdeadly_corridor.cfg‚Äô\n",
            "\n",
            "\rdeadly_corridor.cfg   0%[                    ]       0  --.-KB/s               \rdeadly_corridor.cfg 100%[===================>]     892  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-04 11:36:12 (230 MB/s) - ‚Äòdeadly_corridor.cfg‚Äô saved [892/892]\n",
            "\n",
            "--2020-02-04 11:36:13--  https://github.com/mbloem/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.wad?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/mbloem/Deep_reinforcement_learning_Course/raw/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.wad [following]\n",
            "--2020-02-04 11:36:13--  https://github.com/mbloem/Deep_reinforcement_learning_Course/raw/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.wad\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mbloem/Deep_reinforcement_learning_Course/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.wad [following]\n",
            "--2020-02-04 11:36:13--  https://raw.githubusercontent.com/mbloem/Deep_reinforcement_learning_Course/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/deadly_corridor.wad\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10048 (9.8K) [application/octet-stream]\n",
            "Saving to: ‚Äòdeadly_corridor.wad?raw=true‚Äô\n",
            "\n",
            "deadly_corridor.wad 100%[===================>]   9.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-04 11:36:13 (162 MB/s) - ‚Äòdeadly_corridor.wad?raw=true‚Äô saved [10048/10048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gqT-M6-vltV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv deadly_corridor.wad\\?raw\\=true deadly_corridor.wad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzHWSxqiu64Z",
        "colab_type": "code",
        "outputId": "80bcf29b-3bf9-429f-8b6b-c33dd6c45f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20\n",
            "-rw-r--r-- 1 root root   892 Feb  4 11:36 deadly_corridor.cfg\n",
            "-rw-r--r-- 1 root root 10048 Feb  4 11:36 deadly_corridor.wad\n",
            "drwxr-xr-x 1 root root  4096 Jan 30 17:25 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QblJyohTzoP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Here we create our environment\n",
        "\"\"\"\n",
        "def create_environment():\n",
        "  game = DoomGame()\n",
        "\n",
        "  game.load_config('deadly_corridor.cfg')\n",
        "\n",
        "  game.set_doom_scenario_path('deadly_corridor.wad')\n",
        "\n",
        "  game.set_window_visible(False)\n",
        "\n",
        "  game.init()\n",
        "\n",
        "  # One-hot version of 7 possible actions\n",
        "  # Or is it 5 actions? turn left, turn right, move left, move right, shoot (attack)\n",
        "  possible_actions = np.identity(7,dtype=int).tolist()\n",
        "\n",
        "  return game, possible_actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS939CzCzoP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game, possible_actions = create_environment()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7_k2GP7IOa",
        "colab_type": "code",
        "outputId": "aa092aaa-4c1f-444e-bd9d-dd12b8406566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "possible_actions"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 0, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0],\n",
              " [0, 0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 1, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0],\n",
              " [0, 0, 0, 0, 0, 0, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ry7W5UuzoQE",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
        "### preprocess_frame\n",
        "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
        "<br><br>\n",
        "Our steps:\n",
        "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
        "- Crop the screen (in our case we remove the roof because it contains no information)\n",
        "- We normalize pixel values\n",
        "- Finally we resize the preprocessed frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjI4SeQdzoQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "preprocess_frame:\n",
        "Take a frame.\n",
        "Resize it.\n",
        "    __________________\n",
        "    |                 |\n",
        "    |                 |\n",
        "    |                 |\n",
        "    |                 |\n",
        "    |_________________|\n",
        "    \n",
        "    to\n",
        "    _____________\n",
        "    |            |\n",
        "    |            |\n",
        "    |            |\n",
        "    |____________|\n",
        "Normalize it.\n",
        "\n",
        "return preprocessed_frame\n",
        "\n",
        "\"\"\"\n",
        "def preprocess_frame(frame):\n",
        "  # Crop screen\n",
        "  cropped_frame = frame[15:-5,20:-20]\n",
        "\n",
        "  # Normalize pixel values\n",
        "  normalized_frame = cropped_frame/255.0\n",
        "\n",
        "  preprocessed_frame = transform.resize(cropped_frame, [100,120])\n",
        "\n",
        "  return preprocessed_frame # 100 x 120 x 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDqZdwP3zoQL",
        "colab_type": "text"
      },
      "source": [
        "### stack_frames\n",
        "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
        "\n",
        "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
        "\n",
        "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
        "\n",
        "- First we preprocess frame\n",
        "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
        "- Finally we **build the stacked state**\n",
        "\n",
        "This is how work stack:\n",
        "- For the first frame, we feed 4 frames\n",
        "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
        "- And so on\n",
        "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
        "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXp4-WdXzoQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stack_size = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDZTC3PBzoQR",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
        "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
        "\n",
        "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
        "- Then, you'll add the training hyperparameters when you implement the training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-TQJiYszoQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### MODEL\n",
        "state_size = [100,120,4]  # Input of 4 frames of 100 x 120 x 1\n",
        "action_size = 7 # game.get_available_buttons_size() returns 0 for some reason\n",
        "learning_rate = 0.00025 # alpha (learning rate)\n",
        "\n",
        "### TRAINING\n",
        "total_episodes = 5000\n",
        "max_steps = 5000\n",
        "batch_size = 64\n",
        "\n",
        "# FIXED Q TARGET\n",
        "max_tau = 10000 # this is the C step where we update our target network\n",
        "\n",
        "# EXPLORATION\n",
        "explore_start = 1.0\n",
        "explore_stop = 0.01\n",
        "decay_rate = 0.00005\n",
        "\n",
        "# Q LEARNING\n",
        "gamma = 0.95  # discount rate\n",
        "\n",
        "### MEMORY\n",
        "# If you have GPU then change to 1 million\n",
        "pretrain_length = 90000   # Number of experiences stored in memory when init time\n",
        "memory_size = 90000 # Number of experiences memory can keep\n",
        "\n",
        "# Set to false if you only want to see the trained agent\n",
        "training = True\n",
        "\n",
        "# Set to true if you want to render the environment\n",
        "episode_render = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viXGyS296-PG",
        "colab_type": "code",
        "outputId": "ac80892e-06a6-498d-eb7b-33a68ef1bf16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "game.get_available_buttons()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Button.MOVE_LEFT,\n",
              " Button.MOVE_RIGHT,\n",
              " Button.ATTACK,\n",
              " Button.MOVE_FORWARD,\n",
              " Button.MOVE_BACKWARD,\n",
              " Button.TURN_LEFT,\n",
              " Button.TURN_RIGHT]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNOaKmtzzoQZ",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Create our Dueling Double Deep Q-learning Neural Network model (aka DDDQN) üß†\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1500/1*FkHqwA2eSGixdS-3dvVoMA.png\" alt=\"Dueling Double Deep Q Learning Model\" />\n",
        "This is our Dueling Double Deep Q-learning model:\n",
        "- We take a stack of 4 frames as input\n",
        "- It passes through 3 convnets\n",
        "- Then it is flatened\n",
        "- Then it is passed through 2 streams\n",
        "    - One that calculates V(s)\n",
        "    - The other that calculates A(s,a)\n",
        "- Finally an agregating layer\n",
        "- It outputs a Q value for each actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOrQsA-szoQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DDDQNNet:\n",
        "  def __init__(self, state_size, action_size, learning_rate, name):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.name = name\n",
        "\n",
        "    # Use tf.variable_scope to know which network we're using (DQN or target)\n",
        "    # Will be useful when we update w- params in the target network\n",
        "    # by copying the DQN w params.\n",
        "    with tf.variable_scope(self.name):\n",
        "      self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
        "      \n",
        "      # Importance sampling weights. \n",
        "      # Why this dimension? Just a float? Might be \"b\" in the IS weight eqn\n",
        "      self.ISWeights_ = tf.placeholder(tf.float32, [None,1], name=\"IS_weights\")\n",
        "\n",
        "      self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions\")\n",
        "\n",
        "      # target Q(s,a) is R(s,a) + gamma * Q(s', argmax_{a'} Q(s,a',w), w-)\n",
        "      # though for Deuling double DQN we'll decompose into V(s) and A(s,a)\n",
        "      self.target_Q = tf.placeholder(tf.float32,[None], name='target')\n",
        "\n",
        "      \"\"\"\n",
        "      First convnet:\n",
        "      CNN\n",
        "      ELU\n",
        "      \"\"\"\n",
        "      # Input is 100 x 120 x 4\n",
        "      self.conv1 = tf.layers.conv2d(\n",
        "          inputs = self.inputs_,\n",
        "          filters = 32,\n",
        "          kernel_size = [8,8],\n",
        "          strides = [4,4],\n",
        "          padding = 'VALID',\n",
        "          kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "          name = 'conv1',\n",
        "      )\n",
        "\n",
        "      self.conv1_out = tf.nn.elu(self.conv1, name='conv1_out')\n",
        "\n",
        "      \"\"\"\n",
        "      Second convnet:\n",
        "      CNN\n",
        "      ELU\n",
        "      \"\"\"\n",
        "      # Input should be (100 pixels/4 stride) x (120 pixels/4 stride) x 32 filters\n",
        "      self.conv2 = tf.layers.conv2d(\n",
        "          inputs = self.conv1_out,\n",
        "          filters = 64,\n",
        "          kernel_size = [4,4],\n",
        "          strides = [2,2],\n",
        "          padding = 'VALID',\n",
        "          kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "          name = 'conv2',\n",
        "      )\n",
        "\n",
        "      self.conv2_out = tf.nn.elu(self.conv2, name = 'conv2_out')\n",
        "\n",
        "      \"\"\"\n",
        "      Third convnet:\n",
        "      CNN\n",
        "      ELU\n",
        "      \"\"\"\n",
        "      self.conv3 = tf.layers.conv2d(\n",
        "          inputs = self.conv2_out,\n",
        "          filters = 128,\n",
        "          kernel_size = [4,4],\n",
        "          strides = [2,2],\n",
        "          padding = 'VALID',\n",
        "          kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "          name = 'conv3',\n",
        "      )\n",
        "\n",
        "      self.conv3_out = tf.nn.elu(self.conv3, name='conv3_out')\n",
        "\n",
        "      self.flatten = tf.layers.flatten(self.conv3_out)\n",
        "\n",
        "      ## Separate into 2 streams\n",
        "      # Stream for V(s)\n",
        "      self.value_fc = tf.layers.dense(\n",
        "          inputs = self.flatten,\n",
        "          units = 512,\n",
        "          activation = tf.nn.elu,\n",
        "          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "          name = 'value_fc',\n",
        "      )\n",
        "\n",
        "      self.value = tf.layers.dense(\n",
        "          inputs = self.value_fc,\n",
        "          units = 1,\n",
        "          activation = None,\n",
        "          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "          name = 'value',\n",
        "      )\n",
        "\n",
        "      # Stream for A(s,a)\n",
        "      self.advantage_fc = tf.layers.dense(\n",
        "          inputs = self.flatten,\n",
        "          units = 512,\n",
        "          activation = tf.nn.elu,\n",
        "          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "          name = 'advantage_fc',\n",
        "      )\n",
        "\n",
        "      self.advantage = tf.layers.dense(\n",
        "          inputs = self.advantage_fc,\n",
        "          units = self.action_size,\n",
        "          activation = None,\n",
        "          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "          name = 'advantages',\n",
        "      )\n",
        "\n",
        "      # Aggregating layer\n",
        "      # Q(s,a) = V(s) + [A(s,a) - 1/|A| * sum_{a'} A(s,a')]\n",
        "      # I still don't follow how this works, especially the part about\n",
        "      # \"0 advantage at the chosen action\" in the article\n",
        "      # https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/\n",
        "      self.output = self.value + tf.subtract(\n",
        "          self.advantage,\n",
        "          tf.reduce_mean(\n",
        "              self.advantage,\n",
        "              axis=1,\n",
        "              keepdims=True,\n",
        "          )\n",
        "      )\n",
        "\n",
        "      # Q is our predicted Q value for the action that was selected (?)\n",
        "      # which will be set to 1 in self.actions_ while the non-selected\n",
        "      # will be set to 0 in self.actions_\n",
        "      self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_),axis=1)\n",
        "\n",
        "      # loss is modified because of PER\n",
        "      self.absolute_errors = tf.abs(self.target_Q - self.Q) # for updating sumtree\n",
        "\n",
        "      # weird that the loss uses squared_difference \n",
        "      # but the absolute_errors are used for updating sum_tree\n",
        "      self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q, self.Q))\n",
        "\n",
        "      self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubc1Nu7bzoQf",
        "colab_type": "code",
        "outputId": "31ce9e1d-0eea-48ca-ead3-2c3b7530c04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "# Reset the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Instantiate the DQNetwork\n",
        "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name='DQNetwork')\n",
        "\n",
        "# Instantiate the target network\n",
        "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name='TargetNetwork')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-15-c0eabca84c40>:37: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-15-c0eabca84c40>:77: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-15-c0eabca84c40>:86: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61jbDZKIzoQk",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Prioritized Experience Replay üîÅ\n",
        "Now that we create our Neural Network, **we need to implement the Prioritized Experience Replay method.** <br>\n",
        "\n",
        "As explained in the article, **we can't use a simple array to do that because sampling from it will be not efficient, so we use a binary tree data type (in a binary tree each node has no + than 2 children).** More precisely, a sumtree, which is a binary tree where parents nodes are the sum of the children nodes.\n",
        "\n",
        "If you don't know what is a binary tree check this awesome video https://www.youtube.com/watch?v=oSWTXtMglKE\n",
        "\n",
        "\n",
        "This SumTree implementation was taken from Morvan Zhou in his chinese course about Reinforcement Learning\n",
        "\n",
        "To summarize:\n",
        "- **Step 1**: We construct a SumTree, which is a Binary Sum tree where leaves contains the priorities and a data array where index points to the index of leaves.\n",
        "    <img src=\"https://cdn-images-1.medium.com/max/1200/1*Go9DNr7YY-wMGdIQ7HQduQ.png\" alt=\"SumTree\"/>\n",
        "    <br><br>\n",
        "    - **def __init__**: Initialize our SumTree data object with all nodes = 0 and data (data array) with all = 0.\n",
        "    - **def add**: add our priority score in the sumtree leaf and experience (S, A, R, S', Done) in data.\n",
        "    - **def update**: we update the leaf priority score and propagate through tree.\n",
        "    - **def get_leaf**: retrieve priority score, index and experience associated with a leaf.\n",
        "    - **def total_priority**: get the root node value to calculate the total priority score of our replay buffer.\n",
        "<br><br>\n",
        "- **Step 2**: We create a Memory object that will contain our sumtree and data.\n",
        "    - **def __init__**: generates our sumtree and data by instantiating the SumTree object.\n",
        "    - **def store**: we store a new experience in our tree. Each new experience will **have priority = max_priority** (and then this priority will be corrected during the training (when we'll calculating the TD error hence the priority score).\n",
        "    - **def sample**:\n",
        "         - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
        "         - Then a value is uniformly sampled from each range\n",
        "         - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
        "         - Then, we calculate IS weights for each minibatch element\n",
        "    - **def update_batch**: update the priorities on the tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La0-u50uzoQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SumTree(object):\n",
        "  \"\"\"\n",
        "  This SumTree code is modified version of Morvan Zhou: \n",
        "  https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
        "  \"\"\"\n",
        "  \n",
        "  data_pointer = 0\n",
        "\n",
        "  \"\"\"\n",
        "  Initialize tree\n",
        "  \"\"\"\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity # number of leaf nodes, each w/ an experience\n",
        "\n",
        "    # Generate tree with all node values set to 0\n",
        "    # 2*capacity-1 nodes because at each layer use 1/2 as many nodes\n",
        "    # as the layer below... \n",
        "    self.tree = np.zeros(2 * capacity - 1)\n",
        "\n",
        "    # Store the experiences themselves in data\n",
        "    self.data = np.zeros(capacity, dtype=object)\n",
        "\n",
        "  \"\"\"\n",
        "  Add our priority score in the sumtree leaf and add experience in data\n",
        "  \"\"\"\n",
        "  def add(self, priority, data):\n",
        "    tree_index = self.data_pointer + self.capacity - 1\n",
        "\n",
        "    self.data[self.data_pointer] = data\n",
        "\n",
        "    self.update(tree_index, priority)\n",
        "\n",
        "    self.data_pointer += 1\n",
        "\n",
        "    if self.data_pointer >= self.capacity:\n",
        "      # start overwriting\n",
        "      self.data_pointer = 0\n",
        "\n",
        "  \"\"\"\n",
        "  Update leaf priority score and propogate through tree\n",
        "  \"\"\"\n",
        "  def update(self, tree_index, priority):\n",
        "    change = priority - self.tree[tree_index]\n",
        "    self.tree[tree_index] = priority\n",
        "\n",
        "    # propogate change through the tree\n",
        "    while tree_index != 0:\n",
        "      \"\"\"\n",
        "      Here we are finding the parent node and changing it\n",
        "      THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
        "      \n",
        "          0\n",
        "          / \\\n",
        "        1   2\n",
        "        / \\ / \\\n",
        "      3  4 5  [6] \n",
        "      \n",
        "      If we are in leaf at index 6, we updated the priority score\n",
        "      We need then to update index 2 node\n",
        "      So tree_index = (tree_index - 1) // 2\n",
        "      tree_index = (6-1)//2\n",
        "      tree_index = 2 (because // round the result)\n",
        "      \"\"\"\n",
        "      tree_index = (tree_index - 1) // 2\n",
        "      self.tree[tree_index] += change\n",
        "\n",
        "  \"\"\"\n",
        "  Get the leaf_index, priority value of that leaf \n",
        "  and experience associated with that index\n",
        "  \"\"\"\n",
        "  def get_leaf(self, v):\n",
        "    \"\"\"\n",
        "    Tree structure and array storage:\n",
        "    Tree index:\n",
        "          0         -> storing priority sum\n",
        "        / \\\n",
        "      1     2\n",
        "      / \\   / \\\n",
        "    3   4 5   6    -> storing priority for experiences\n",
        "    Array type for storing:\n",
        "    [0,1,2,3,4,5,6]\n",
        "    \"\"\"\n",
        "    \n",
        "    parent_index = 0\n",
        "\n",
        "    while True: # Implements a recursion\n",
        "      left_child_index = 2 * parent_index + 1\n",
        "      right_child_index = left_child_index + 1\n",
        "\n",
        "      # Check if hit bottom\n",
        "      # Why not use right child index here?\n",
        "      if left_child_index >= len(self.tree):\n",
        "        leaf_index = parent_index\n",
        "        break\n",
        "\n",
        "      else:\n",
        "        # always search for a higher priority node\n",
        "        if v <= self.tree[left_child_index]:\n",
        "          parent_index = left_child_index\n",
        "\n",
        "        else:\n",
        "          v -= self.tree[left_child_index]\n",
        "          parent_index = right_child_index\n",
        "\n",
        "    data_index = leaf_index - self.capacity + 1\n",
        "\n",
        "    # Return leaf index, its priority, and its data (the experience)\n",
        "    return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
        "\n",
        "  @property\n",
        "  def total_priority(self):\n",
        "    return self.tree[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9kqf0VbzoQr",
        "colab_type": "text"
      },
      "source": [
        "Here we don't use deque anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iXZMN1CzoQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stored as ( s, a, r, s_ ) in SumTree\n",
        "class Memory(object): \n",
        "  \"\"\"\n",
        "  This SumTree code is modified version and the original code is from:\n",
        "  https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
        "  \"\"\"\n",
        "  PER_e = 0.01 # avoid 0 priority (and therefore 0 probability) experiences\n",
        "  PER_a = 0.6 # trades off between only sampling high-priority experiences\n",
        "  # (when a is 1) and sampling randomly (when a is 0)\n",
        "  PER_b = 0.4 # importance sampling, increases from here to 1 \n",
        "  # (at 1 we use the complete/full weight correction for our PER sampling)\n",
        "\n",
        "  PER_b_increment_per_sampling = 0.001\n",
        "\n",
        "  absolute_error_upper = 1. # clipped abs error\n",
        "\n",
        "  def __init__(self,capacity):\n",
        "    # Making the tree\n",
        "    self.tree = SumTree(capacity)\n",
        "\n",
        "  \"\"\"\n",
        "  Store new experience\n",
        "  Will give a max priority initially, then improve after used\n",
        "  \"\"\"\n",
        "  def store(self, experience):\n",
        "    # Find max priority\n",
        "    # max over the leaves\n",
        "    max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
        "\n",
        "    if max_priority == 0:\n",
        "      max_priority = self.absolute_error_upper\n",
        "\n",
        "    self.tree.add(max_priority, experience)\n",
        "\n",
        "  \"\"\"\n",
        "  Sample minibatch of size k\n",
        "  1) divide [0,priority_total] into k ranges\n",
        "  (I think this is to help avoid sampling same experience multiple times)\n",
        "  (though this could still happen when a sample's priority happens to\n",
        "  fall on the boundary between 2 of these ranges)\n",
        "  2) sample a value uniformly from each range\n",
        "  3) find the experience in the sumtree where the priority score corresponds\n",
        "  to each sampled value\n",
        "  4) calculate the IS weights for each minibatch element\n",
        "  (will differ for each because the have different priority scores and thus\n",
        "  different probabilities of being sampled)\n",
        "  (however, the extent of the IS weight correction quantified by PER_b\n",
        "  should be the same for all samples in a given minibatch)\n",
        "  \"\"\"\n",
        "  def sample(self,n):\n",
        "    memory_b = []\n",
        "\n",
        "    b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n,1), dtype=np.float32)\n",
        "\n",
        "    # Calculate the priority segment\n",
        "    # Divide into the n ranges in [0, priority total]\n",
        "    priority_segment = self.tree.total_priority / n\n",
        "\n",
        "    # Increase PER_b each time\n",
        "    self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])\n",
        "\n",
        "    # Calculate max_weight\n",
        "    # this is the biggest IS weighting we could have\n",
        "    # (for the sample with the very smallest probability of selection)\n",
        "    # used to normalize the IS weights later\n",
        "    p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
        "    max_weight = (p_min * n) ** (-self.PER_b)\n",
        "\n",
        "    for i in range(n):\n",
        "      # Sample a value uniformly from each range\n",
        "      a, b = priority_segment * i, priority_segment * (i + 1)\n",
        "\n",
        "      value = np.random.uniform(a,b)\n",
        "\n",
        "      # Experience that correspond to each value is retrieved\n",
        "      index, priority, data = self.tree.get_leaf(value)\n",
        "\n",
        "      # P(j): probability of the sample\n",
        "      sampling_probability = priority / self.tree.total_priority\n",
        "\n",
        "      # IS weight\n",
        "      b_ISWeights[i,0] = np.power(\n",
        "          n * sampling_probability,\n",
        "          -self.PER_b\n",
        "      ) / max_weight\n",
        "\n",
        "      b_idx[i] = index\n",
        "\n",
        "      experience = [data]\n",
        "\n",
        "      memory_b.append(experience)\n",
        "\n",
        "    return b_idx, memory_b, b_ISWeights\n",
        "\n",
        "  \"\"\"\n",
        "  batch update the priorities in the tree\n",
        "  used after running them through the DDQN and getting abs errors back\n",
        "  \"\"\"\n",
        "  def batch_update(self, tree_idx, abs_errors):\n",
        "    abs_errors += self.PER_e # avoid 0 probability\n",
        "    clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
        "    ps = np.power(clipped_errors, self.PER_a)\n",
        "\n",
        "    for ti, p in zip(tree_idx, ps):\n",
        "      self.tree.update(ti, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tADnzVEczoQz",
        "colab_type": "text"
      },
      "source": [
        "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkZac8kuzoQ0",
        "colab_type": "code",
        "outputId": "af0f865b-621b-4b3e-b9b9-d148743780fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "memory = Memory(memory_size)\n",
        "\n",
        "game.new_episode()\n",
        "\n",
        "for i in tqdm(range(pretrain_length)):\n",
        "  # If first step\n",
        "  if i == 0:\n",
        "    # Get a state\n",
        "    state = game.get_state().screen_buffer\n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "  action = random.choice(possible_actions)\n",
        "\n",
        "  reward = game.make_action(action)\n",
        "\n",
        "  done = game.is_episode_finished()\n",
        "\n",
        "  if done:\n",
        "    next_state = np.zeros(state.shape)\n",
        "    experience = state, action, reward, next_state, done\n",
        "    memory.store(experience)\n",
        "\n",
        "    game.new_episode()\n",
        "\n",
        "    state = game.get_state().screen_buffer\n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "  else:\n",
        "    next_state = game.get_state().screen_buffer\n",
        "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "    experience = state, action, reward, next_state, done\n",
        "    memory.store(experience)\n",
        "\n",
        "    state = next_state"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90000/90000 [03:09<00:00, 474.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyb9CYWBzoQ5",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Set up Tensorboard üìä\n",
        "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
        "To launch tensorboard : `tensorboard --logdir=/tensorboard/dddqn/1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN2fY7atzoQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = tf.summary.FileWriter('/tensorboard/dddqn/1')\n",
        "\n",
        "tf.summary.scalar('Loss', DQNetwork.loss)\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZtZNVLhxleI",
        "colab_type": "text"
      },
      "source": [
        "See https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU8pgp7SxkXk",
        "colab_type": "code",
        "outputId": "900bf15b-c955-4b90-cb36-e21072919e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-04 11:39:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.71.139.107, 52.204.223.154, 52.4.202.19, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.71.139.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‚Äòngrok-stable-linux-amd64.zip‚Äô\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  15.3MB/s    in 0.9s    \n",
            "\n",
            "2020-02-04 11:39:32 (15.3 MB/s) - ‚Äòngrok-stable-linux-amd64.zip‚Äô saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "899jb6_2xqhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = '/tensorboard/dddqn/1'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C24sEnEPxvmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh4m0vjKxzY9",
        "colab_type": "code",
        "outputId": "90766a71-429c-4197-b5bb-4dcfcbe4292b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://a107c83a.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DCNjq8LzoQ_",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
        "\n",
        "Our algorithm:\n",
        "<br>\n",
        "* Initialize the weights for DQN\n",
        "* Initialize target value weights w- <- w\n",
        "* Init the environment\n",
        "* Initialize the decay rate (that will use to reduce epsilon) \n",
        "<br><br>\n",
        "* **For** episode to max_episode **do** \n",
        "    * Make new episode\n",
        "    * Set step to 0\n",
        "    * Observe the first state $s_0$\n",
        "    <br><br>\n",
        "    * **While** step < max_steps **do**:\n",
        "        * Increase decay_rate\n",
        "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
        "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
        "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
        "        \n",
        "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
        "        * Set target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma Q(s',argmax_{a'}{Q(s', a', w), w^-)}$\n",
        "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
        "        * Every C steps, reset: $w^- \\leftarrow w$\n",
        "    * **endfor**\n",
        "    <br><br>\n",
        "* **endfor**\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99iQNoYbzoRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_action(\n",
        "    explore_start,\n",
        "    explore_stop,\n",
        "    decay_rate,\n",
        "    decay_step,\n",
        "    state,\n",
        "    actions\n",
        "):\n",
        "  exp_exp_tradeoff = np.random.rand()\n",
        "\n",
        "  explore_probability = explore_stop + (explore_start - explore_stop) \\\n",
        "   + np.exp(-decay_rate * decay_step)\n",
        "\n",
        "  if (explore_probability > exp_exp_tradeoff):\n",
        "    # Random action (exploration)\n",
        "    action = random.choice(possible_actions)\n",
        "\n",
        "  else:\n",
        "    # Get action from Q-network (exploitation)\n",
        "    # Estimate the Qs values state\n",
        "    Qs = sess.run(\n",
        "        DQNetwork.output,\n",
        "        feed_dict = {\n",
        "            DQNewtork.inputs_: state.reshape((1, *state_shape))\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Take the biggest Q\n",
        "    choice = np.argmax(Qs)\n",
        "    action = possible_actions[int(choice)]\n",
        "\n",
        "  return action, explore_probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2DDp6_9zoRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helps copy one set of variables to another\n",
        "# Used to copy params of DQN to Target_network\n",
        "def update_target_graph():\n",
        "\n",
        "  from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'DQNetwork')\n",
        "\n",
        "  to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'TargetNetwork')\n",
        "\n",
        "  op_holder = []\n",
        "\n",
        "  # Update our target_network parameters with DQNNetwork params\n",
        "  for from_var, to_var in zip(from_vars, to_vars):\n",
        "    op_holder.append(to_var.assign(from_var))\n",
        "  return op_holder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYqfQBvVzoRL",
        "colab_type": "code",
        "outputId": "72c3335b-cf0b-4cb5-da15-8f615b8121a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "\n",
        "if training==True:\n",
        "  with tf.Session() as sess:\n",
        "    # Initialize the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    decay_step = 0\n",
        "    tau = 0\n",
        "\n",
        "    game.init()\n",
        "\n",
        "    update_target = update_target_graph()\n",
        "    sess.run(update_target)\n",
        "\n",
        "    for episode in range(total_episodes):\n",
        "      step = 0\n",
        "\n",
        "      episode_rewards = []\n",
        "\n",
        "      game.new_episode()\n",
        "\n",
        "      state = game.get_state().screen_buffer\n",
        "\n",
        "      state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "      while step < max_steps:\n",
        "        step += 1\n",
        "\n",
        "        tau += 1\n",
        "\n",
        "        decay_step += 1\n",
        "\n",
        "        action, explore_probability = predict_action(\n",
        "          explore_start, \n",
        "          explore_stop, \n",
        "          decay_rate, \n",
        "          decay_step, \n",
        "          state, \n",
        "          possible_actions\n",
        "        )\n",
        "        \n",
        "        reward = game.make_action(action)\n",
        "\n",
        "        done = game.is_episode_finished()\n",
        "\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        if done:\n",
        "          next_state = np.zeros((120,140), dtype=np.int)\n",
        "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "          step = max_steps # to end the episode\n",
        "\n",
        "          total_reward = np.sum(episode_rewards)\n",
        "\n",
        "          print('Episode {}\\n'.format(episode),\n",
        "                '\\tTotal reward: {}\\n'.format(total_reward),\n",
        "                '\\tTraining loss: {:.4f}\\n'.format(loss),\n",
        "                '\\tExplore P: {:.4f}\\n'.format(explore_probability))\n",
        "          \n",
        "          experience = state, action, reward, next_state, done\n",
        "          memory.store(experience)\n",
        "\n",
        "        else:\n",
        "          next_state = game.get_state().screen_buffer\n",
        "\n",
        "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "          experience = state, action, reward, next_state, done\n",
        "          memory.store(experience)\n",
        "\n",
        "          state = next_state\n",
        "      \n",
        "        ### LEARNING\n",
        "        # Random mini-batch from memory\n",
        "        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
        "\n",
        "        states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
        "        actions_mb = np.array([each[0][1] for each in batch])\n",
        "        rewards_mb = np.array([each[0][2] for each in batch])\n",
        "        next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
        "        dones_mb = np.array([each[0][4] for each in batch])\n",
        "\n",
        "        target_Qs_batch = []\n",
        "\n",
        "        ### DOUBLE DQN LOGIC\n",
        "        # Use DQNNetwork to select the action to take at next_state (a') \n",
        "        # (action with largest Q value estimate)\n",
        "        # Use Target Newtork to calculate the Q_val of Q(s',a')\n",
        "\n",
        "        # Get Q values for next_state\n",
        "        q_next_state = sess.run(\n",
        "            DQNetwork.output,\n",
        "            feed_dict = {\n",
        "                DQNetwork.inputs_: next_states_mb\n",
        "            }\n",
        "        )\n",
        "\n",
        "        q_target_next_state = sess.run(\n",
        "            TargetNetwork.output,\n",
        "            feed_dict = {\n",
        "                TargetNetwork.inputs_: next_states_mb\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Set Q_target = r if episode ends at s+1, \n",
        "        # otherwise set to r + gamma * Qtarget(s',a')\n",
        "        for i in range(0, len(batch)):\n",
        "          terminal = dones_mb[i]\n",
        "\n",
        "          # We got a'\n",
        "          action = np.argmax(q_next_state[i])\n",
        "\n",
        "          # If we are in a terminal stae, only equals reward\n",
        "          if terminal:\n",
        "            target_Qs_batch.append(rewards_mb[i])\n",
        "\n",
        "          else:\n",
        "            # Take Qtarget for action a'\n",
        "            target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
        "            target_Qs_batch.append(target)\n",
        "\n",
        "        targets_mb = np.array([each for each in target_Qs_batch])\n",
        "\n",
        "        _, loss, absolute_errors = sess.run(\n",
        "            [DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
        "            feed_dict = {\n",
        "                DQNetwork.inputs_: states_mb,\n",
        "                DQNetwork.target_Q: targets_mb,\n",
        "                DQNetwork.actions_: actions_mb,\n",
        "                DQNetwork.ISWeights_: ISWeights_mb\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Update priorities in tree\n",
        "        memory.batch_update(tree_idx, absolute_errors)\n",
        "\n",
        "        # Write TF summaries\n",
        "        summary = sess.run(\n",
        "            write_op,\n",
        "            feed_dict = {\n",
        "                DQNetwork.inputs_: states_mb,\n",
        "                DQNetwork.target_Q: targets_mb,\n",
        "                DQNetwork.actions_: actions_mb,\n",
        "                DQNetwork.ISWeights_: ISWeights_mb\n",
        "            }\n",
        "        )\n",
        "        writer.add_summary(summary, episode)\n",
        "        writer.flush()\n",
        "\n",
        "        if tau > max_tau:\n",
        "          # Update parameters of target network with DQN weights\n",
        "          update_target = update_target_graph()\n",
        "          sess.run(update_target)\n",
        "          tau = 0\n",
        "          print('Model updated')\n",
        "\n",
        "      # Save model every 5 episodes\n",
        "      if episode % 5 == 0:\n",
        "        save_path = saver.save(sess, './models/model.ckpt')\n",
        "        print('Model saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0\n",
            " \tTotal reward: -115.92875671386719\n",
            " \tTraining loss: 29.1081\n",
            " \tExplore P: 1.9946\n",
            "\n",
            "Model saved\n",
            "Episode 1\n",
            " \tTotal reward: -104.08341979980469\n",
            " \tTraining loss: 0.3790\n",
            " \tExplore P: 1.9888\n",
            "\n",
            "Episode 2\n",
            " \tTotal reward: -114.1473388671875\n",
            " \tTraining loss: 0.1847\n",
            " \tExplore P: 1.9847\n",
            "\n",
            "Episode 3\n",
            " \tTotal reward: -89.62098693847656\n",
            " \tTraining loss: 10.4930\n",
            " \tExplore P: 1.9809\n",
            "\n",
            "Episode 4\n",
            " \tTotal reward: -112.6153564453125\n",
            " \tTraining loss: 0.2072\n",
            " \tExplore P: 1.9768\n",
            "\n",
            "Episode 5\n",
            " \tTotal reward: -115.21371459960938\n",
            " \tTraining loss: 0.1449\n",
            " \tExplore P: 1.9711\n",
            "\n",
            "Model saved\n",
            "Episode 6\n",
            " \tTotal reward: -110.41108703613281\n",
            " \tTraining loss: 22.4657\n",
            " \tExplore P: 1.9631\n",
            "\n",
            "Episode 7\n",
            " \tTotal reward: -105.44924926757812\n",
            " \tTraining loss: 0.2490\n",
            " \tExplore P: 1.9597\n",
            "\n",
            "Episode 8\n",
            " \tTotal reward: -102.93873596191406\n",
            " \tTraining loss: 0.4555\n",
            " \tExplore P: 1.9462\n",
            "\n",
            "Episode 9\n",
            " \tTotal reward: -106.80209350585938\n",
            " \tTraining loss: 12.6444\n",
            " \tExplore P: 1.9407\n",
            "\n",
            "Episode 10\n",
            " \tTotal reward: -80.47903442382812\n",
            " \tTraining loss: 0.2306\n",
            " \tExplore P: 1.9358\n",
            "\n",
            "Model saved\n",
            "Episode 11\n",
            " \tTotal reward: -104.80105590820312\n",
            " \tTraining loss: 11.0169\n",
            " \tExplore P: 1.9276\n",
            "\n",
            "Episode 12\n",
            " \tTotal reward: -92.52757263183594\n",
            " \tTraining loss: 0.3491\n",
            " \tExplore P: 1.9252\n",
            "\n",
            "Episode 13\n",
            " \tTotal reward: -115.98161315917969\n",
            " \tTraining loss: 21.9694\n",
            " \tExplore P: 1.9232\n",
            "\n",
            "Episode 14\n",
            " \tTotal reward: -83.08453369140625\n",
            " \tTraining loss: 0.4691\n",
            " \tExplore P: 1.9210\n",
            "\n",
            "Episode 15\n",
            " \tTotal reward: -60.696441650390625\n",
            " \tTraining loss: 0.2772\n",
            " \tExplore P: 1.9068\n",
            "\n",
            "Model saved\n",
            "Episode 16\n",
            " \tTotal reward: -112.56922912597656\n",
            " \tTraining loss: 0.2638\n",
            " \tExplore P: 1.9015\n",
            "\n",
            "Episode 17\n",
            " \tTotal reward: -98.41331481933594\n",
            " \tTraining loss: 25.3749\n",
            " \tExplore P: 1.8949\n",
            "\n",
            "Episode 18\n",
            " \tTotal reward: -94.17756652832031\n",
            " \tTraining loss: 12.6821\n",
            " \tExplore P: 1.8900\n",
            "\n",
            "Episode 19\n",
            " \tTotal reward: -90.75434875488281\n",
            " \tTraining loss: 12.3182\n",
            " \tExplore P: 1.8866\n",
            "\n",
            "Episode 20\n",
            " \tTotal reward: -114.88272094726562\n",
            " \tTraining loss: 9.5476\n",
            " \tExplore P: 1.8817\n",
            "\n",
            "Model saved\n",
            "Episode 21\n",
            " \tTotal reward: -108.429443359375\n",
            " \tTraining loss: 10.6268\n",
            " \tExplore P: 1.8798\n",
            "\n",
            "Episode 22\n",
            " \tTotal reward: -115.2232666015625\n",
            " \tTraining loss: 0.3443\n",
            " \tExplore P: 1.8779\n",
            "\n",
            "Episode 23\n",
            " \tTotal reward: -106.42982482910156\n",
            " \tTraining loss: 0.5724\n",
            " \tExplore P: 1.8745\n",
            "\n",
            "Episode 24\n",
            " \tTotal reward: -105.97308349609375\n",
            " \tTraining loss: 25.0582\n",
            " \tExplore P: 1.8708\n",
            "\n",
            "Episode 25\n",
            " \tTotal reward: -100.87319946289062\n",
            " \tTraining loss: 12.1066\n",
            " \tExplore P: 1.8645\n",
            "\n",
            "Model saved\n",
            "Episode 26\n",
            " \tTotal reward: -112.65887451171875\n",
            " \tTraining loss: 0.7169\n",
            " \tExplore P: 1.8614\n",
            "\n",
            "Episode 27\n",
            " \tTotal reward: -81.82545471191406\n",
            " \tTraining loss: 0.4803\n",
            " \tExplore P: 1.8556\n",
            "\n",
            "Episode 28\n",
            " \tTotal reward: -115.9755859375\n",
            " \tTraining loss: 11.8380\n",
            " \tExplore P: 1.8534\n",
            "\n",
            "Episode 29\n",
            " \tTotal reward: -104.17587280273438\n",
            " \tTraining loss: 19.0197\n",
            " \tExplore P: 1.8501\n",
            "\n",
            "Episode 30\n",
            " \tTotal reward: -96.55647277832031\n",
            " \tTraining loss: 0.5716\n",
            " \tExplore P: 1.8451\n",
            "\n",
            "Model saved\n",
            "Episode 31\n",
            " \tTotal reward: -110.3231201171875\n",
            " \tTraining loss: 0.4740\n",
            " \tExplore P: 1.8389\n",
            "\n",
            "Episode 32\n",
            " \tTotal reward: -102.3258056640625\n",
            " \tTraining loss: 30.2891\n",
            " \tExplore P: 1.8340\n",
            "\n",
            "Episode 33\n",
            " \tTotal reward: -97.7396240234375\n",
            " \tTraining loss: 12.5853\n",
            " \tExplore P: 1.8319\n",
            "\n",
            "Episode 34\n",
            " \tTotal reward: -110.46665954589844\n",
            " \tTraining loss: 0.7148\n",
            " \tExplore P: 1.8288\n",
            "\n",
            "Episode 35\n",
            " \tTotal reward: -108.51258850097656\n",
            " \tTraining loss: 10.1086\n",
            " \tExplore P: 1.8254\n",
            "\n",
            "Model saved\n",
            "Episode 36\n",
            " \tTotal reward: -37.62901306152344\n",
            " \tTraining loss: 1.1128\n",
            " \tExplore P: 1.8209\n",
            "\n",
            "Episode 37\n",
            " \tTotal reward: -115.87832641601562\n",
            " \tTraining loss: 0.8409\n",
            " \tExplore P: 1.8189\n",
            "\n",
            "Episode 38\n",
            " \tTotal reward: -99.29603576660156\n",
            " \tTraining loss: 19.8878\n",
            " \tExplore P: 1.8159\n",
            "\n",
            "Episode 39\n",
            " \tTotal reward: -98.28660583496094\n",
            " \tTraining loss: 9.1456\n",
            " \tExplore P: 1.8105\n",
            "\n",
            "Episode 40\n",
            " \tTotal reward: -114.86698913574219\n",
            " \tTraining loss: 21.1400\n",
            " \tExplore P: 1.8074\n",
            "\n",
            "Model saved\n",
            "Episode 41\n",
            " \tTotal reward: -102.29501342773438\n",
            " \tTraining loss: 0.7933\n",
            " \tExplore P: 1.8055\n",
            "\n",
            "Episode 42\n",
            " \tTotal reward: -110.42118835449219\n",
            " \tTraining loss: 10.7579\n",
            " \tExplore P: 1.8022\n",
            "\n",
            "Episode 43\n",
            " \tTotal reward: -98.51284790039062\n",
            " \tTraining loss: 13.6758\n",
            " \tExplore P: 1.8004\n",
            "\n",
            "Episode 44\n",
            " \tTotal reward: -107.92259216308594\n",
            " \tTraining loss: 10.1034\n",
            " \tExplore P: 1.7959\n",
            "\n",
            "Episode 45\n",
            " \tTotal reward: -107.39486694335938\n",
            " \tTraining loss: 9.2755\n",
            " \tExplore P: 1.7928\n",
            "\n",
            "Model saved\n",
            "Episode 46\n",
            " \tTotal reward: -109.95770263671875\n",
            " \tTraining loss: 19.0609\n",
            " \tExplore P: 1.7895\n",
            "\n",
            "Episode 47\n",
            " \tTotal reward: -115.86930847167969\n",
            " \tTraining loss: 11.1826\n",
            " \tExplore P: 1.7875\n",
            "\n",
            "Episode 48\n",
            " \tTotal reward: -97.92997741699219\n",
            " \tTraining loss: 1.0627\n",
            " \tExplore P: 1.7824\n",
            "\n",
            "Episode 49\n",
            " \tTotal reward: -112.796630859375\n",
            " \tTraining loss: 1.3758\n",
            " \tExplore P: 1.7793\n",
            "\n",
            "Episode 50\n",
            " \tTotal reward: -108.28419494628906\n",
            " \tTraining loss: 0.8784\n",
            " \tExplore P: 1.7752\n",
            "\n",
            "Model saved\n",
            "Episode 51\n",
            " \tTotal reward: -93.84107971191406\n",
            " \tTraining loss: 8.4745\n",
            " \tExplore P: 1.7721\n",
            "\n",
            "Episode 52\n",
            " \tTotal reward: -101.97567749023438\n",
            " \tTraining loss: 0.7463\n",
            " \tExplore P: 1.7689\n",
            "\n",
            "Episode 53\n",
            " \tTotal reward: -103.77110290527344\n",
            " \tTraining loss: 10.9927\n",
            " \tExplore P: 1.7633\n",
            "\n",
            "Episode 54\n",
            " \tTotal reward: -100.1527099609375\n",
            " \tTraining loss: 40.3863\n",
            " \tExplore P: 1.7576\n",
            "\n",
            "Episode 55\n",
            " \tTotal reward: -88.8709716796875\n",
            " \tTraining loss: 9.6527\n",
            " \tExplore P: 1.7533\n",
            "\n",
            "Model saved\n",
            "Episode 56\n",
            " \tTotal reward: -90.47718811035156\n",
            " \tTraining loss: 14.4832\n",
            " \tExplore P: 1.7516\n",
            "\n",
            "Episode 57\n",
            " \tTotal reward: -84.14912414550781\n",
            " \tTraining loss: 21.5892\n",
            " \tExplore P: 1.7297\n",
            "\n",
            "Episode 58\n",
            " \tTotal reward: -102.67436218261719\n",
            " \tTraining loss: 6.7816\n",
            " \tExplore P: 1.7267\n",
            "\n",
            "Episode 59\n",
            " \tTotal reward: -110.94967651367188\n",
            " \tTraining loss: 19.3554\n",
            " \tExplore P: 1.7241\n",
            "\n",
            "Episode 60\n",
            " \tTotal reward: -83.22457885742188\n",
            " \tTraining loss: 22.5258\n",
            " \tExplore P: 1.7223\n",
            "\n",
            "Model saved\n",
            "Episode 61\n",
            " \tTotal reward: -114.46624755859375\n",
            " \tTraining loss: 1.1466\n",
            " \tExplore P: 1.7194\n",
            "\n",
            "Episode 62\n",
            " \tTotal reward: -94.01742553710938\n",
            " \tTraining loss: 11.0400\n",
            " \tExplore P: 1.7164\n",
            "\n",
            "Episode 63\n",
            " \tTotal reward: -80.63487243652344\n",
            " \tTraining loss: 15.7918\n",
            " \tExplore P: 1.7136\n",
            "\n",
            "Episode 64\n",
            " \tTotal reward: -107.7823486328125\n",
            " \tTraining loss: 1.1966\n",
            " \tExplore P: 1.7120\n",
            "\n",
            "Episode 65\n",
            " \tTotal reward: -98.66896057128906\n",
            " \tTraining loss: 9.9387\n",
            " \tExplore P: 1.7105\n",
            "\n",
            "Model saved\n",
            "Episode 66\n",
            " \tTotal reward: -115.97813415527344\n",
            " \tTraining loss: 1.3587\n",
            " \tExplore P: 1.7075\n",
            "\n",
            "Episode 67\n",
            " \tTotal reward: -115.97685241699219\n",
            " \tTraining loss: 1.1870\n",
            " \tExplore P: 1.7035\n",
            "\n",
            "Episode 68\n",
            " \tTotal reward: -115.97810363769531\n",
            " \tTraining loss: 14.9373\n",
            " \tExplore P: 1.7005\n",
            "\n",
            "Episode 69\n",
            " \tTotal reward: -94.28936767578125\n",
            " \tTraining loss: 1.1665\n",
            " \tExplore P: 1.6990\n",
            "\n",
            "Episode 70\n",
            " \tTotal reward: -111.2449951171875\n",
            " \tTraining loss: 1.4990\n",
            " \tExplore P: 1.6949\n",
            "\n",
            "Model saved\n",
            "Episode 71\n",
            " \tTotal reward: -113.02188110351562\n",
            " \tTraining loss: 0.9745\n",
            " \tExplore P: 1.6924\n",
            "\n",
            "Episode 72\n",
            " \tTotal reward: -111.52969360351562\n",
            " \tTraining loss: 1.0144\n",
            " \tExplore P: 1.6886\n",
            "\n",
            "Episode 73\n",
            " \tTotal reward: -102.92453002929688\n",
            " \tTraining loss: 7.7216\n",
            " \tExplore P: 1.6850\n",
            "\n",
            "Episode 74\n",
            " \tTotal reward: -109.13763427734375\n",
            " \tTraining loss: 1.2558\n",
            " \tExplore P: 1.6811\n",
            "\n",
            "Episode 75\n",
            " \tTotal reward: -110.92057800292969\n",
            " \tTraining loss: 6.1287\n",
            " \tExplore P: 1.6785\n",
            "\n",
            "Model saved\n",
            "Episode 76\n",
            " \tTotal reward: -110.5445556640625\n",
            " \tTraining loss: 1.4046\n",
            " \tExplore P: 1.6735\n",
            "\n",
            "Episode 77\n",
            " \tTotal reward: -82.3643798828125\n",
            " \tTraining loss: 10.1655\n",
            " \tExplore P: 1.6695\n",
            "\n",
            "Episode 78\n",
            " \tTotal reward: -115.88482666015625\n",
            " \tTraining loss: 0.9858\n",
            " \tExplore P: 1.6656\n",
            "\n",
            "Episode 79\n",
            " \tTotal reward: -96.63050842285156\n",
            " \tTraining loss: 4.4556\n",
            " \tExplore P: 1.6552\n",
            "\n",
            "Episode 80\n",
            " \tTotal reward: -115.99716186523438\n",
            " \tTraining loss: 3.2710\n",
            " \tExplore P: 1.6505\n",
            "\n",
            "Model saved\n",
            "Episode 81\n",
            " \tTotal reward: -93.95863342285156\n",
            " \tTraining loss: 1.3081\n",
            " \tExplore P: 1.6476\n",
            "\n",
            "Episode 82\n",
            " \tTotal reward: -112.43659973144531\n",
            " \tTraining loss: 0.7666\n",
            " \tExplore P: 1.6461\n",
            "\n",
            "Episode 83\n",
            " \tTotal reward: -115.9957275390625\n",
            " \tTraining loss: 0.9080\n",
            " \tExplore P: 1.6444\n",
            "\n",
            "Episode 84\n",
            " \tTotal reward: -115.88951110839844\n",
            " \tTraining loss: 1.2428\n",
            " \tExplore P: 1.6421\n",
            "\n",
            "Episode 85\n",
            " \tTotal reward: -114.73532104492188\n",
            " \tTraining loss: 0.6485\n",
            " \tExplore P: 1.6395\n",
            "\n",
            "Model saved\n",
            "Episode 86\n",
            " \tTotal reward: -110.29388427734375\n",
            " \tTraining loss: 0.8283\n",
            " \tExplore P: 1.6368\n",
            "\n",
            "Episode 87\n",
            " \tTotal reward: -109.00930786132812\n",
            " \tTraining loss: 0.4994\n",
            " \tExplore P: 1.6331\n",
            "\n",
            "Episode 88\n",
            " \tTotal reward: -97.71330261230469\n",
            " \tTraining loss: 8.6834\n",
            " \tExplore P: 1.6283\n",
            "\n",
            "Episode 89\n",
            " \tTotal reward: -85.98713684082031\n",
            " \tTraining loss: 4.4185\n",
            " \tExplore P: 1.6237\n",
            "\n",
            "Episode 90\n",
            " \tTotal reward: -91.42636108398438\n",
            " \tTraining loss: 2.7739\n",
            " \tExplore P: 1.6223\n",
            "\n",
            "Model saved\n",
            "Episode 91\n",
            " \tTotal reward: -114.423828125\n",
            " \tTraining loss: 2.7701\n",
            " \tExplore P: 1.6208\n",
            "\n",
            "Episode 92\n",
            " \tTotal reward: -115.99937438964844\n",
            " \tTraining loss: 13.8342\n",
            " \tExplore P: 1.6184\n",
            "\n",
            "Episode 93\n",
            " \tTotal reward: -114.81498718261719\n",
            " \tTraining loss: 6.0726\n",
            " \tExplore P: 1.6160\n",
            "\n",
            "Episode 94\n",
            " \tTotal reward: -115.98150634765625\n",
            " \tTraining loss: 11.3298\n",
            " \tExplore P: 1.6124\n",
            "\n",
            "Model updated\n",
            "Episode 95\n",
            " \tTotal reward: -111.4027099609375\n",
            " \tTraining loss: 13.4076\n",
            " \tExplore P: 1.5817\n",
            "\n",
            "Model saved\n",
            "Episode 96\n",
            " \tTotal reward: -111.60589599609375\n",
            " \tTraining loss: 0.7020\n",
            " \tExplore P: 1.5794\n",
            "\n",
            "Episode 97\n",
            " \tTotal reward: -94.72393798828125\n",
            " \tTraining loss: 10.8723\n",
            " \tExplore P: 1.5769\n",
            "\n",
            "Episode 98\n",
            " \tTotal reward: -111.26371765136719\n",
            " \tTraining loss: 12.6705\n",
            " \tExplore P: 1.5736\n",
            "\n",
            "Episode 99\n",
            " \tTotal reward: -97.26264953613281\n",
            " \tTraining loss: 3.6948\n",
            " \tExplore P: 1.5715\n",
            "\n",
            "Episode 100\n",
            " \tTotal reward: -70.66802978515625\n",
            " \tTraining loss: 8.9555\n",
            " \tExplore P: 1.5693\n",
            "\n",
            "Model saved\n",
            "Episode 101\n",
            " \tTotal reward: -114.697021484375\n",
            " \tTraining loss: 0.7500\n",
            " \tExplore P: 1.5670\n",
            "\n",
            "Episode 102\n",
            " \tTotal reward: -73.16488647460938\n",
            " \tTraining loss: 7.9023\n",
            " \tExplore P: 1.5647\n",
            "\n",
            "Episode 103\n",
            " \tTotal reward: -114.15818786621094\n",
            " \tTraining loss: 1.4819\n",
            " \tExplore P: 1.5624\n",
            "\n",
            "Episode 104\n",
            " \tTotal reward: -115.93685913085938\n",
            " \tTraining loss: 6.3245\n",
            " \tExplore P: 1.5592\n",
            "\n",
            "Episode 105\n",
            " \tTotal reward: -113.04486083984375\n",
            " \tTraining loss: 1.2833\n",
            " \tExplore P: 1.5587\n",
            "\n",
            "Model saved\n",
            "Episode 106\n",
            " \tTotal reward: -109.89012145996094\n",
            " \tTraining loss: 1.1079\n",
            " \tExplore P: 1.5540\n",
            "\n",
            "Episode 107\n",
            " \tTotal reward: -115.19096374511719\n",
            " \tTraining loss: 1.7354\n",
            " \tExplore P: 1.5527\n",
            "\n",
            "Episode 108\n",
            " \tTotal reward: -97.42935180664062\n",
            " \tTraining loss: 1.9505\n",
            " \tExplore P: 1.5505\n",
            "\n",
            "Episode 109\n",
            " \tTotal reward: -102.41546630859375\n",
            " \tTraining loss: 2.9897\n",
            " \tExplore P: 1.5493\n",
            "\n",
            "Episode 110\n",
            " \tTotal reward: -111.85884094238281\n",
            " \tTraining loss: 1.8247\n",
            " \tExplore P: 1.5462\n",
            "\n",
            "Model saved\n",
            "Episode 111\n",
            " \tTotal reward: -97.66542053222656\n",
            " \tTraining loss: 1.7149\n",
            " \tExplore P: 1.5430\n",
            "\n",
            "Episode 112\n",
            " \tTotal reward: -101.29847717285156\n",
            " \tTraining loss: 3.3133\n",
            " \tExplore P: 1.5407\n",
            "\n",
            "Episode 113\n",
            " \tTotal reward: -51.352264404296875\n",
            " \tTraining loss: 3.2714\n",
            " \tExplore P: 1.5356\n",
            "\n",
            "Episode 114\n",
            " \tTotal reward: -111.02836608886719\n",
            " \tTraining loss: 6.4384\n",
            " \tExplore P: 1.5343\n",
            "\n",
            "Episode 115\n",
            " \tTotal reward: -101.07241821289062\n",
            " \tTraining loss: 0.9591\n",
            " \tExplore P: 1.5331\n",
            "\n",
            "Model saved\n",
            "Episode 116\n",
            " \tTotal reward: -114.48924255371094\n",
            " \tTraining loss: 5.1566\n",
            " \tExplore P: 1.5301\n",
            "\n",
            "Episode 117\n",
            " \tTotal reward: -115.95115661621094\n",
            " \tTraining loss: 1.2966\n",
            " \tExplore P: 1.5280\n",
            "\n",
            "Episode 118\n",
            " \tTotal reward: -103.93183898925781\n",
            " \tTraining loss: 1.8110\n",
            " \tExplore P: 1.5252\n",
            "\n",
            "Episode 119\n",
            " \tTotal reward: -113.57510375976562\n",
            " \tTraining loss: 2.2008\n",
            " \tExplore P: 1.5191\n",
            "\n",
            "Episode 120\n",
            " \tTotal reward: -102.33868408203125\n",
            " \tTraining loss: 7.2339\n",
            " \tExplore P: 1.5139\n",
            "\n",
            "Model saved\n",
            "Episode 121\n",
            " \tTotal reward: -114.95298767089844\n",
            " \tTraining loss: 0.7447\n",
            " \tExplore P: 1.5118\n",
            "\n",
            "Episode 122\n",
            " \tTotal reward: -115.13050842285156\n",
            " \tTraining loss: 28.0200\n",
            " \tExplore P: 1.5097\n",
            "\n",
            "Episode 123\n",
            " \tTotal reward: -114.32603454589844\n",
            " \tTraining loss: 4.0116\n",
            " \tExplore P: 1.5072\n",
            "\n",
            "Episode 124\n",
            " \tTotal reward: -109.07696533203125\n",
            " \tTraining loss: 2.4680\n",
            " \tExplore P: 1.5036\n",
            "\n",
            "Episode 125\n",
            " \tTotal reward: -113.90005493164062\n",
            " \tTraining loss: 0.9205\n",
            " \tExplore P: 1.5023\n",
            "\n",
            "Model saved\n",
            "Episode 126\n",
            " \tTotal reward: -115.66964721679688\n",
            " \tTraining loss: 3.1047\n",
            " \tExplore P: 1.5009\n",
            "\n",
            "Episode 127\n",
            " \tTotal reward: -109.459228515625\n",
            " \tTraining loss: 1.9195\n",
            " \tExplore P: 1.4992\n",
            "\n",
            "Episode 128\n",
            " \tTotal reward: -115.99713134765625\n",
            " \tTraining loss: 1.3178\n",
            " \tExplore P: 1.4965\n",
            "\n",
            "Episode 129\n",
            " \tTotal reward: -109.84806823730469\n",
            " \tTraining loss: 0.6351\n",
            " \tExplore P: 1.4917\n",
            "\n",
            "Episode 130\n",
            " \tTotal reward: -106.7469482421875\n",
            " \tTraining loss: 0.9632\n",
            " \tExplore P: 1.4897\n",
            "\n",
            "Model saved\n",
            "Episode 131\n",
            " \tTotal reward: -107.755126953125\n",
            " \tTraining loss: 2.0640\n",
            " \tExplore P: 1.4878\n",
            "\n",
            "Episode 132\n",
            " \tTotal reward: -90.01480102539062\n",
            " \tTraining loss: 1.1073\n",
            " \tExplore P: 1.4851\n",
            "\n",
            "Episode 133\n",
            " \tTotal reward: -115.97587585449219\n",
            " \tTraining loss: 8.4475\n",
            " \tExplore P: 1.4812\n",
            "\n",
            "Episode 134\n",
            " \tTotal reward: -111.84518432617188\n",
            " \tTraining loss: 6.7456\n",
            " \tExplore P: 1.4794\n",
            "\n",
            "Episode 135\n",
            " \tTotal reward: -109.16506958007812\n",
            " \tTraining loss: 8.6406\n",
            " \tExplore P: 1.4750\n",
            "\n",
            "Model saved\n",
            "Episode 136\n",
            " \tTotal reward: -114.48481750488281\n",
            " \tTraining loss: 2.6609\n",
            " \tExplore P: 1.4732\n",
            "\n",
            "Episode 137\n",
            " \tTotal reward: -95.9036865234375\n",
            " \tTraining loss: 1.2797\n",
            " \tExplore P: 1.4706\n",
            "\n",
            "Episode 138\n",
            " \tTotal reward: -106.14407348632812\n",
            " \tTraining loss: 15.0381\n",
            " \tExplore P: 1.4680\n",
            "\n",
            "Episode 139\n",
            " \tTotal reward: -106.98565673828125\n",
            " \tTraining loss: 21.5705\n",
            " \tExplore P: 1.4668\n",
            "\n",
            "Episode 140\n",
            " \tTotal reward: -110.11187744140625\n",
            " \tTraining loss: 14.6151\n",
            " \tExplore P: 1.4643\n",
            "\n",
            "Model saved\n",
            "Episode 141\n",
            " \tTotal reward: -104.66960144042969\n",
            " \tTraining loss: 1.5833\n",
            " \tExplore P: 1.4570\n",
            "\n",
            "Episode 142\n",
            " \tTotal reward: -113.96083068847656\n",
            " \tTraining loss: 4.6112\n",
            " \tExplore P: 1.4545\n",
            "\n",
            "Episode 143\n",
            " \tTotal reward: -104.71023559570312\n",
            " \tTraining loss: 8.9803\n",
            " \tExplore P: 1.4495\n",
            "\n",
            "Episode 144\n",
            " \tTotal reward: -115.99859619140625\n",
            " \tTraining loss: 8.3955\n",
            " \tExplore P: 1.4478\n",
            "\n",
            "Episode 145\n",
            " \tTotal reward: -115.97134399414062\n",
            " \tTraining loss: 1.1291\n",
            " \tExplore P: 1.4459\n",
            "\n",
            "Model saved\n",
            "Episode 146\n",
            " \tTotal reward: -114.08354187011719\n",
            " \tTraining loss: 4.2504\n",
            " \tExplore P: 1.4450\n",
            "\n",
            "Episode 147\n",
            " \tTotal reward: -111.82417297363281\n",
            " \tTraining loss: 14.5947\n",
            " \tExplore P: 1.4420\n",
            "\n",
            "Episode 148\n",
            " \tTotal reward: -42.89717102050781\n",
            " \tTraining loss: 5.4527\n",
            " \tExplore P: 1.4390\n",
            "\n",
            "Episode 149\n",
            " \tTotal reward: -114.84550476074219\n",
            " \tTraining loss: 0.9169\n",
            " \tExplore P: 1.4366\n",
            "\n",
            "Episode 150\n",
            " \tTotal reward: -115.881591796875\n",
            " \tTraining loss: 8.8939\n",
            " \tExplore P: 1.4350\n",
            "\n",
            "Model saved\n",
            "Episode 151\n",
            " \tTotal reward: -99.66476440429688\n",
            " \tTraining loss: 3.2965\n",
            " \tExplore P: 1.4325\n",
            "\n",
            "Episode 152\n",
            " \tTotal reward: -109.15602111816406\n",
            " \tTraining loss: 2.2889\n",
            " \tExplore P: 1.4307\n",
            "\n",
            "Episode 153\n",
            " \tTotal reward: -115.22869873046875\n",
            " \tTraining loss: 1.6408\n",
            " \tExplore P: 1.4289\n",
            "\n",
            "Episode 154\n",
            " \tTotal reward: -115.9315185546875\n",
            " \tTraining loss: 5.3407\n",
            " \tExplore P: 1.4278\n",
            "\n",
            "Episode 155\n",
            " \tTotal reward: -115.19474792480469\n",
            " \tTraining loss: 1.8709\n",
            " \tExplore P: 1.4253\n",
            "\n",
            "Model saved\n",
            "Episode 156\n",
            " \tTotal reward: -95.51036071777344\n",
            " \tTraining loss: 1.1681\n",
            " \tExplore P: 1.4243\n",
            "\n",
            "Episode 157\n",
            " \tTotal reward: -111.16841125488281\n",
            " \tTraining loss: 11.5209\n",
            " \tExplore P: 1.4226\n",
            "\n",
            "Episode 158\n",
            " \tTotal reward: -115.81550598144531\n",
            " \tTraining loss: 2.8180\n",
            " \tExplore P: 1.4195\n",
            "\n",
            "Episode 159\n",
            " \tTotal reward: -104.95210266113281\n",
            " \tTraining loss: 0.7353\n",
            " \tExplore P: 1.4184\n",
            "\n",
            "Episode 160\n",
            " \tTotal reward: -115.94850158691406\n",
            " \tTraining loss: 1.4512\n",
            " \tExplore P: 1.4168\n",
            "\n",
            "Model saved\n",
            "Episode 161\n",
            " \tTotal reward: -104.32817077636719\n",
            " \tTraining loss: 1.1416\n",
            " \tExplore P: 1.4150\n",
            "\n",
            "Episode 162\n",
            " \tTotal reward: -104.97784423828125\n",
            " \tTraining loss: 1.3998\n",
            " \tExplore P: 1.4141\n",
            "\n",
            "Episode 163\n",
            " \tTotal reward: -109.65425109863281\n",
            " \tTraining loss: 1.2625\n",
            " \tExplore P: 1.4123\n",
            "\n",
            "Episode 164\n",
            " \tTotal reward: -115.98103332519531\n",
            " \tTraining loss: 4.4393\n",
            " \tExplore P: 1.4112\n",
            "\n",
            "Episode 165\n",
            " \tTotal reward: -110.37924194335938\n",
            " \tTraining loss: 2.0415\n",
            " \tExplore P: 1.4097\n",
            "\n",
            "Model saved\n",
            "Episode 166\n",
            " \tTotal reward: -98.44210815429688\n",
            " \tTraining loss: 0.8573\n",
            " \tExplore P: 1.4081\n",
            "\n",
            "Episode 167\n",
            " \tTotal reward: -110.29545593261719\n",
            " \tTraining loss: 9.2882\n",
            " \tExplore P: 1.4057\n",
            "\n",
            "Episode 168\n",
            " \tTotal reward: -106.82023620605469\n",
            " \tTraining loss: 1.6003\n",
            " \tExplore P: 1.4041\n",
            "\n",
            "Episode 169\n",
            " \tTotal reward: -115.97763061523438\n",
            " \tTraining loss: 4.3580\n",
            " \tExplore P: 1.4031\n",
            "\n",
            "Episode 170\n",
            " \tTotal reward: -92.32861328125\n",
            " \tTraining loss: 2.3923\n",
            " \tExplore P: 1.4014\n",
            "\n",
            "Model saved\n",
            "Episode 171\n",
            " \tTotal reward: -104.45773315429688\n",
            " \tTraining loss: 3.2100\n",
            " \tExplore P: 1.4000\n",
            "\n",
            "Episode 172\n",
            " \tTotal reward: -91.35969543457031\n",
            " \tTraining loss: 2.1402\n",
            " \tExplore P: 1.3971\n",
            "\n",
            "Episode 173\n",
            " \tTotal reward: -104.295654296875\n",
            " \tTraining loss: 2.2124\n",
            " \tExplore P: 1.3961\n",
            "\n",
            "Episode 174\n",
            " \tTotal reward: -96.69125366210938\n",
            " \tTraining loss: 6.4219\n",
            " \tExplore P: 1.3947\n",
            "\n",
            "Episode 175\n",
            " \tTotal reward: -110.30641174316406\n",
            " \tTraining loss: 0.5050\n",
            " \tExplore P: 1.3931\n",
            "\n",
            "Model saved\n",
            "Episode 176\n",
            " \tTotal reward: -93.33024597167969\n",
            " \tTraining loss: 1.6204\n",
            " \tExplore P: 1.3914\n",
            "\n",
            "Episode 177\n",
            " \tTotal reward: -113.28872680664062\n",
            " \tTraining loss: 18.3545\n",
            " \tExplore P: 1.3892\n",
            "\n",
            "Episode 178\n",
            " \tTotal reward: -115.99742126464844\n",
            " \tTraining loss: 1.5871\n",
            " \tExplore P: 1.3883\n",
            "\n",
            "Episode 179\n",
            " \tTotal reward: -94.16050720214844\n",
            " \tTraining loss: 1.9969\n",
            " \tExplore P: 1.3874\n",
            "\n",
            "Episode 180\n",
            " \tTotal reward: -98.39614868164062\n",
            " \tTraining loss: 0.5690\n",
            " \tExplore P: 1.3858\n",
            "\n",
            "Model saved\n",
            "Episode 181\n",
            " \tTotal reward: -115.92532348632812\n",
            " \tTraining loss: 0.8912\n",
            " \tExplore P: 1.3824\n",
            "\n",
            "Episode 182\n",
            " \tTotal reward: -108.85321044921875\n",
            " \tTraining loss: 1.2193\n",
            " \tExplore P: 1.3815\n",
            "\n",
            "Episode 183\n",
            " \tTotal reward: -110.87022399902344\n",
            " \tTraining loss: 1.2672\n",
            " \tExplore P: 1.3800\n",
            "\n",
            "Episode 184\n",
            " \tTotal reward: -111.05133056640625\n",
            " \tTraining loss: 2.7360\n",
            " \tExplore P: 1.3784\n",
            "\n",
            "Episode 185\n",
            " \tTotal reward: -114.4188232421875\n",
            " \tTraining loss: 2.4485\n",
            " \tExplore P: 1.3756\n",
            "\n",
            "Model saved\n",
            "Episode 186\n",
            " \tTotal reward: -96.54167175292969\n",
            " \tTraining loss: 12.5599\n",
            " \tExplore P: 1.3740\n",
            "\n",
            "Episode 187\n",
            " \tTotal reward: -111.98103332519531\n",
            " \tTraining loss: 1.6798\n",
            " \tExplore P: 1.3713\n",
            "\n",
            "Episode 188\n",
            " \tTotal reward: -102.21672058105469\n",
            " \tTraining loss: 1.2711\n",
            " \tExplore P: 1.3699\n",
            "\n",
            "Episode 189\n",
            " \tTotal reward: -106.8133544921875\n",
            " \tTraining loss: 1.2102\n",
            " \tExplore P: 1.3689\n",
            "\n",
            "Model updated\n",
            "Episode 190\n",
            " \tTotal reward: -107.61489868164062\n",
            " \tTraining loss: 2.5327\n",
            " \tExplore P: 1.3668\n",
            "\n",
            "Model saved\n",
            "Episode 191\n",
            " \tTotal reward: -111.51422119140625\n",
            " \tTraining loss: 2.1423\n",
            " \tExplore P: 1.3649\n",
            "\n",
            "Episode 192\n",
            " \tTotal reward: -115.99955749511719\n",
            " \tTraining loss: 0.7232\n",
            " \tExplore P: 1.3634\n",
            "\n",
            "Episode 193\n",
            " \tTotal reward: -86.31723022460938\n",
            " \tTraining loss: 1.9296\n",
            " \tExplore P: 1.3626\n",
            "\n",
            "Episode 194\n",
            " \tTotal reward: -106.10528564453125\n",
            " \tTraining loss: 1.3144\n",
            " \tExplore P: 1.3602\n",
            "\n",
            "Episode 195\n",
            " \tTotal reward: -113.07635498046875\n",
            " \tTraining loss: 0.8146\n",
            " \tExplore P: 1.3588\n",
            "\n",
            "Model saved\n",
            "Episode 196\n",
            " \tTotal reward: -102.52781677246094\n",
            " \tTraining loss: 1.0414\n",
            " \tExplore P: 1.3580\n",
            "\n",
            "Episode 197\n",
            " \tTotal reward: -110.53712463378906\n",
            " \tTraining loss: 1.2584\n",
            " \tExplore P: 1.3572\n",
            "\n",
            "Episode 198\n",
            " \tTotal reward: -115.97787475585938\n",
            " \tTraining loss: 0.7237\n",
            " \tExplore P: 1.3557\n",
            "\n",
            "Episode 199\n",
            " \tTotal reward: -105.45399475097656\n",
            " \tTraining loss: 1.7951\n",
            " \tExplore P: 1.3522\n",
            "\n",
            "Episode 200\n",
            " \tTotal reward: -113.51730346679688\n",
            " \tTraining loss: 0.7487\n",
            " \tExplore P: 1.3499\n",
            "\n",
            "Model saved\n",
            "Episode 201\n",
            " \tTotal reward: -115.97377014160156\n",
            " \tTraining loss: 1.3618\n",
            " \tExplore P: 1.3491\n",
            "\n",
            "Episode 202\n",
            " \tTotal reward: -94.03627014160156\n",
            " \tTraining loss: 16.7151\n",
            " \tExplore P: 1.3478\n",
            "\n",
            "Episode 203\n",
            " \tTotal reward: -64.04183959960938\n",
            " \tTraining loss: 0.6703\n",
            " \tExplore P: 1.3406\n",
            "\n",
            "Episode 204\n",
            " \tTotal reward: -113.20060729980469\n",
            " \tTraining loss: 1.7377\n",
            " \tExplore P: 1.3393\n",
            "\n",
            "Episode 205\n",
            " \tTotal reward: -111.98838806152344\n",
            " \tTraining loss: 12.2217\n",
            " \tExplore P: 1.3374\n",
            "\n",
            "Model saved\n",
            "Episode 206\n",
            " \tTotal reward: -115.99794006347656\n",
            " \tTraining loss: 1.4157\n",
            " \tExplore P: 1.3361\n",
            "\n",
            "Episode 207\n",
            " \tTotal reward: -104.06867980957031\n",
            " \tTraining loss: 1.0583\n",
            " \tExplore P: 1.3347\n",
            "\n",
            "Episode 208\n",
            " \tTotal reward: -91.8270263671875\n",
            " \tTraining loss: 1.3092\n",
            " \tExplore P: 1.3334\n",
            "\n",
            "Episode 209\n",
            " \tTotal reward: -98.51861572265625\n",
            " \tTraining loss: 3.1746\n",
            " \tExplore P: 1.3326\n",
            "\n",
            "Episode 210\n",
            " \tTotal reward: -107.81504821777344\n",
            " \tTraining loss: 11.8310\n",
            " \tExplore P: 1.3319\n",
            "\n",
            "Model saved\n",
            "Episode 211\n",
            " \tTotal reward: -110.89418029785156\n",
            " \tTraining loss: 1.2388\n",
            " \tExplore P: 1.3307\n",
            "\n",
            "Episode 212\n",
            " \tTotal reward: -114.4012451171875\n",
            " \tTraining loss: 0.4343\n",
            " \tExplore P: 1.3282\n",
            "\n",
            "Episode 213\n",
            " \tTotal reward: -113.95130920410156\n",
            " \tTraining loss: 0.7948\n",
            " \tExplore P: 1.3270\n",
            "\n",
            "Episode 214\n",
            " \tTotal reward: -101.36175537109375\n",
            " \tTraining loss: 0.8244\n",
            " \tExplore P: 1.3251\n",
            "\n",
            "Episode 215\n",
            " \tTotal reward: -15.676666259765625\n",
            " \tTraining loss: 0.9313\n",
            " \tExplore P: 1.3182\n",
            "\n",
            "Model saved\n",
            "Episode 216\n",
            " \tTotal reward: -105.6280517578125\n",
            " \tTraining loss: 15.9249\n",
            " \tExplore P: 1.3164\n",
            "\n",
            "Episode 217\n",
            " \tTotal reward: -110.10942077636719\n",
            " \tTraining loss: 9.0919\n",
            " \tExplore P: 1.3141\n",
            "\n",
            "Episode 218\n",
            " \tTotal reward: -105.53718566894531\n",
            " \tTraining loss: 1.0568\n",
            " \tExplore P: 1.3124\n",
            "\n",
            "Episode 219\n",
            " \tTotal reward: -95.58253479003906\n",
            " \tTraining loss: 5.2282\n",
            " \tExplore P: 1.3117\n",
            "\n",
            "Episode 220\n",
            " \tTotal reward: -115.99293518066406\n",
            " \tTraining loss: 2.2666\n",
            " \tExplore P: 1.3082\n",
            "\n",
            "Model saved\n",
            "Episode 221\n",
            " \tTotal reward: -100.25039672851562\n",
            " \tTraining loss: 1.0889\n",
            " \tExplore P: 1.3065\n",
            "\n",
            "Episode 222\n",
            " \tTotal reward: -105.82331848144531\n",
            " \tTraining loss: 1.4329\n",
            " \tExplore P: 1.3058\n",
            "\n",
            "Episode 223\n",
            " \tTotal reward: -96.451171875\n",
            " \tTraining loss: 1.5510\n",
            " \tExplore P: 1.3050\n",
            "\n",
            "Episode 224\n",
            " \tTotal reward: -100.29435729980469\n",
            " \tTraining loss: 7.8842\n",
            " \tExplore P: 1.3043\n",
            "\n",
            "Episode 225\n",
            " \tTotal reward: -101.18637084960938\n",
            " \tTraining loss: 1.7436\n",
            " \tExplore P: 1.3036\n",
            "\n",
            "Model saved\n",
            "Episode 226\n",
            " \tTotal reward: -105.90565490722656\n",
            " \tTraining loss: 0.4328\n",
            " \tExplore P: 1.3020\n",
            "\n",
            "Episode 227\n",
            " \tTotal reward: -113.0750732421875\n",
            " \tTraining loss: 0.5548\n",
            " \tExplore P: 1.3002\n",
            "\n",
            "Episode 228\n",
            " \tTotal reward: -87.82888793945312\n",
            " \tTraining loss: 6.0935\n",
            " \tExplore P: 1.2980\n",
            "\n",
            "Episode 229\n",
            " \tTotal reward: -115.91546630859375\n",
            " \tTraining loss: 1.0815\n",
            " \tExplore P: 1.2958\n",
            "\n",
            "Episode 230\n",
            " \tTotal reward: -115.78831481933594\n",
            " \tTraining loss: 4.7043\n",
            " \tExplore P: 1.2951\n",
            "\n",
            "Model saved\n",
            "Episode 231\n",
            " \tTotal reward: -113.249755859375\n",
            " \tTraining loss: 15.3147\n",
            " \tExplore P: 1.2944\n",
            "\n",
            "Episode 232\n",
            " \tTotal reward: -112.34568786621094\n",
            " \tTraining loss: 0.4355\n",
            " \tExplore P: 1.2931\n",
            "\n",
            "Episode 233\n",
            " \tTotal reward: -115.88787841796875\n",
            " \tTraining loss: 0.5461\n",
            " \tExplore P: 1.2920\n",
            "\n",
            "Episode 234\n",
            " \tTotal reward: -115.21923828125\n",
            " \tTraining loss: 0.8511\n",
            " \tExplore P: 1.2904\n",
            "\n",
            "Episode 235\n",
            " \tTotal reward: -112.4598388671875\n",
            " \tTraining loss: 16.6185\n",
            " \tExplore P: 1.2898\n",
            "\n",
            "Model saved\n",
            "Episode 236\n",
            " \tTotal reward: -115.95159912109375\n",
            " \tTraining loss: 1.6946\n",
            " \tExplore P: 1.2877\n",
            "\n",
            "Episode 237\n",
            " \tTotal reward: -112.46102905273438\n",
            " \tTraining loss: 2.8973\n",
            " \tExplore P: 1.2866\n",
            "\n",
            "Episode 238\n",
            " \tTotal reward: -110.15789794921875\n",
            " \tTraining loss: 1.0082\n",
            " \tExplore P: 1.2855\n",
            "\n",
            "Episode 239\n",
            " \tTotal reward: -84.96484375\n",
            " \tTraining loss: 1.4346\n",
            " \tExplore P: 1.2843\n",
            "\n",
            "Episode 240\n",
            " \tTotal reward: -57.012451171875\n",
            " \tTraining loss: 1.1454\n",
            " \tExplore P: 1.2822\n",
            "\n",
            "Model saved\n",
            "Episode 241\n",
            " \tTotal reward: -106.87490844726562\n",
            " \tTraining loss: 0.9448\n",
            " \tExplore P: 1.2805\n",
            "\n",
            "Episode 242\n",
            " \tTotal reward: -115.91822814941406\n",
            " \tTraining loss: 0.9687\n",
            " \tExplore P: 1.2794\n",
            "\n",
            "Episode 243\n",
            " \tTotal reward: -115.39329528808594\n",
            " \tTraining loss: 2.8756\n",
            " \tExplore P: 1.2788\n",
            "\n",
            "Episode 244\n",
            " \tTotal reward: -96.41731262207031\n",
            " \tTraining loss: 2.6375\n",
            " \tExplore P: 1.2772\n",
            "\n",
            "Episode 245\n",
            " \tTotal reward: -111.98385620117188\n",
            " \tTraining loss: 2.2809\n",
            " \tExplore P: 1.2762\n",
            "\n",
            "Model saved\n",
            "Episode 246\n",
            " \tTotal reward: -111.39002990722656\n",
            " \tTraining loss: 5.1105\n",
            " \tExplore P: 1.2745\n",
            "\n",
            "Episode 247\n",
            " \tTotal reward: -96.41900634765625\n",
            " \tTraining loss: 3.9971\n",
            " \tExplore P: 1.2734\n",
            "\n",
            "Episode 248\n",
            " \tTotal reward: -113.00270080566406\n",
            " \tTraining loss: 2.7579\n",
            " \tExplore P: 1.2723\n",
            "\n",
            "Episode 249\n",
            " \tTotal reward: -101.10955810546875\n",
            " \tTraining loss: 1.1239\n",
            " \tExplore P: 1.2713\n",
            "\n",
            "Episode 250\n",
            " \tTotal reward: -104.2166748046875\n",
            " \tTraining loss: 2.4004\n",
            " \tExplore P: 1.2700\n",
            "\n",
            "Model saved\n",
            "Episode 251\n",
            " \tTotal reward: -114.1739501953125\n",
            " \tTraining loss: 0.4904\n",
            " \tExplore P: 1.2683\n",
            "\n",
            "Episode 252\n",
            " \tTotal reward: -106.86126708984375\n",
            " \tTraining loss: 1.1692\n",
            " \tExplore P: 1.2678\n",
            "\n",
            "Episode 253\n",
            " \tTotal reward: -105.78599548339844\n",
            " \tTraining loss: 14.7645\n",
            " \tExplore P: 1.2667\n",
            "\n",
            "Episode 254\n",
            " \tTotal reward: -74.27745056152344\n",
            " \tTraining loss: 1.5905\n",
            " \tExplore P: 1.2626\n",
            "\n",
            "Episode 255\n",
            " \tTotal reward: -110.00172424316406\n",
            " \tTraining loss: 1.2510\n",
            " \tExplore P: 1.2610\n",
            "\n",
            "Model saved\n",
            "Episode 256\n",
            " \tTotal reward: -103.8828125\n",
            " \tTraining loss: 0.6747\n",
            " \tExplore P: 1.2597\n",
            "\n",
            "Episode 257\n",
            " \tTotal reward: -115.979248046875\n",
            " \tTraining loss: 15.0254\n",
            " \tExplore P: 1.2587\n",
            "\n",
            "Episode 258\n",
            " \tTotal reward: -98.34541320800781\n",
            " \tTraining loss: 0.5516\n",
            " \tExplore P: 1.2578\n",
            "\n",
            "Episode 259\n",
            " \tTotal reward: -86.14935302734375\n",
            " \tTraining loss: 1.0806\n",
            " \tExplore P: 1.2558\n",
            "\n",
            "Episode 260\n",
            " \tTotal reward: -98.60554504394531\n",
            " \tTraining loss: 0.7326\n",
            " \tExplore P: 1.2548\n",
            "\n",
            "Model saved\n",
            "Episode 261\n",
            " \tTotal reward: -111.0164794921875\n",
            " \tTraining loss: 0.7718\n",
            " \tExplore P: 1.2538\n",
            "\n",
            "Episode 262\n",
            " \tTotal reward: -115.77261352539062\n",
            " \tTraining loss: 1.4265\n",
            " \tExplore P: 1.2527\n",
            "\n",
            "Episode 263\n",
            " \tTotal reward: -77.45243835449219\n",
            " \tTraining loss: 13.3334\n",
            " \tExplore P: 1.2521\n",
            "\n",
            "Episode 264\n",
            " \tTotal reward: -112.87631225585938\n",
            " \tTraining loss: 0.9788\n",
            " \tExplore P: 1.2511\n",
            "\n",
            "Episode 265\n",
            " \tTotal reward: -98.18107604980469\n",
            " \tTraining loss: 2.4520\n",
            " \tExplore P: 1.2501\n",
            "\n",
            "Model saved\n",
            "Episode 266\n",
            " \tTotal reward: -87.08601379394531\n",
            " \tTraining loss: 2.1913\n",
            " \tExplore P: 1.2492\n",
            "\n",
            "Episode 267\n",
            " \tTotal reward: -59.423919677734375\n",
            " \tTraining loss: 15.7218\n",
            " \tExplore P: 1.2477\n",
            "\n",
            "Episode 268\n",
            " \tTotal reward: -115.93647766113281\n",
            " \tTraining loss: 1.8672\n",
            " \tExplore P: 1.2472\n",
            "\n",
            "Episode 269\n",
            " \tTotal reward: -115.97625732421875\n",
            " \tTraining loss: 0.6645\n",
            " \tExplore P: 1.2466\n",
            "\n",
            "Episode 270\n",
            " \tTotal reward: -115.929931640625\n",
            " \tTraining loss: 1.5178\n",
            " \tExplore P: 1.2460\n",
            "\n",
            "Model saved\n",
            "Episode 271\n",
            " \tTotal reward: -96.97071838378906\n",
            " \tTraining loss: 12.0817\n",
            " \tExplore P: 1.2450\n",
            "\n",
            "Episode 272\n",
            " \tTotal reward: -115.62767028808594\n",
            " \tTraining loss: 4.7819\n",
            " \tExplore P: 1.2444\n",
            "\n",
            "Episode 273\n",
            " \tTotal reward: -107.95401000976562\n",
            " \tTraining loss: 0.6001\n",
            " \tExplore P: 1.2439\n",
            "\n",
            "Episode 274\n",
            " \tTotal reward: -115.06217956542969\n",
            " \tTraining loss: 0.9770\n",
            " \tExplore P: 1.2433\n",
            "\n",
            "Episode 275\n",
            " \tTotal reward: -87.1845703125\n",
            " \tTraining loss: 7.5436\n",
            " \tExplore P: 1.2424\n",
            "\n",
            "Model saved\n",
            "Episode 276\n",
            " \tTotal reward: -103.59761047363281\n",
            " \tTraining loss: 2.9231\n",
            " \tExplore P: 1.2394\n",
            "\n",
            "Episode 277\n",
            " \tTotal reward: -115.98886108398438\n",
            " \tTraining loss: 1.4961\n",
            " \tExplore P: 1.2382\n",
            "\n",
            "Episode 278\n",
            " \tTotal reward: -114.43795776367188\n",
            " \tTraining loss: 2.2826\n",
            " \tExplore P: 1.2365\n",
            "\n",
            "Episode 279\n",
            " \tTotal reward: -106.44198608398438\n",
            " \tTraining loss: 1.1459\n",
            " \tExplore P: 1.2355\n",
            "\n",
            "Episode 280\n",
            " \tTotal reward: -115.84765625\n",
            " \tTraining loss: 0.5032\n",
            " \tExplore P: 1.2339\n",
            "\n",
            "Model saved\n",
            "Episode 281\n",
            " \tTotal reward: -92.99082946777344\n",
            " \tTraining loss: 0.8512\n",
            " \tExplore P: 1.2329\n",
            "\n",
            "Episode 282\n",
            " \tTotal reward: -112.75009155273438\n",
            " \tTraining loss: 1.3403\n",
            " \tExplore P: 1.2319\n",
            "\n",
            "Episode 283\n",
            " \tTotal reward: -115.6866455078125\n",
            " \tTraining loss: 14.3278\n",
            " \tExplore P: 1.2311\n",
            "\n",
            "Episode 284\n",
            " \tTotal reward: -87.58888244628906\n",
            " \tTraining loss: 4.0892\n",
            " \tExplore P: 1.2298\n",
            "\n",
            "Episode 285\n",
            " \tTotal reward: -115.97576904296875\n",
            " \tTraining loss: 0.9033\n",
            " \tExplore P: 1.2289\n",
            "\n",
            "Model saved\n",
            "Episode 286\n",
            " \tTotal reward: -115.88313293457031\n",
            " \tTraining loss: 0.6360\n",
            " \tExplore P: 1.2267\n",
            "\n",
            "Episode 287\n",
            " \tTotal reward: -115.9141845703125\n",
            " \tTraining loss: 1.2073\n",
            " \tExplore P: 1.2258\n",
            "\n",
            "Episode 288\n",
            " \tTotal reward: -114.08457946777344\n",
            " \tTraining loss: 0.5922\n",
            " \tExplore P: 1.2245\n",
            "\n",
            "Episode 289\n",
            " \tTotal reward: -111.24070739746094\n",
            " \tTraining loss: 2.7447\n",
            " \tExplore P: 1.2240\n",
            "\n",
            "Model updated\n",
            "Episode 290\n",
            " \tTotal reward: -90.93356323242188\n",
            " \tTraining loss: 2.1047\n",
            " \tExplore P: 1.2229\n",
            "\n",
            "Model saved\n",
            "Episode 291\n",
            " \tTotal reward: -107.10464477539062\n",
            " \tTraining loss: 18.8605\n",
            " \tExplore P: 1.2218\n",
            "\n",
            "Episode 292\n",
            " \tTotal reward: -110.58587646484375\n",
            " \tTraining loss: 13.1744\n",
            " \tExplore P: 1.2208\n",
            "\n",
            "Episode 293\n",
            " \tTotal reward: -113.02023315429688\n",
            " \tTraining loss: 0.8837\n",
            " \tExplore P: 1.2203\n",
            "\n",
            "Episode 294\n",
            " \tTotal reward: -107.37892150878906\n",
            " \tTraining loss: 3.3851\n",
            " \tExplore P: 1.2198\n",
            "\n",
            "Episode 295\n",
            " \tTotal reward: -98.83930969238281\n",
            " \tTraining loss: 16.9167\n",
            " \tExplore P: 1.2190\n",
            "\n",
            "Model saved\n",
            "Episode 296\n",
            " \tTotal reward: -113.09022521972656\n",
            " \tTraining loss: 0.3981\n",
            " \tExplore P: 1.2181\n",
            "\n",
            "Episode 297\n",
            " \tTotal reward: -115.99716186523438\n",
            " \tTraining loss: 1.0673\n",
            " \tExplore P: 1.2176\n",
            "\n",
            "Episode 298\n",
            " \tTotal reward: -81.54435729980469\n",
            " \tTraining loss: 0.6558\n",
            " \tExplore P: 1.2164\n",
            "\n",
            "Episode 299\n",
            " \tTotal reward: -115.119873046875\n",
            " \tTraining loss: 0.7033\n",
            " \tExplore P: 1.2156\n",
            "\n",
            "Episode 300\n",
            " \tTotal reward: -88.38618469238281\n",
            " \tTraining loss: 1.8061\n",
            " \tExplore P: 1.2133\n",
            "\n",
            "Model saved\n",
            "Episode 301\n",
            " \tTotal reward: -112.03587341308594\n",
            " \tTraining loss: 1.4155\n",
            " \tExplore P: 1.2125\n",
            "\n",
            "Episode 302\n",
            " \tTotal reward: -85.83177185058594\n",
            " \tTraining loss: 12.4683\n",
            " \tExplore P: 1.2106\n",
            "\n",
            "Episode 303\n",
            " \tTotal reward: -114.59300231933594\n",
            " \tTraining loss: 2.6696\n",
            " \tExplore P: 1.2101\n",
            "\n",
            "Episode 304\n",
            " \tTotal reward: -100.23756408691406\n",
            " \tTraining loss: 1.4423\n",
            " \tExplore P: 1.2090\n",
            "\n",
            "Episode 305\n",
            " \tTotal reward: -112.04216003417969\n",
            " \tTraining loss: 1.9219\n",
            " \tExplore P: 1.2085\n",
            "\n",
            "Model saved\n",
            "Episode 306\n",
            " \tTotal reward: -99.7728271484375\n",
            " \tTraining loss: 2.1333\n",
            " \tExplore P: 1.2076\n",
            "\n",
            "Episode 307\n",
            " \tTotal reward: -113.19961547851562\n",
            " \tTraining loss: 1.9245\n",
            " \tExplore P: 1.2061\n",
            "\n",
            "Episode 308\n",
            " \tTotal reward: -112.232666015625\n",
            " \tTraining loss: 0.5678\n",
            " \tExplore P: 1.2056\n",
            "\n",
            "Episode 309\n",
            " \tTotal reward: -81.49331665039062\n",
            " \tTraining loss: 1.0568\n",
            " \tExplore P: 1.2044\n",
            "\n",
            "Episode 310\n",
            " \tTotal reward: -115.84268188476562\n",
            " \tTraining loss: 2.6004\n",
            " \tExplore P: 1.2039\n",
            "\n",
            "Model saved\n",
            "Episode 311\n",
            " \tTotal reward: -111.05043029785156\n",
            " \tTraining loss: 1.7335\n",
            " \tExplore P: 1.2029\n",
            "\n",
            "Episode 312\n",
            " \tTotal reward: -115.94235229492188\n",
            " \tTraining loss: 4.8436\n",
            " \tExplore P: 1.2017\n",
            "\n",
            "Episode 313\n",
            " \tTotal reward: -114.5108642578125\n",
            " \tTraining loss: 0.8200\n",
            " \tExplore P: 1.2013\n",
            "\n",
            "Episode 314\n",
            " \tTotal reward: -59.43693542480469\n",
            " \tTraining loss: 0.7530\n",
            " \tExplore P: 1.1995\n",
            "\n",
            "Episode 315\n",
            " \tTotal reward: -115.99867248535156\n",
            " \tTraining loss: 8.5081\n",
            " \tExplore P: 1.1990\n",
            "\n",
            "Model saved\n",
            "Episode 316\n",
            " \tTotal reward: -115.9779052734375\n",
            " \tTraining loss: 0.9531\n",
            " \tExplore P: 1.1980\n",
            "\n",
            "Episode 317\n",
            " \tTotal reward: -114.60154724121094\n",
            " \tTraining loss: 1.2031\n",
            " \tExplore P: 1.1967\n",
            "\n",
            "Episode 318\n",
            " \tTotal reward: -110.38630676269531\n",
            " \tTraining loss: 2.3986\n",
            " \tExplore P: 1.1954\n",
            "\n",
            "Episode 319\n",
            " \tTotal reward: -86.77342224121094\n",
            " \tTraining loss: 2.5685\n",
            " \tExplore P: 1.1943\n",
            "\n",
            "Episode 320\n",
            " \tTotal reward: -115.97872924804688\n",
            " \tTraining loss: 0.6194\n",
            " \tExplore P: 1.1932\n",
            "\n",
            "Model saved\n",
            "Episode 321\n",
            " \tTotal reward: -99.07257080078125\n",
            " \tTraining loss: 3.0997\n",
            " \tExplore P: 1.1921\n",
            "\n",
            "Episode 322\n",
            " \tTotal reward: -83.94720458984375\n",
            " \tTraining loss: 2.4213\n",
            " \tExplore P: 1.1914\n",
            "\n",
            "Episode 323\n",
            " \tTotal reward: -105.69471740722656\n",
            " \tTraining loss: 1.0296\n",
            " \tExplore P: 1.1904\n",
            "\n",
            "Episode 324\n",
            " \tTotal reward: -115.19316101074219\n",
            " \tTraining loss: 0.9344\n",
            " \tExplore P: 1.1891\n",
            "\n",
            "Episode 325\n",
            " \tTotal reward: -115.95318603515625\n",
            " \tTraining loss: 0.7106\n",
            " \tExplore P: 1.1886\n",
            "\n",
            "Model saved\n",
            "Episode 326\n",
            " \tTotal reward: -97.24118041992188\n",
            " \tTraining loss: 0.6468\n",
            " \tExplore P: 1.1870\n",
            "\n",
            "Episode 327\n",
            " \tTotal reward: -115.21994018554688\n",
            " \tTraining loss: 17.5002\n",
            " \tExplore P: 1.1863\n",
            "\n",
            "Episode 328\n",
            " \tTotal reward: -98.29890441894531\n",
            " \tTraining loss: 1.5445\n",
            " \tExplore P: 1.1856\n",
            "\n",
            "Episode 329\n",
            " \tTotal reward: -105.72616577148438\n",
            " \tTraining loss: 1.6490\n",
            " \tExplore P: 1.1848\n",
            "\n",
            "Episode 330\n",
            " \tTotal reward: -110.36395263671875\n",
            " \tTraining loss: 15.0673\n",
            " \tExplore P: 1.1835\n",
            "\n",
            "Model saved\n",
            "Episode 331\n",
            " \tTotal reward: -103.54476928710938\n",
            " \tTraining loss: 3.4328\n",
            " \tExplore P: 1.1827\n",
            "\n",
            "Episode 332\n",
            " \tTotal reward: -109.15553283691406\n",
            " \tTraining loss: 0.7075\n",
            " \tExplore P: 1.1821\n",
            "\n",
            "Episode 333\n",
            " \tTotal reward: -115.9774169921875\n",
            " \tTraining loss: 2.2787\n",
            " \tExplore P: 1.1813\n",
            "\n",
            "Episode 334\n",
            " \tTotal reward: -97.12457275390625\n",
            " \tTraining loss: 1.2472\n",
            " \tExplore P: 1.1803\n",
            "\n",
            "Episode 335\n",
            " \tTotal reward: -105.14109802246094\n",
            " \tTraining loss: 0.8851\n",
            " \tExplore P: 1.1793\n",
            "\n",
            "Model saved\n",
            "Episode 336\n",
            " \tTotal reward: -111.53201293945312\n",
            " \tTraining loss: 0.9301\n",
            " \tExplore P: 1.1783\n",
            "\n",
            "Episode 337\n",
            " \tTotal reward: -101.85997009277344\n",
            " \tTraining loss: 28.8566\n",
            " \tExplore P: 1.1774\n",
            "\n",
            "Episode 338\n",
            " \tTotal reward: -101.13050842285156\n",
            " \tTraining loss: 1.0620\n",
            " \tExplore P: 1.1758\n",
            "\n",
            "Episode 339\n",
            " \tTotal reward: -115.771728515625\n",
            " \tTraining loss: 1.1133\n",
            " \tExplore P: 1.1754\n",
            "\n",
            "Episode 340\n",
            " \tTotal reward: -115.99430847167969\n",
            " \tTraining loss: 0.6210\n",
            " \tExplore P: 1.1738\n",
            "\n",
            "Model saved\n",
            "Episode 341\n",
            " \tTotal reward: -97.695068359375\n",
            " \tTraining loss: 0.8202\n",
            " \tExplore P: 1.1727\n",
            "\n",
            "Episode 342\n",
            " \tTotal reward: -115.06645202636719\n",
            " \tTraining loss: 1.1234\n",
            " \tExplore P: 1.1720\n",
            "\n",
            "Episode 343\n",
            " \tTotal reward: -105.98258972167969\n",
            " \tTraining loss: 1.6593\n",
            " \tExplore P: 1.1710\n",
            "\n",
            "Episode 344\n",
            " \tTotal reward: -96.52859497070312\n",
            " \tTraining loss: 0.5296\n",
            " \tExplore P: 1.1703\n",
            "\n",
            "Episode 345\n",
            " \tTotal reward: -105.98081970214844\n",
            " \tTraining loss: 2.9805\n",
            " \tExplore P: 1.1673\n",
            "\n",
            "Model saved\n",
            "Episode 346\n",
            " \tTotal reward: -88.37043762207031\n",
            " \tTraining loss: 0.7435\n",
            " \tExplore P: 1.1667\n",
            "\n",
            "Episode 347\n",
            " \tTotal reward: -73.18466186523438\n",
            " \tTraining loss: 0.9191\n",
            " \tExplore P: 1.1648\n",
            "\n",
            "Episode 348\n",
            " \tTotal reward: -97.37910461425781\n",
            " \tTraining loss: 0.8289\n",
            " \tExplore P: 1.1639\n",
            "\n",
            "Episode 349\n",
            " \tTotal reward: -115.98789978027344\n",
            " \tTraining loss: 0.6775\n",
            " \tExplore P: 1.1633\n",
            "\n",
            "Episode 350\n",
            " \tTotal reward: -95.30735778808594\n",
            " \tTraining loss: 3.5951\n",
            " \tExplore P: 1.1626\n",
            "\n",
            "Model saved\n",
            "Episode 351\n",
            " \tTotal reward: -103.61332702636719\n",
            " \tTraining loss: 1.6184\n",
            " \tExplore P: 1.1613\n",
            "\n",
            "Episode 352\n",
            " \tTotal reward: -109.67234802246094\n",
            " \tTraining loss: 0.4660\n",
            " \tExplore P: 1.1607\n",
            "\n",
            "Episode 353\n",
            " \tTotal reward: -100.38587951660156\n",
            " \tTraining loss: 1.6444\n",
            " \tExplore P: 1.1597\n",
            "\n",
            "Episode 354\n",
            " \tTotal reward: -103.49980163574219\n",
            " \tTraining loss: 13.1126\n",
            " \tExplore P: 1.1585\n",
            "\n",
            "Episode 355\n",
            " \tTotal reward: -115.99644470214844\n",
            " \tTraining loss: 0.8053\n",
            " \tExplore P: 1.1581\n",
            "\n",
            "Model saved\n",
            "Episode 356\n",
            " \tTotal reward: -110.3046875\n",
            " \tTraining loss: 2.3822\n",
            " \tExplore P: 1.1572\n",
            "\n",
            "Episode 357\n",
            " \tTotal reward: -110.90750122070312\n",
            " \tTraining loss: 0.4862\n",
            " \tExplore P: 1.1566\n",
            "\n",
            "Episode 358\n",
            " \tTotal reward: -114.45578002929688\n",
            " \tTraining loss: 2.1033\n",
            " \tExplore P: 1.1560\n",
            "\n",
            "Episode 359\n",
            " \tTotal reward: -108.35984802246094\n",
            " \tTraining loss: 11.4455\n",
            " \tExplore P: 1.1551\n",
            "\n",
            "Episode 360\n",
            " \tTotal reward: -94.28433227539062\n",
            " \tTraining loss: 0.7753\n",
            " \tExplore P: 1.1545\n",
            "\n",
            "Model saved\n",
            "Episode 361\n",
            " \tTotal reward: -115.76246643066406\n",
            " \tTraining loss: 3.1416\n",
            " \tExplore P: 1.1539\n",
            "\n",
            "Episode 362\n",
            " \tTotal reward: -105.75143432617188\n",
            " \tTraining loss: 1.2692\n",
            " \tExplore P: 1.1530\n",
            "\n",
            "Episode 363\n",
            " \tTotal reward: -63.635345458984375\n",
            " \tTraining loss: 1.5240\n",
            " \tExplore P: 1.1516\n",
            "\n",
            "Episode 364\n",
            " \tTotal reward: -109.10328674316406\n",
            " \tTraining loss: 1.4124\n",
            " \tExplore P: 1.1505\n",
            "\n",
            "Episode 365\n",
            " \tTotal reward: -97.04803466796875\n",
            " \tTraining loss: 1.0331\n",
            " \tExplore P: 1.1499\n",
            "\n",
            "Model saved\n",
            "Episode 366\n",
            " \tTotal reward: -102.70199584960938\n",
            " \tTraining loss: 1.2567\n",
            " \tExplore P: 1.1493\n",
            "\n",
            "Episode 367\n",
            " \tTotal reward: -86.57940673828125\n",
            " \tTraining loss: 1.7476\n",
            " \tExplore P: 1.1487\n",
            "\n",
            "Episode 368\n",
            " \tTotal reward: -113.72026062011719\n",
            " \tTraining loss: 1.5220\n",
            " \tExplore P: 1.1482\n",
            "\n",
            "Episode 369\n",
            " \tTotal reward: -115.99935913085938\n",
            " \tTraining loss: 2.1636\n",
            " \tExplore P: 1.1479\n",
            "\n",
            "Episode 370\n",
            " \tTotal reward: -90.71994018554688\n",
            " \tTraining loss: 1.5333\n",
            " \tExplore P: 1.1475\n",
            "\n",
            "Model saved\n",
            "Episode 371\n",
            " \tTotal reward: -68.89768981933594\n",
            " \tTraining loss: 1.9036\n",
            " \tExplore P: 1.1470\n",
            "\n",
            "Episode 372\n",
            " \tTotal reward: -87.63703918457031\n",
            " \tTraining loss: 0.4206\n",
            " \tExplore P: 1.1453\n",
            "\n",
            "Episode 373\n",
            " \tTotal reward: -67.10960388183594\n",
            " \tTraining loss: 0.9716\n",
            " \tExplore P: 1.1447\n",
            "\n",
            "Episode 374\n",
            " \tTotal reward: -115.99935913085938\n",
            " \tTraining loss: 0.4468\n",
            " \tExplore P: 1.1439\n",
            "\n",
            "Episode 375\n",
            " \tTotal reward: -58.2139892578125\n",
            " \tTraining loss: 0.8043\n",
            " \tExplore P: 1.1416\n",
            "\n",
            "Model saved\n",
            "Episode 376\n",
            " \tTotal reward: -107.81312561035156\n",
            " \tTraining loss: 15.6667\n",
            " \tExplore P: 1.1411\n",
            "\n",
            "Episode 377\n",
            " \tTotal reward: -114.25862121582031\n",
            " \tTraining loss: 2.0914\n",
            " \tExplore P: 1.1403\n",
            "\n",
            "Episode 378\n",
            " \tTotal reward: -109.89144897460938\n",
            " \tTraining loss: 1.4782\n",
            " \tExplore P: 1.1397\n",
            "\n",
            "Episode 379\n",
            " \tTotal reward: -110.66897583007812\n",
            " \tTraining loss: 5.2422\n",
            " \tExplore P: 1.1392\n",
            "\n",
            "Episode 380\n",
            " \tTotal reward: -114.53729248046875\n",
            " \tTraining loss: 0.9827\n",
            " \tExplore P: 1.1389\n",
            "\n",
            "Model saved\n",
            "Episode 381\n",
            " \tTotal reward: -78.38365173339844\n",
            " \tTraining loss: 0.4445\n",
            " \tExplore P: 1.1384\n",
            "\n",
            "Episode 382\n",
            " \tTotal reward: -108.27310180664062\n",
            " \tTraining loss: 3.3483\n",
            " \tExplore P: 1.1374\n",
            "\n",
            "Episode 383\n",
            " \tTotal reward: -76.50984191894531\n",
            " \tTraining loss: 0.6751\n",
            " \tExplore P: 1.1364\n",
            "\n",
            "Episode 384\n",
            " \tTotal reward: -106.11747741699219\n",
            " \tTraining loss: 1.2468\n",
            " \tExplore P: 1.1357\n",
            "\n",
            "Model updated\n",
            "Episode 385\n",
            " \tTotal reward: -114.61175537109375\n",
            " \tTraining loss: 1.0759\n",
            " \tExplore P: 1.1349\n",
            "\n",
            "Model saved\n",
            "Episode 386\n",
            " \tTotal reward: -115.97434997558594\n",
            " \tTraining loss: 1.3718\n",
            " \tExplore P: 1.1344\n",
            "\n",
            "Episode 387\n",
            " \tTotal reward: -113.97793579101562\n",
            " \tTraining loss: 7.4883\n",
            " \tExplore P: 1.1333\n",
            "\n",
            "Episode 388\n",
            " \tTotal reward: -109.75028991699219\n",
            " \tTraining loss: 2.2198\n",
            " \tExplore P: 1.1330\n",
            "\n",
            "Episode 389\n",
            " \tTotal reward: -98.46726989746094\n",
            " \tTraining loss: 0.8639\n",
            " \tExplore P: 1.1324\n",
            "\n",
            "Episode 390\n",
            " \tTotal reward: -97.84786987304688\n",
            " \tTraining loss: 1.4149\n",
            " \tExplore P: 1.1317\n",
            "\n",
            "Model saved\n",
            "Episode 391\n",
            " \tTotal reward: -103.85568237304688\n",
            " \tTraining loss: 10.4785\n",
            " \tExplore P: 1.1312\n",
            "\n",
            "Episode 392\n",
            " \tTotal reward: -81.04014587402344\n",
            " \tTraining loss: 1.0024\n",
            " \tExplore P: 1.1307\n",
            "\n",
            "Episode 393\n",
            " \tTotal reward: -45.282012939453125\n",
            " \tTraining loss: 20.8274\n",
            " \tExplore P: 1.1291\n",
            "\n",
            "Episode 394\n",
            " \tTotal reward: -115.96098327636719\n",
            " \tTraining loss: 1.7939\n",
            " \tExplore P: 1.1288\n",
            "\n",
            "Episode 395\n",
            " \tTotal reward: -93.80006408691406\n",
            " \tTraining loss: 11.6408\n",
            " \tExplore P: 1.1283\n",
            "\n",
            "Model saved\n",
            "Episode 396\n",
            " \tTotal reward: -111.44696044921875\n",
            " \tTraining loss: 0.6794\n",
            " \tExplore P: 1.1276\n",
            "\n",
            "Episode 397\n",
            " \tTotal reward: -114.16395568847656\n",
            " \tTraining loss: 16.8274\n",
            " \tExplore P: 1.1268\n",
            "\n",
            "Episode 398\n",
            " \tTotal reward: -106.81134033203125\n",
            " \tTraining loss: 0.6895\n",
            " \tExplore P: 1.1261\n",
            "\n",
            "Episode 399\n",
            " \tTotal reward: -110.00286865234375\n",
            " \tTraining loss: 2.3881\n",
            " \tExplore P: 1.1254\n",
            "\n",
            "Episode 400\n",
            " \tTotal reward: -104.31507873535156\n",
            " \tTraining loss: 0.4037\n",
            " \tExplore P: 1.1251\n",
            "\n",
            "Model saved\n",
            "Episode 401\n",
            " \tTotal reward: -109.77597045898438\n",
            " \tTraining loss: 6.5492\n",
            " \tExplore P: 1.1244\n",
            "\n",
            "Episode 402\n",
            " \tTotal reward: -107.93887329101562\n",
            " \tTraining loss: 3.1437\n",
            " \tExplore P: 1.1237\n",
            "\n",
            "Episode 403\n",
            " \tTotal reward: -115.83157348632812\n",
            " \tTraining loss: 12.8076\n",
            " \tExplore P: 1.1234\n",
            "\n",
            "Episode 404\n",
            " \tTotal reward: -115.76553344726562\n",
            " \tTraining loss: 2.8901\n",
            " \tExplore P: 1.1229\n",
            "\n",
            "Episode 405\n",
            " \tTotal reward: -107.74592590332031\n",
            " \tTraining loss: 19.6896\n",
            " \tExplore P: 1.1222\n",
            "\n",
            "Model saved\n",
            "Episode 406\n",
            " \tTotal reward: -106.44178771972656\n",
            " \tTraining loss: 0.7362\n",
            " \tExplore P: 1.1217\n",
            "\n",
            "Episode 407\n",
            " \tTotal reward: -109.14155578613281\n",
            " \tTraining loss: 15.0162\n",
            " \tExplore P: 1.1212\n",
            "\n",
            "Episode 408\n",
            " \tTotal reward: -95.35198974609375\n",
            " \tTraining loss: 0.8524\n",
            " \tExplore P: 1.1207\n",
            "\n",
            "Episode 409\n",
            " \tTotal reward: -115.93016052246094\n",
            " \tTraining loss: 2.0292\n",
            " \tExplore P: 1.1204\n",
            "\n",
            "Episode 410\n",
            " \tTotal reward: -100.09799194335938\n",
            " \tTraining loss: 0.7409\n",
            " \tExplore P: 1.1199\n",
            "\n",
            "Model saved\n",
            "Episode 411\n",
            " \tTotal reward: -112.15449523925781\n",
            " \tTraining loss: 1.6918\n",
            " \tExplore P: 1.1195\n",
            "\n",
            "Episode 412\n",
            " \tTotal reward: -95.48817443847656\n",
            " \tTraining loss: 1.0897\n",
            " \tExplore P: 1.1188\n",
            "\n",
            "Episode 413\n",
            " \tTotal reward: -115.95094299316406\n",
            " \tTraining loss: 1.7597\n",
            " \tExplore P: 1.1180\n",
            "\n",
            "Episode 414\n",
            " \tTotal reward: -111.69216918945312\n",
            " \tTraining loss: 1.1068\n",
            " \tExplore P: 1.1178\n",
            "\n",
            "Episode 415\n",
            " \tTotal reward: -110.80070495605469\n",
            " \tTraining loss: 3.6880\n",
            " \tExplore P: 1.1173\n",
            "\n",
            "Model saved\n",
            "Episode 416\n",
            " \tTotal reward: -91.60507202148438\n",
            " \tTraining loss: 17.1787\n",
            " \tExplore P: 1.1164\n",
            "\n",
            "Episode 417\n",
            " \tTotal reward: -108.83869934082031\n",
            " \tTraining loss: 0.5880\n",
            " \tExplore P: 1.1160\n",
            "\n",
            "Episode 418\n",
            " \tTotal reward: -102.86227416992188\n",
            " \tTraining loss: 1.6423\n",
            " \tExplore P: 1.1151\n",
            "\n",
            "Episode 419\n",
            " \tTotal reward: -115.9927978515625\n",
            " \tTraining loss: 0.4914\n",
            " \tExplore P: 1.1147\n",
            "\n",
            "Episode 420\n",
            " \tTotal reward: -104.21316528320312\n",
            " \tTraining loss: 1.3608\n",
            " \tExplore P: 1.1144\n",
            "\n",
            "Model saved\n",
            "Episode 421\n",
            " \tTotal reward: -105.62820434570312\n",
            " \tTraining loss: 0.6719\n",
            " \tExplore P: 1.1137\n",
            "\n",
            "Episode 422\n",
            " \tTotal reward: -103.16249084472656\n",
            " \tTraining loss: 1.0354\n",
            " \tExplore P: 1.1133\n",
            "\n",
            "Episode 423\n",
            " \tTotal reward: -110.152099609375\n",
            " \tTraining loss: 1.1096\n",
            " \tExplore P: 1.1125\n",
            "\n",
            "Episode 424\n",
            " \tTotal reward: -115.85072326660156\n",
            " \tTraining loss: 0.7466\n",
            " \tExplore P: 1.1117\n",
            "\n",
            "Episode 425\n",
            " \tTotal reward: -112.92662048339844\n",
            " \tTraining loss: 1.2118\n",
            " \tExplore P: 1.1114\n",
            "\n",
            "Model saved\n",
            "Episode 426\n",
            " \tTotal reward: -111.13523864746094\n",
            " \tTraining loss: 0.9000\n",
            " \tExplore P: 1.1113\n",
            "\n",
            "Episode 427\n",
            " \tTotal reward: -115.86184692382812\n",
            " \tTraining loss: 3.6592\n",
            " \tExplore P: 1.1110\n",
            "\n",
            "Episode 428\n",
            " \tTotal reward: -115.76492309570312\n",
            " \tTraining loss: 0.6700\n",
            " \tExplore P: 1.1104\n",
            "\n",
            "Episode 429\n",
            " \tTotal reward: -110.66987609863281\n",
            " \tTraining loss: 1.7809\n",
            " \tExplore P: 1.1101\n",
            "\n",
            "Episode 430\n",
            " \tTotal reward: -77.82191467285156\n",
            " \tTraining loss: 2.1113\n",
            " \tExplore P: 1.1086\n",
            "\n",
            "Model saved\n",
            "Episode 431\n",
            " \tTotal reward: -98.20237731933594\n",
            " \tTraining loss: 1.0715\n",
            " \tExplore P: 1.1079\n",
            "\n",
            "Episode 432\n",
            " \tTotal reward: -106.03056335449219\n",
            " \tTraining loss: 0.7937\n",
            " \tExplore P: 1.1073\n",
            "\n",
            "Episode 433\n",
            " \tTotal reward: -107.32957458496094\n",
            " \tTraining loss: 1.3572\n",
            " \tExplore P: 1.1065\n",
            "\n",
            "Episode 434\n",
            " \tTotal reward: -113.91412353515625\n",
            " \tTraining loss: 0.5563\n",
            " \tExplore P: 1.1062\n",
            "\n",
            "Episode 435\n",
            " \tTotal reward: -93.87254333496094\n",
            " \tTraining loss: 1.4910\n",
            " \tExplore P: 1.1056\n",
            "\n",
            "Model saved\n",
            "Episode 436\n",
            " \tTotal reward: -115.97740173339844\n",
            " \tTraining loss: 2.0654\n",
            " \tExplore P: 1.1051\n",
            "\n",
            "Episode 437\n",
            " \tTotal reward: -113.705078125\n",
            " \tTraining loss: 1.5909\n",
            " \tExplore P: 1.1049\n",
            "\n",
            "Episode 438\n",
            " \tTotal reward: -80.54031372070312\n",
            " \tTraining loss: 0.9627\n",
            " \tExplore P: 1.1046\n",
            "\n",
            "Episode 439\n",
            " \tTotal reward: -2.20263671875\n",
            " \tTraining loss: 0.9131\n",
            " \tExplore P: 1.1040\n",
            "\n",
            "Episode 440\n",
            " \tTotal reward: -101.25736999511719\n",
            " \tTraining loss: 0.4270\n",
            " \tExplore P: 1.1036\n",
            "\n",
            "Model saved\n",
            "Episode 441\n",
            " \tTotal reward: -113.29153442382812\n",
            " \tTraining loss: 1.4140\n",
            " \tExplore P: 1.1032\n",
            "\n",
            "Episode 442\n",
            " \tTotal reward: -108.63981628417969\n",
            " \tTraining loss: 0.7833\n",
            " \tExplore P: 1.1028\n",
            "\n",
            "Episode 443\n",
            " \tTotal reward: -115.99935913085938\n",
            " \tTraining loss: 1.4888\n",
            " \tExplore P: 1.1024\n",
            "\n",
            "Episode 444\n",
            " \tTotal reward: -95.40357971191406\n",
            " \tTraining loss: 0.6748\n",
            " \tExplore P: 1.1019\n",
            "\n",
            "Episode 445\n",
            " \tTotal reward: -114.50299072265625\n",
            " \tTraining loss: 2.1092\n",
            " \tExplore P: 1.0994\n",
            "\n",
            "Model saved\n",
            "Episode 446\n",
            " \tTotal reward: -115.50909423828125\n",
            " \tTraining loss: 1.1555\n",
            " \tExplore P: 1.0990\n",
            "\n",
            "Episode 447\n",
            " \tTotal reward: -115.97869873046875\n",
            " \tTraining loss: 0.3170\n",
            " \tExplore P: 1.0986\n",
            "\n",
            "Episode 448\n",
            " \tTotal reward: -83.08940124511719\n",
            " \tTraining loss: 1.5511\n",
            " \tExplore P: 1.0980\n",
            "\n",
            "Episode 449\n",
            " \tTotal reward: -115.20381164550781\n",
            " \tTraining loss: 0.9240\n",
            " \tExplore P: 1.0976\n",
            "\n",
            "Episode 450\n",
            " \tTotal reward: -92.77769470214844\n",
            " \tTraining loss: 12.9436\n",
            " \tExplore P: 1.0969\n",
            "\n",
            "Model saved\n",
            "Episode 451\n",
            " \tTotal reward: -103.08273315429688\n",
            " \tTraining loss: 0.6121\n",
            " \tExplore P: 1.0965\n",
            "\n",
            "Episode 452\n",
            " \tTotal reward: -70.95747375488281\n",
            " \tTraining loss: 3.1517\n",
            " \tExplore P: 1.0953\n",
            "\n",
            "Episode 453\n",
            " \tTotal reward: -115.90052795410156\n",
            " \tTraining loss: 0.7562\n",
            " \tExplore P: 1.0948\n",
            "\n",
            "Episode 454\n",
            " \tTotal reward: -115.19491577148438\n",
            " \tTraining loss: 2.3373\n",
            " \tExplore P: 1.0944\n",
            "\n",
            "Episode 455\n",
            " \tTotal reward: -111.89967346191406\n",
            " \tTraining loss: 5.3044\n",
            " \tExplore P: 1.0940\n",
            "\n",
            "Model saved\n",
            "Episode 456\n",
            " \tTotal reward: -105.80976867675781\n",
            " \tTraining loss: 0.5380\n",
            " \tExplore P: 1.0936\n",
            "\n",
            "Episode 457\n",
            " \tTotal reward: -104.52349853515625\n",
            " \tTraining loss: 0.6877\n",
            " \tExplore P: 1.0928\n",
            "\n",
            "Episode 458\n",
            " \tTotal reward: -87.19837951660156\n",
            " \tTraining loss: 0.9855\n",
            " \tExplore P: 1.0926\n",
            "\n",
            "Episode 459\n",
            " \tTotal reward: -98.62962341308594\n",
            " \tTraining loss: 3.2749\n",
            " \tExplore P: 1.0918\n",
            "\n",
            "Episode 460\n",
            " \tTotal reward: -114.82597351074219\n",
            " \tTraining loss: 2.3175\n",
            " \tExplore P: 1.0915\n",
            "\n",
            "Model saved\n",
            "Episode 461\n",
            " \tTotal reward: -95.89578247070312\n",
            " \tTraining loss: 0.4887\n",
            " \tExplore P: 1.0910\n",
            "\n",
            "Episode 462\n",
            " \tTotal reward: -115.84016418457031\n",
            " \tTraining loss: 15.3804\n",
            " \tExplore P: 1.0907\n",
            "\n",
            "Episode 463\n",
            " \tTotal reward: -99.77716064453125\n",
            " \tTraining loss: 0.9908\n",
            " \tExplore P: 1.0901\n",
            "\n",
            "Episode 464\n",
            " \tTotal reward: -107.94908142089844\n",
            " \tTraining loss: 1.3105\n",
            " \tExplore P: 1.0899\n",
            "\n",
            "Episode 465\n",
            " \tTotal reward: -113.85298156738281\n",
            " \tTraining loss: 0.9490\n",
            " \tExplore P: 1.0893\n",
            "\n",
            "Model saved\n",
            "Episode 466\n",
            " \tTotal reward: -115.78744506835938\n",
            " \tTraining loss: 0.9539\n",
            " \tExplore P: 1.0891\n",
            "\n",
            "Episode 467\n",
            " \tTotal reward: -106.1683349609375\n",
            " \tTraining loss: 1.5882\n",
            " \tExplore P: 1.0886\n",
            "\n",
            "Episode 468\n",
            " \tTotal reward: -105.01620483398438\n",
            " \tTraining loss: 1.5605\n",
            " \tExplore P: 1.0884\n",
            "\n",
            "Episode 469\n",
            " \tTotal reward: -115.71730041503906\n",
            " \tTraining loss: 0.8990\n",
            " \tExplore P: 1.0880\n",
            "\n",
            "Episode 470\n",
            " \tTotal reward: -88.88066101074219\n",
            " \tTraining loss: 0.5891\n",
            " \tExplore P: 1.0878\n",
            "\n",
            "Model saved\n",
            "Episode 471\n",
            " \tTotal reward: -113.61019897460938\n",
            " \tTraining loss: 1.5784\n",
            " \tExplore P: 1.0873\n",
            "\n",
            "Episode 472\n",
            " \tTotal reward: -114.49974060058594\n",
            " \tTraining loss: 2.2658\n",
            " \tExplore P: 1.0868\n",
            "\n",
            "Episode 473\n",
            " \tTotal reward: -94.61846923828125\n",
            " \tTraining loss: 0.7888\n",
            " \tExplore P: 1.0865\n",
            "\n",
            "Episode 474\n",
            " \tTotal reward: -107.96913146972656\n",
            " \tTraining loss: 0.9264\n",
            " \tExplore P: 1.0863\n",
            "\n",
            "Episode 475\n",
            " \tTotal reward: -93.27581787109375\n",
            " \tTraining loss: 0.8952\n",
            " \tExplore P: 1.0858\n",
            "\n",
            "Model saved\n",
            "Episode 476\n",
            " \tTotal reward: -85.94828796386719\n",
            " \tTraining loss: 0.7697\n",
            " \tExplore P: 1.0853\n",
            "\n",
            "Episode 477\n",
            " \tTotal reward: -91.23756408691406\n",
            " \tTraining loss: 0.8733\n",
            " \tExplore P: 1.0847\n",
            "\n",
            "Episode 478\n",
            " \tTotal reward: -115.55564880371094\n",
            " \tTraining loss: 13.9337\n",
            " \tExplore P: 1.0841\n",
            "\n",
            "Episode 479\n",
            " \tTotal reward: -101.37033081054688\n",
            " \tTraining loss: 3.9855\n",
            " \tExplore P: 1.0837\n",
            "\n",
            "Episode 480\n",
            " \tTotal reward: -86.5841064453125\n",
            " \tTraining loss: 0.4390\n",
            " \tExplore P: 1.0833\n",
            "\n",
            "Model saved\n",
            "Episode 481\n",
            " \tTotal reward: -115.99644470214844\n",
            " \tTraining loss: 0.9362\n",
            " \tExplore P: 1.0830\n",
            "\n",
            "Episode 482\n",
            " \tTotal reward: -113.20269775390625\n",
            " \tTraining loss: 0.8381\n",
            " \tExplore P: 1.0827\n",
            "\n",
            "Episode 483\n",
            " \tTotal reward: -107.84941101074219\n",
            " \tTraining loss: 0.9391\n",
            " \tExplore P: 1.0821\n",
            "\n",
            "Model updated\n",
            "Episode 484\n",
            " \tTotal reward: -115.17607116699219\n",
            " \tTraining loss: 0.9735\n",
            " \tExplore P: 1.0817\n",
            "\n",
            "Episode 485\n",
            " \tTotal reward: -108.13053894042969\n",
            " \tTraining loss: 2.4750\n",
            " \tExplore P: 1.0813\n",
            "\n",
            "Model saved\n",
            "Episode 486\n",
            " \tTotal reward: -104.90180969238281\n",
            " \tTraining loss: 1.2286\n",
            " \tExplore P: 1.0808\n",
            "\n",
            "Episode 487\n",
            " \tTotal reward: -90.09245300292969\n",
            " \tTraining loss: 1.1531\n",
            " \tExplore P: 1.0804\n",
            "\n",
            "Episode 488\n",
            " \tTotal reward: -85.72409057617188\n",
            " \tTraining loss: 1.9305\n",
            " \tExplore P: 1.0799\n",
            "\n",
            "Episode 489\n",
            " \tTotal reward: -108.63768005371094\n",
            " \tTraining loss: 5.2624\n",
            " \tExplore P: 1.0792\n",
            "\n",
            "Episode 490\n",
            " \tTotal reward: -66.33953857421875\n",
            " \tTraining loss: 1.4501\n",
            " \tExplore P: 1.0782\n",
            "\n",
            "Model saved\n",
            "Episode 491\n",
            " \tTotal reward: -115.9774169921875\n",
            " \tTraining loss: 1.3638\n",
            " \tExplore P: 1.0781\n",
            "\n",
            "Episode 492\n",
            " \tTotal reward: -115.98944091796875\n",
            " \tTraining loss: 0.8181\n",
            " \tExplore P: 1.0777\n",
            "\n",
            "Episode 493\n",
            " \tTotal reward: -114.71614074707031\n",
            " \tTraining loss: 1.2010\n",
            " \tExplore P: 1.0774\n",
            "\n",
            "Episode 494\n",
            " \tTotal reward: -115.97987365722656\n",
            " \tTraining loss: 2.1588\n",
            " \tExplore P: 1.0772\n",
            "\n",
            "Episode 495\n",
            " \tTotal reward: -115.81707763671875\n",
            " \tTraining loss: 1.6098\n",
            " \tExplore P: 1.0767\n",
            "\n",
            "Model saved\n",
            "Episode 496\n",
            " \tTotal reward: -104.36122131347656\n",
            " \tTraining loss: 0.5821\n",
            " \tExplore P: 1.0763\n",
            "\n",
            "Episode 497\n",
            " \tTotal reward: -88.02815246582031\n",
            " \tTraining loss: 1.1295\n",
            " \tExplore P: 1.0760\n",
            "\n",
            "Episode 498\n",
            " \tTotal reward: -102.38912963867188\n",
            " \tTraining loss: 0.9163\n",
            " \tExplore P: 1.0757\n",
            "\n",
            "Episode 499\n",
            " \tTotal reward: -102.12828063964844\n",
            " \tTraining loss: 0.9753\n",
            " \tExplore P: 1.0756\n",
            "\n",
            "Episode 500\n",
            " \tTotal reward: -94.02574157714844\n",
            " \tTraining loss: 1.2651\n",
            " \tExplore P: 1.0751\n",
            "\n",
            "Model saved\n",
            "Episode 501\n",
            " \tTotal reward: -102.76002502441406\n",
            " \tTraining loss: 1.1525\n",
            " \tExplore P: 1.0749\n",
            "\n",
            "Episode 502\n",
            " \tTotal reward: -109.62635803222656\n",
            " \tTraining loss: 1.3189\n",
            " \tExplore P: 1.0745\n",
            "\n",
            "Episode 503\n",
            " \tTotal reward: -92.98136901855469\n",
            " \tTraining loss: 0.7065\n",
            " \tExplore P: 1.0741\n",
            "\n",
            "Episode 504\n",
            " \tTotal reward: -108.15896606445312\n",
            " \tTraining loss: 0.9797\n",
            " \tExplore P: 1.0739\n",
            "\n",
            "Episode 505\n",
            " \tTotal reward: -102.58285522460938\n",
            " \tTraining loss: 2.1603\n",
            " \tExplore P: 1.0736\n",
            "\n",
            "Model saved\n",
            "Episode 506\n",
            " \tTotal reward: -68.83100891113281\n",
            " \tTraining loss: 12.7140\n",
            " \tExplore P: 1.0731\n",
            "\n",
            "Episode 507\n",
            " \tTotal reward: -114.81074523925781\n",
            " \tTraining loss: 1.6780\n",
            " \tExplore P: 1.0728\n",
            "\n",
            "Episode 508\n",
            " \tTotal reward: -105.59599304199219\n",
            " \tTraining loss: 1.1355\n",
            " \tExplore P: 1.0725\n",
            "\n",
            "Episode 509\n",
            " \tTotal reward: -114.96356201171875\n",
            " \tTraining loss: 1.7852\n",
            " \tExplore P: 1.0723\n",
            "\n",
            "Episode 510\n",
            " \tTotal reward: -113.50390625\n",
            " \tTraining loss: 6.4557\n",
            " \tExplore P: 1.0720\n",
            "\n",
            "Model saved\n",
            "Episode 511\n",
            " \tTotal reward: -79.82009887695312\n",
            " \tTraining loss: 18.1744\n",
            " \tExplore P: 1.0717\n",
            "\n",
            "Episode 512\n",
            " \tTotal reward: -111.94853210449219\n",
            " \tTraining loss: 0.5860\n",
            " \tExplore P: 1.0715\n",
            "\n",
            "Episode 513\n",
            " \tTotal reward: -69.81645202636719\n",
            " \tTraining loss: 1.0864\n",
            " \tExplore P: 1.0711\n",
            "\n",
            "Episode 514\n",
            " \tTotal reward: -107.136474609375\n",
            " \tTraining loss: 0.6660\n",
            " \tExplore P: 1.0708\n",
            "\n",
            "Episode 515\n",
            " \tTotal reward: -115.34719848632812\n",
            " \tTraining loss: 2.0050\n",
            " \tExplore P: 1.0706\n",
            "\n",
            "Model saved\n",
            "Episode 516\n",
            " \tTotal reward: -115.95535278320312\n",
            " \tTraining loss: 11.1459\n",
            " \tExplore P: 1.0701\n",
            "\n",
            "Episode 517\n",
            " \tTotal reward: -97.59378051757812\n",
            " \tTraining loss: 1.1104\n",
            " \tExplore P: 1.0696\n",
            "\n",
            "Episode 518\n",
            " \tTotal reward: -108.90599060058594\n",
            " \tTraining loss: 0.8779\n",
            " \tExplore P: 1.0694\n",
            "\n",
            "Episode 519\n",
            " \tTotal reward: -101.48783874511719\n",
            " \tTraining loss: 0.8316\n",
            " \tExplore P: 1.0689\n",
            "\n",
            "Episode 520\n",
            " \tTotal reward: -110.35009765625\n",
            " \tTraining loss: 12.6280\n",
            " \tExplore P: 1.0684\n",
            "\n",
            "Model saved\n",
            "Episode 521\n",
            " \tTotal reward: -105.50222778320312\n",
            " \tTraining loss: 2.1363\n",
            " \tExplore P: 1.0678\n",
            "\n",
            "Episode 522\n",
            " \tTotal reward: -103.89927673339844\n",
            " \tTraining loss: 0.8969\n",
            " \tExplore P: 1.0676\n",
            "\n",
            "Episode 523\n",
            " \tTotal reward: -111.09635925292969\n",
            " \tTraining loss: 9.3161\n",
            " \tExplore P: 1.0675\n",
            "\n",
            "Episode 524\n",
            " \tTotal reward: -96.33818054199219\n",
            " \tTraining loss: 0.7975\n",
            " \tExplore P: 1.0672\n",
            "\n",
            "Episode 525\n",
            " \tTotal reward: -114.51251220703125\n",
            " \tTraining loss: 9.6249\n",
            " \tExplore P: 1.0669\n",
            "\n",
            "Model saved\n",
            "Episode 526\n",
            " \tTotal reward: -114.6083984375\n",
            " \tTraining loss: 1.1242\n",
            " \tExplore P: 1.0666\n",
            "\n",
            "Episode 527\n",
            " \tTotal reward: -115.41729736328125\n",
            " \tTraining loss: 1.9704\n",
            " \tExplore P: 1.0665\n",
            "\n",
            "Episode 528\n",
            " \tTotal reward: -101.41952514648438\n",
            " \tTraining loss: 1.0123\n",
            " \tExplore P: 1.0661\n",
            "\n",
            "Episode 529\n",
            " \tTotal reward: -105.54464721679688\n",
            " \tTraining loss: 0.8989\n",
            " \tExplore P: 1.0658\n",
            "\n",
            "Episode 530\n",
            " \tTotal reward: -103.24961853027344\n",
            " \tTraining loss: 4.4260\n",
            " \tExplore P: 1.0656\n",
            "\n",
            "Model saved\n",
            "Episode 531\n",
            " \tTotal reward: -68.69664001464844\n",
            " \tTraining loss: 2.2115\n",
            " \tExplore P: 1.0652\n",
            "\n",
            "Episode 532\n",
            " \tTotal reward: -115.99647521972656\n",
            " \tTraining loss: 0.8180\n",
            " \tExplore P: 1.0650\n",
            "\n",
            "Episode 533\n",
            " \tTotal reward: -113.06346130371094\n",
            " \tTraining loss: 0.7456\n",
            " \tExplore P: 1.0647\n",
            "\n",
            "Episode 534\n",
            " \tTotal reward: -85.57872009277344\n",
            " \tTraining loss: 3.6344\n",
            " \tExplore P: 1.0643\n",
            "\n",
            "Episode 535\n",
            " \tTotal reward: -93.40301513671875\n",
            " \tTraining loss: 11.4913\n",
            " \tExplore P: 1.0636\n",
            "\n",
            "Model saved\n",
            "Episode 536\n",
            " \tTotal reward: -111.86708068847656\n",
            " \tTraining loss: 1.6720\n",
            " \tExplore P: 1.0631\n",
            "\n",
            "Episode 537\n",
            " \tTotal reward: -114.21206665039062\n",
            " \tTraining loss: 0.6734\n",
            " \tExplore P: 1.0626\n",
            "\n",
            "Episode 538\n",
            " \tTotal reward: -101.56808471679688\n",
            " \tTraining loss: 1.4637\n",
            " \tExplore P: 1.0623\n",
            "\n",
            "Episode 539\n",
            " \tTotal reward: -111.42507934570312\n",
            " \tTraining loss: 0.7373\n",
            " \tExplore P: 1.0612\n",
            "\n",
            "Episode 540\n",
            " \tTotal reward: -115.75514221191406\n",
            " \tTraining loss: 3.3370\n",
            " \tExplore P: 1.0611\n",
            "\n",
            "Model saved\n",
            "Episode 541\n",
            " \tTotal reward: -113.86192321777344\n",
            " \tTraining loss: 1.9553\n",
            " \tExplore P: 1.0609\n",
            "\n",
            "Episode 542\n",
            " \tTotal reward: -102.84942626953125\n",
            " \tTraining loss: 12.3748\n",
            " \tExplore P: 1.0604\n",
            "\n",
            "Episode 543\n",
            " \tTotal reward: -103.66050720214844\n",
            " \tTraining loss: 0.9516\n",
            " \tExplore P: 1.0598\n",
            "\n",
            "Episode 544\n",
            " \tTotal reward: -92.90315246582031\n",
            " \tTraining loss: 1.5418\n",
            " \tExplore P: 1.0597\n",
            "\n",
            "Episode 545\n",
            " \tTotal reward: -109.47251892089844\n",
            " \tTraining loss: 0.3362\n",
            " \tExplore P: 1.0595\n",
            "\n",
            "Model saved\n",
            "Episode 546\n",
            " \tTotal reward: -94.70378112792969\n",
            " \tTraining loss: 1.4434\n",
            " \tExplore P: 1.0592\n",
            "\n",
            "Episode 547\n",
            " \tTotal reward: -100.22193908691406\n",
            " \tTraining loss: 0.6085\n",
            " \tExplore P: 1.0590\n",
            "\n",
            "Episode 548\n",
            " \tTotal reward: -108.85496520996094\n",
            " \tTraining loss: 2.0433\n",
            " \tExplore P: 1.0588\n",
            "\n",
            "Episode 549\n",
            " \tTotal reward: -114.49517822265625\n",
            " \tTraining loss: 1.1320\n",
            " \tExplore P: 1.0586\n",
            "\n",
            "Episode 550\n",
            " \tTotal reward: -104.03013610839844\n",
            " \tTraining loss: 1.3279\n",
            " \tExplore P: 1.0582\n",
            "\n",
            "Model saved\n",
            "Episode 551\n",
            " \tTotal reward: -105.87776184082031\n",
            " \tTraining loss: 7.2426\n",
            " \tExplore P: 1.0581\n",
            "\n",
            "Episode 552\n",
            " \tTotal reward: -115.97688293457031\n",
            " \tTraining loss: 0.9862\n",
            " \tExplore P: 1.0580\n",
            "\n",
            "Episode 553\n",
            " \tTotal reward: -108.40768432617188\n",
            " \tTraining loss: 0.6249\n",
            " \tExplore P: 1.0575\n",
            "\n",
            "Episode 554\n",
            " \tTotal reward: -113.14277648925781\n",
            " \tTraining loss: 12.0408\n",
            " \tExplore P: 1.0573\n",
            "\n",
            "Episode 555\n",
            " \tTotal reward: -109.25502014160156\n",
            " \tTraining loss: 0.8041\n",
            " \tExplore P: 1.0571\n",
            "\n",
            "Model saved\n",
            "Episode 556\n",
            " \tTotal reward: -112.37100219726562\n",
            " \tTraining loss: 1.0373\n",
            " \tExplore P: 1.0569\n",
            "\n",
            "Episode 557\n",
            " \tTotal reward: -113.45793151855469\n",
            " \tTraining loss: 0.8342\n",
            " \tExplore P: 1.0567\n",
            "\n",
            "Episode 558\n",
            " \tTotal reward: -106.77891540527344\n",
            " \tTraining loss: 0.4605\n",
            " \tExplore P: 1.0563\n",
            "\n",
            "Episode 559\n",
            " \tTotal reward: -106.46304321289062\n",
            " \tTraining loss: 2.4723\n",
            " \tExplore P: 1.0562\n",
            "\n",
            "Episode 560\n",
            " \tTotal reward: -110.55574035644531\n",
            " \tTraining loss: 2.2231\n",
            " \tExplore P: 1.0556\n",
            "\n",
            "Model saved\n",
            "Episode 561\n",
            " \tTotal reward: -114.65913391113281\n",
            " \tTraining loss: 1.2453\n",
            " \tExplore P: 1.0554\n",
            "\n",
            "Episode 562\n",
            " \tTotal reward: -103.35881042480469\n",
            " \tTraining loss: 0.6688\n",
            " \tExplore P: 1.0550\n",
            "\n",
            "Episode 563\n",
            " \tTotal reward: -90.81590270996094\n",
            " \tTraining loss: 1.6579\n",
            " \tExplore P: 1.0547\n",
            "\n",
            "Episode 564\n",
            " \tTotal reward: -115.85151672363281\n",
            " \tTraining loss: 19.0681\n",
            " \tExplore P: 1.0545\n",
            "\n",
            "Episode 565\n",
            " \tTotal reward: -95.96005249023438\n",
            " \tTraining loss: 0.7921\n",
            " \tExplore P: 1.0544\n",
            "\n",
            "Model saved\n",
            "Episode 566\n",
            " \tTotal reward: -115.84291076660156\n",
            " \tTraining loss: 0.5146\n",
            " \tExplore P: 1.0542\n",
            "\n",
            "Episode 567\n",
            " \tTotal reward: -87.45799255371094\n",
            " \tTraining loss: 1.7005\n",
            " \tExplore P: 1.0539\n",
            "\n",
            "Episode 568\n",
            " \tTotal reward: -115.99856567382812\n",
            " \tTraining loss: 0.4786\n",
            " \tExplore P: 1.0536\n",
            "\n",
            "Episode 569\n",
            " \tTotal reward: -92.391845703125\n",
            " \tTraining loss: 1.0669\n",
            " \tExplore P: 1.0535\n",
            "\n",
            "Episode 570\n",
            " \tTotal reward: -96.2879638671875\n",
            " \tTraining loss: 0.9368\n",
            " \tExplore P: 1.0532\n",
            "\n",
            "Model saved\n",
            "Episode 571\n",
            " \tTotal reward: -115.59649658203125\n",
            " \tTraining loss: 0.9411\n",
            " \tExplore P: 1.0528\n",
            "\n",
            "Episode 572\n",
            " \tTotal reward: -107.31414794921875\n",
            " \tTraining loss: 1.1458\n",
            " \tExplore P: 1.0525\n",
            "\n",
            "Episode 573\n",
            " \tTotal reward: -114.08164978027344\n",
            " \tTraining loss: 2.2056\n",
            " \tExplore P: 1.0522\n",
            "\n",
            "Episode 574\n",
            " \tTotal reward: -105.70698547363281\n",
            " \tTraining loss: 0.8157\n",
            " \tExplore P: 1.0520\n",
            "\n",
            "Episode 575\n",
            " \tTotal reward: -91.57463073730469\n",
            " \tTraining loss: 16.1306\n",
            " \tExplore P: 1.0517\n",
            "\n",
            "Model saved\n",
            "Episode 576\n",
            " \tTotal reward: -115.69145202636719\n",
            " \tTraining loss: 1.2793\n",
            " \tExplore P: 1.0515\n",
            "\n",
            "Episode 577\n",
            " \tTotal reward: -107.85317993164062\n",
            " \tTraining loss: 1.3819\n",
            " \tExplore P: 1.0513\n",
            "\n",
            "Episode 578\n",
            " \tTotal reward: -106.48744201660156\n",
            " \tTraining loss: 1.3064\n",
            " \tExplore P: 1.0512\n",
            "\n",
            "Episode 579\n",
            " \tTotal reward: -115.977294921875\n",
            " \tTraining loss: 1.4164\n",
            " \tExplore P: 1.0508\n",
            "\n",
            "Episode 580\n",
            " \tTotal reward: -106.35281372070312\n",
            " \tTraining loss: 13.8808\n",
            " \tExplore P: 1.0506\n",
            "\n",
            "Model saved\n",
            "Episode 581\n",
            " \tTotal reward: -68.2589111328125\n",
            " \tTraining loss: 0.8632\n",
            " \tExplore P: 1.0504\n",
            "\n",
            "Episode 582\n",
            " \tTotal reward: -87.96321105957031\n",
            " \tTraining loss: 1.1816\n",
            " \tExplore P: 1.0501\n",
            "\n",
            "Episode 583\n",
            " \tTotal reward: -106.60684204101562\n",
            " \tTraining loss: 0.9285\n",
            " \tExplore P: 1.0499\n",
            "\n",
            "Model updated\n",
            "Episode 584\n",
            " \tTotal reward: -71.22523498535156\n",
            " \tTraining loss: 0.5377\n",
            " \tExplore P: 1.0495\n",
            "\n",
            "Episode 585\n",
            " \tTotal reward: -95.5003662109375\n",
            " \tTraining loss: 0.7270\n",
            " \tExplore P: 1.0492\n",
            "\n",
            "Model saved\n",
            "Episode 586\n",
            " \tTotal reward: -112.69851684570312\n",
            " \tTraining loss: 1.4634\n",
            " \tExplore P: 1.0489\n",
            "\n",
            "Episode 587\n",
            " \tTotal reward: -115.95252990722656\n",
            " \tTraining loss: 1.7814\n",
            " \tExplore P: 1.0488\n",
            "\n",
            "Episode 588\n",
            " \tTotal reward: -115.11801147460938\n",
            " \tTraining loss: 0.9110\n",
            " \tExplore P: 1.0485\n",
            "\n",
            "Episode 589\n",
            " \tTotal reward: -95.63612365722656\n",
            " \tTraining loss: 8.3720\n",
            " \tExplore P: 1.0482\n",
            "\n",
            "Episode 590\n",
            " \tTotal reward: -104.77037048339844\n",
            " \tTraining loss: 0.8286\n",
            " \tExplore P: 1.0478\n",
            "\n",
            "Model saved\n",
            "Episode 591\n",
            " \tTotal reward: -115.96029663085938\n",
            " \tTraining loss: 1.1226\n",
            " \tExplore P: 1.0476\n",
            "\n",
            "Episode 592\n",
            " \tTotal reward: -64.70166015625\n",
            " \tTraining loss: 0.9846\n",
            " \tExplore P: 1.0472\n",
            "\n",
            "Episode 593\n",
            " \tTotal reward: -106.57891845703125\n",
            " \tTraining loss: 4.8927\n",
            " \tExplore P: 1.0470\n",
            "\n",
            "Episode 594\n",
            " \tTotal reward: -111.34326171875\n",
            " \tTraining loss: 1.2021\n",
            " \tExplore P: 1.0466\n",
            "\n",
            "Episode 595\n",
            " \tTotal reward: -99.63246154785156\n",
            " \tTraining loss: 0.5377\n",
            " \tExplore P: 1.0464\n",
            "\n",
            "Model saved\n",
            "Episode 596\n",
            " \tTotal reward: -114.12425231933594\n",
            " \tTraining loss: 1.4158\n",
            " \tExplore P: 1.0463\n",
            "\n",
            "Episode 597\n",
            " \tTotal reward: -104.76263427734375\n",
            " \tTraining loss: 11.0220\n",
            " \tExplore P: 1.0461\n",
            "\n",
            "Episode 598\n",
            " \tTotal reward: -110.60481262207031\n",
            " \tTraining loss: 1.6000\n",
            " \tExplore P: 1.0460\n",
            "\n",
            "Episode 599\n",
            " \tTotal reward: -109.26449584960938\n",
            " \tTraining loss: 0.7073\n",
            " \tExplore P: 1.0459\n",
            "\n",
            "Episode 600\n",
            " \tTotal reward: -115.21803283691406\n",
            " \tTraining loss: 1.2232\n",
            " \tExplore P: 1.0457\n",
            "\n",
            "Model saved\n",
            "Episode 601\n",
            " \tTotal reward: -108.38528442382812\n",
            " \tTraining loss: 0.7902\n",
            " \tExplore P: 1.0456\n",
            "\n",
            "Episode 602\n",
            " \tTotal reward: -109.42979431152344\n",
            " \tTraining loss: 0.6116\n",
            " \tExplore P: 1.0453\n",
            "\n",
            "Episode 603\n",
            " \tTotal reward: -107.91987609863281\n",
            " \tTraining loss: 1.6949\n",
            " \tExplore P: 1.0451\n",
            "\n",
            "Episode 604\n",
            " \tTotal reward: -106.04254150390625\n",
            " \tTraining loss: 0.7356\n",
            " \tExplore P: 1.0448\n",
            "\n",
            "Episode 605\n",
            " \tTotal reward: -108.68359375\n",
            " \tTraining loss: 0.6226\n",
            " \tExplore P: 1.0447\n",
            "\n",
            "Model saved\n",
            "Episode 606\n",
            " \tTotal reward: -56.733306884765625\n",
            " \tTraining loss: 0.8700\n",
            " \tExplore P: 1.0440\n",
            "\n",
            "Episode 607\n",
            " \tTotal reward: -102.57684326171875\n",
            " \tTraining loss: 5.3137\n",
            " \tExplore P: 1.0439\n",
            "\n",
            "Episode 608\n",
            " \tTotal reward: -110.2535400390625\n",
            " \tTraining loss: 0.5916\n",
            " \tExplore P: 1.0437\n",
            "\n",
            "Episode 609\n",
            " \tTotal reward: -106.62612915039062\n",
            " \tTraining loss: 0.5464\n",
            " \tExplore P: 1.0434\n",
            "\n",
            "Episode 610\n",
            " \tTotal reward: -92.03123474121094\n",
            " \tTraining loss: 1.2805\n",
            " \tExplore P: 1.0433\n",
            "\n",
            "Model saved\n",
            "Episode 611\n",
            " \tTotal reward: -108.44583129882812\n",
            " \tTraining loss: 0.7826\n",
            " \tExplore P: 1.0432\n",
            "\n",
            "Episode 612\n",
            " \tTotal reward: -75.94459533691406\n",
            " \tTraining loss: 4.5468\n",
            " \tExplore P: 1.0429\n",
            "\n",
            "Episode 613\n",
            " \tTotal reward: -103.60025024414062\n",
            " \tTraining loss: 0.6606\n",
            " \tExplore P: 1.0428\n",
            "\n",
            "Episode 614\n",
            " \tTotal reward: -115.99226379394531\n",
            " \tTraining loss: 0.5067\n",
            " \tExplore P: 1.0425\n",
            "\n",
            "Episode 615\n",
            " \tTotal reward: -100.9283447265625\n",
            " \tTraining loss: 0.6092\n",
            " \tExplore P: 1.0422\n",
            "\n",
            "Model saved\n",
            "Episode 616\n",
            " \tTotal reward: -110.13816833496094\n",
            " \tTraining loss: 0.9276\n",
            " \tExplore P: 1.0420\n",
            "\n",
            "Episode 617\n",
            " \tTotal reward: -109.84202575683594\n",
            " \tTraining loss: 0.9135\n",
            " \tExplore P: 1.0418\n",
            "\n",
            "Episode 618\n",
            " \tTotal reward: -75.58486938476562\n",
            " \tTraining loss: 1.0598\n",
            " \tExplore P: 1.0415\n",
            "\n",
            "Episode 619\n",
            " \tTotal reward: -114.49803161621094\n",
            " \tTraining loss: 0.6025\n",
            " \tExplore P: 1.0412\n",
            "\n",
            "Episode 620\n",
            " \tTotal reward: -102.67068481445312\n",
            " \tTraining loss: 0.8629\n",
            " \tExplore P: 1.0409\n",
            "\n",
            "Model saved\n",
            "Episode 621\n",
            " \tTotal reward: -102.90310668945312\n",
            " \tTraining loss: 0.5492\n",
            " \tExplore P: 1.0407\n",
            "\n",
            "Episode 622\n",
            " \tTotal reward: -91.19647216796875\n",
            " \tTraining loss: 12.0641\n",
            " \tExplore P: 1.0405\n",
            "\n",
            "Episode 623\n",
            " \tTotal reward: -115.97682189941406\n",
            " \tTraining loss: 1.6251\n",
            " \tExplore P: 1.0403\n",
            "\n",
            "Episode 624\n",
            " \tTotal reward: -41.57037353515625\n",
            " \tTraining loss: 0.5579\n",
            " \tExplore P: 1.0397\n",
            "\n",
            "Episode 625\n",
            " \tTotal reward: -113.99288940429688\n",
            " \tTraining loss: 0.7254\n",
            " \tExplore P: 1.0395\n",
            "\n",
            "Model saved\n",
            "Episode 626\n",
            " \tTotal reward: -110.11917114257812\n",
            " \tTraining loss: 3.0169\n",
            " \tExplore P: 1.0393\n",
            "\n",
            "Episode 627\n",
            " \tTotal reward: -79.15739440917969\n",
            " \tTraining loss: 0.3905\n",
            " \tExplore P: 1.0392\n",
            "\n",
            "Episode 628\n",
            " \tTotal reward: -107.74107360839844\n",
            " \tTraining loss: 10.5310\n",
            " \tExplore P: 1.0390\n",
            "\n",
            "Episode 629\n",
            " \tTotal reward: -104.76193237304688\n",
            " \tTraining loss: 0.6182\n",
            " \tExplore P: 1.0388\n",
            "\n",
            "Episode 630\n",
            " \tTotal reward: -114.74142456054688\n",
            " \tTraining loss: 7.7990\n",
            " \tExplore P: 1.0387\n",
            "\n",
            "Model saved\n",
            "Episode 631\n",
            " \tTotal reward: -115.77784729003906\n",
            " \tTraining loss: 0.4156\n",
            " \tExplore P: 1.0386\n",
            "\n",
            "Episode 632\n",
            " \tTotal reward: -91.17112731933594\n",
            " \tTraining loss: 0.6793\n",
            " \tExplore P: 1.0384\n",
            "\n",
            "Episode 633\n",
            " \tTotal reward: -98.15919494628906\n",
            " \tTraining loss: 1.8936\n",
            " \tExplore P: 1.0383\n",
            "\n",
            "Episode 634\n",
            " \tTotal reward: -88.51835632324219\n",
            " \tTraining loss: 0.5893\n",
            " \tExplore P: 1.0381\n",
            "\n",
            "Episode 635\n",
            " \tTotal reward: -91.23809814453125\n",
            " \tTraining loss: 12.0549\n",
            " \tExplore P: 1.0379\n",
            "\n",
            "Model saved\n",
            "Episode 636\n",
            " \tTotal reward: -115.99859619140625\n",
            " \tTraining loss: 2.1745\n",
            " \tExplore P: 1.0378\n",
            "\n",
            "Episode 637\n",
            " \tTotal reward: -111.78048706054688\n",
            " \tTraining loss: 2.2396\n",
            " \tExplore P: 1.0376\n",
            "\n",
            "Episode 638\n",
            " \tTotal reward: -114.71199035644531\n",
            " \tTraining loss: 1.0288\n",
            " \tExplore P: 1.0375\n",
            "\n",
            "Episode 639\n",
            " \tTotal reward: -89.76486206054688\n",
            " \tTraining loss: 1.1849\n",
            " \tExplore P: 1.0374\n",
            "\n",
            "Episode 640\n",
            " \tTotal reward: -107.74867248535156\n",
            " \tTraining loss: 0.4168\n",
            " \tExplore P: 1.0372\n",
            "\n",
            "Model saved\n",
            "Episode 641\n",
            " \tTotal reward: -108.02516174316406\n",
            " \tTraining loss: 13.0298\n",
            " \tExplore P: 1.0370\n",
            "\n",
            "Episode 642\n",
            " \tTotal reward: -60.71563720703125\n",
            " \tTraining loss: 15.5268\n",
            " \tExplore P: 1.0365\n",
            "\n",
            "Episode 643\n",
            " \tTotal reward: -109.69441223144531\n",
            " \tTraining loss: 0.9941\n",
            " \tExplore P: 1.0364\n",
            "\n",
            "Episode 644\n",
            " \tTotal reward: -108.43898010253906\n",
            " \tTraining loss: 1.1626\n",
            " \tExplore P: 1.0362\n",
            "\n",
            "Episode 645\n",
            " \tTotal reward: -115.94635009765625\n",
            " \tTraining loss: 1.5902\n",
            " \tExplore P: 1.0361\n",
            "\n",
            "Model saved\n",
            "Episode 646\n",
            " \tTotal reward: -100.18025207519531\n",
            " \tTraining loss: 1.1372\n",
            " \tExplore P: 1.0359\n",
            "\n",
            "Episode 647\n",
            " \tTotal reward: -113.86279296875\n",
            " \tTraining loss: 1.0009\n",
            " \tExplore P: 1.0358\n",
            "\n",
            "Episode 648\n",
            " \tTotal reward: -115.86373901367188\n",
            " \tTraining loss: 0.6627\n",
            " \tExplore P: 1.0356\n",
            "\n",
            "Episode 649\n",
            " \tTotal reward: -107.54536437988281\n",
            " \tTraining loss: 0.5148\n",
            " \tExplore P: 1.0355\n",
            "\n",
            "Episode 650\n",
            " \tTotal reward: -78.15312194824219\n",
            " \tTraining loss: 0.7068\n",
            " \tExplore P: 1.0350\n",
            "\n",
            "Model saved\n",
            "Episode 651\n",
            " \tTotal reward: -107.43072509765625\n",
            " \tTraining loss: 2.1970\n",
            " \tExplore P: 1.0349\n",
            "\n",
            "Episode 652\n",
            " \tTotal reward: -113.53875732421875\n",
            " \tTraining loss: 2.2510\n",
            " \tExplore P: 1.0348\n",
            "\n",
            "Episode 653\n",
            " \tTotal reward: -94.19039916992188\n",
            " \tTraining loss: 1.4694\n",
            " \tExplore P: 1.0346\n",
            "\n",
            "Episode 654\n",
            " \tTotal reward: -115.72763061523438\n",
            " \tTraining loss: 11.9864\n",
            " \tExplore P: 1.0344\n",
            "\n",
            "Episode 655\n",
            " \tTotal reward: -79.77964782714844\n",
            " \tTraining loss: 1.1958\n",
            " \tExplore P: 1.0342\n",
            "\n",
            "Model saved\n",
            "Episode 656\n",
            " \tTotal reward: -104.92276000976562\n",
            " \tTraining loss: 2.2417\n",
            " \tExplore P: 1.0342\n",
            "\n",
            "Episode 657\n",
            " \tTotal reward: -71.22093200683594\n",
            " \tTraining loss: 1.0107\n",
            " \tExplore P: 1.0339\n",
            "\n",
            "Episode 658\n",
            " \tTotal reward: -104.76020812988281\n",
            " \tTraining loss: 2.3086\n",
            " \tExplore P: 1.0338\n",
            "\n",
            "Episode 659\n",
            " \tTotal reward: -115.95719909667969\n",
            " \tTraining loss: 1.6947\n",
            " \tExplore P: 1.0337\n",
            "\n",
            "Episode 660\n",
            " \tTotal reward: -111.21078491210938\n",
            " \tTraining loss: 0.6023\n",
            " \tExplore P: 1.0334\n",
            "\n",
            "Model saved\n",
            "Episode 661\n",
            " \tTotal reward: -102.20303344726562\n",
            " \tTraining loss: 2.6441\n",
            " \tExplore P: 1.0332\n",
            "\n",
            "Episode 662\n",
            " \tTotal reward: -107.19471740722656\n",
            " \tTraining loss: 11.3815\n",
            " \tExplore P: 1.0332\n",
            "\n",
            "Episode 663\n",
            " \tTotal reward: -82.44012451171875\n",
            " \tTraining loss: 9.7728\n",
            " \tExplore P: 1.0329\n",
            "\n",
            "Episode 664\n",
            " \tTotal reward: -105.76646423339844\n",
            " \tTraining loss: 0.6615\n",
            " \tExplore P: 1.0328\n",
            "\n",
            "Episode 665\n",
            " \tTotal reward: -115.89482116699219\n",
            " \tTraining loss: 5.3803\n",
            " \tExplore P: 1.0328\n",
            "\n",
            "Model saved\n",
            "Episode 666\n",
            " \tTotal reward: -89.40899658203125\n",
            " \tTraining loss: 0.9859\n",
            " \tExplore P: 1.0326\n",
            "\n",
            "Episode 667\n",
            " \tTotal reward: -100.56631469726562\n",
            " \tTraining loss: 0.6106\n",
            " \tExplore P: 1.0324\n",
            "\n",
            "Episode 668\n",
            " \tTotal reward: -115.95538330078125\n",
            " \tTraining loss: 2.0346\n",
            " \tExplore P: 1.0323\n",
            "\n",
            "Episode 669\n",
            " \tTotal reward: -105.04899597167969\n",
            " \tTraining loss: 10.5683\n",
            " \tExplore P: 1.0322\n",
            "\n",
            "Episode 670\n",
            " \tTotal reward: -99.68556213378906\n",
            " \tTraining loss: 0.7022\n",
            " \tExplore P: 1.0320\n",
            "\n",
            "Model saved\n",
            "Episode 671\n",
            " \tTotal reward: -109.12847900390625\n",
            " \tTraining loss: 1.4835\n",
            " \tExplore P: 1.0319\n",
            "\n",
            "Episode 672\n",
            " \tTotal reward: -115.9114990234375\n",
            " \tTraining loss: 0.8585\n",
            " \tExplore P: 1.0318\n",
            "\n",
            "Episode 673\n",
            " \tTotal reward: -105.34860229492188\n",
            " \tTraining loss: 1.4531\n",
            " \tExplore P: 1.0316\n",
            "\n",
            "Episode 674\n",
            " \tTotal reward: -105.33413696289062\n",
            " \tTraining loss: 0.9607\n",
            " \tExplore P: 1.0314\n",
            "\n",
            "Episode 675\n",
            " \tTotal reward: -95.16850280761719\n",
            " \tTraining loss: 1.4052\n",
            " \tExplore P: 1.0312\n",
            "\n",
            "Model saved\n",
            "Episode 676\n",
            " \tTotal reward: -93.14974975585938\n",
            " \tTraining loss: 0.9591\n",
            " \tExplore P: 1.0311\n",
            "\n",
            "Episode 677\n",
            " \tTotal reward: -110.0533447265625\n",
            " \tTraining loss: 1.1561\n",
            " \tExplore P: 1.0309\n",
            "\n",
            "Episode 678\n",
            " \tTotal reward: -109.74617004394531\n",
            " \tTraining loss: 3.3647\n",
            " \tExplore P: 1.0308\n",
            "\n",
            "Episode 679\n",
            " \tTotal reward: -86.71633911132812\n",
            " \tTraining loss: 0.7210\n",
            " \tExplore P: 1.0306\n",
            "\n",
            "Episode 680\n",
            " \tTotal reward: -91.26686096191406\n",
            " \tTraining loss: 0.3240\n",
            " \tExplore P: 1.0305\n",
            "\n",
            "Model saved\n",
            "Episode 681\n",
            " \tTotal reward: -115.77153015136719\n",
            " \tTraining loss: 1.8109\n",
            " \tExplore P: 1.0304\n",
            "\n",
            "Episode 682\n",
            " \tTotal reward: -109.3055419921875\n",
            " \tTraining loss: 0.6112\n",
            " \tExplore P: 1.0303\n",
            "\n",
            "Model updated\n",
            "Episode 683\n",
            " \tTotal reward: -113.7547607421875\n",
            " \tTraining loss: 0.8591\n",
            " \tExplore P: 1.0301\n",
            "\n",
            "Episode 684\n",
            " \tTotal reward: -106.40342712402344\n",
            " \tTraining loss: 1.3363\n",
            " \tExplore P: 1.0300\n",
            "\n",
            "Episode 685\n",
            " \tTotal reward: -115.34950256347656\n",
            " \tTraining loss: 1.1342\n",
            " \tExplore P: 1.0299\n",
            "\n",
            "Model saved\n",
            "Episode 686\n",
            " \tTotal reward: -115.99856567382812\n",
            " \tTraining loss: 3.2671\n",
            " \tExplore P: 1.0298\n",
            "\n",
            "Episode 687\n",
            " \tTotal reward: -115.93470764160156\n",
            " \tTraining loss: 1.2940\n",
            " \tExplore P: 1.0298\n",
            "\n",
            "Episode 688\n",
            " \tTotal reward: -115.97840881347656\n",
            " \tTraining loss: 12.4337\n",
            " \tExplore P: 1.0297\n",
            "\n",
            "Episode 689\n",
            " \tTotal reward: -112.81581115722656\n",
            " \tTraining loss: 2.6467\n",
            " \tExplore P: 1.0296\n",
            "\n",
            "Episode 690\n",
            " \tTotal reward: -102.9891357421875\n",
            " \tTraining loss: 3.3827\n",
            " \tExplore P: 1.0294\n",
            "\n",
            "Model saved\n",
            "Episode 691\n",
            " \tTotal reward: -84.58953857421875\n",
            " \tTraining loss: 1.0658\n",
            " \tExplore P: 1.0292\n",
            "\n",
            "Episode 692\n",
            " \tTotal reward: -115.90322875976562\n",
            " \tTraining loss: 1.1044\n",
            " \tExplore P: 1.0291\n",
            "\n",
            "Episode 693\n",
            " \tTotal reward: -95.28215026855469\n",
            " \tTraining loss: 2.1334\n",
            " \tExplore P: 1.0289\n",
            "\n",
            "Episode 694\n",
            " \tTotal reward: -83.82341003417969\n",
            " \tTraining loss: 0.7563\n",
            " \tExplore P: 1.0287\n",
            "\n",
            "Episode 695\n",
            " \tTotal reward: -105.09107971191406\n",
            " \tTraining loss: 7.5147\n",
            " \tExplore P: 1.0285\n",
            "\n",
            "Model saved\n",
            "Episode 696\n",
            " \tTotal reward: -84.07742309570312\n",
            " \tTraining loss: 1.6104\n",
            " \tExplore P: 1.0282\n",
            "\n",
            "Episode 697\n",
            " \tTotal reward: -97.94638061523438\n",
            " \tTraining loss: 1.5529\n",
            " \tExplore P: 1.0281\n",
            "\n",
            "Episode 698\n",
            " \tTotal reward: -94.79843139648438\n",
            " \tTraining loss: 0.6286\n",
            " \tExplore P: 1.0279\n",
            "\n",
            "Episode 699\n",
            " \tTotal reward: -115.96969604492188\n",
            " \tTraining loss: 4.5741\n",
            " \tExplore P: 1.0278\n",
            "\n",
            "Episode 700\n",
            " \tTotal reward: -104.875732421875\n",
            " \tTraining loss: 0.7640\n",
            " \tExplore P: 1.0277\n",
            "\n",
            "Model saved\n",
            "Episode 701\n",
            " \tTotal reward: -113.56355285644531\n",
            " \tTraining loss: 2.0028\n",
            " \tExplore P: 1.0276\n",
            "\n",
            "Episode 702\n",
            " \tTotal reward: -108.75776672363281\n",
            " \tTraining loss: 3.5123\n",
            " \tExplore P: 1.0275\n",
            "\n",
            "Episode 703\n",
            " \tTotal reward: -113.78941345214844\n",
            " \tTraining loss: 11.4881\n",
            " \tExplore P: 1.0274\n",
            "\n",
            "Episode 704\n",
            " \tTotal reward: -99.99691772460938\n",
            " \tTraining loss: 3.1614\n",
            " \tExplore P: 1.0274\n",
            "\n",
            "Episode 705\n",
            " \tTotal reward: -113.82113647460938\n",
            " \tTraining loss: 11.2330\n",
            " \tExplore P: 1.0272\n",
            "\n",
            "Model saved\n",
            "Episode 706\n",
            " \tTotal reward: -115.99786376953125\n",
            " \tTraining loss: 0.7936\n",
            " \tExplore P: 1.0271\n",
            "\n",
            "Episode 707\n",
            " \tTotal reward: -109.78439331054688\n",
            " \tTraining loss: 0.7121\n",
            " \tExplore P: 1.0270\n",
            "\n",
            "Episode 708\n",
            " \tTotal reward: -94.48048400878906\n",
            " \tTraining loss: 2.4837\n",
            " \tExplore P: 1.0269\n",
            "\n",
            "Episode 709\n",
            " \tTotal reward: -112.85726928710938\n",
            " \tTraining loss: 1.9052\n",
            " \tExplore P: 1.0267\n",
            "\n",
            "Episode 710\n",
            " \tTotal reward: -115.58067321777344\n",
            " \tTraining loss: 0.9272\n",
            " \tExplore P: 1.0266\n",
            "\n",
            "Model saved\n",
            "Episode 711\n",
            " \tTotal reward: -110.50666809082031\n",
            " \tTraining loss: 3.5617\n",
            " \tExplore P: 1.0265\n",
            "\n",
            "Episode 712\n",
            " \tTotal reward: -90.90408325195312\n",
            " \tTraining loss: 0.6268\n",
            " \tExplore P: 1.0264\n",
            "\n",
            "Episode 713\n",
            " \tTotal reward: -89.40863037109375\n",
            " \tTraining loss: 1.1344\n",
            " \tExplore P: 1.0263\n",
            "\n",
            "Episode 714\n",
            " \tTotal reward: -114.82640075683594\n",
            " \tTraining loss: 0.5955\n",
            " \tExplore P: 1.0262\n",
            "\n",
            "Episode 715\n",
            " \tTotal reward: -115.67434692382812\n",
            " \tTraining loss: 1.5810\n",
            " \tExplore P: 1.0261\n",
            "\n",
            "Model saved\n",
            "Episode 716\n",
            " \tTotal reward: -100.81352233886719\n",
            " \tTraining loss: 1.0839\n",
            " \tExplore P: 1.0260\n",
            "\n",
            "Episode 717\n",
            " \tTotal reward: -94.11337280273438\n",
            " \tTraining loss: 9.8521\n",
            " \tExplore P: 1.0259\n",
            "\n",
            "Episode 718\n",
            " \tTotal reward: -114.87385559082031\n",
            " \tTraining loss: 1.9625\n",
            " \tExplore P: 1.0258\n",
            "\n",
            "Episode 719\n",
            " \tTotal reward: -95.98135375976562\n",
            " \tTraining loss: 1.3824\n",
            " \tExplore P: 1.0257\n",
            "\n",
            "Episode 720\n",
            " \tTotal reward: -103.65277099609375\n",
            " \tTraining loss: 1.5253\n",
            " \tExplore P: 1.0256\n",
            "\n",
            "Model saved\n",
            "Episode 721\n",
            " \tTotal reward: -115.39834594726562\n",
            " \tTraining loss: 1.3109\n",
            " \tExplore P: 1.0255\n",
            "\n",
            "Episode 722\n",
            " \tTotal reward: -115.21884155273438\n",
            " \tTraining loss: 2.0385\n",
            " \tExplore P: 1.0253\n",
            "\n",
            "Episode 723\n",
            " \tTotal reward: -76.17568969726562\n",
            " \tTraining loss: 0.7872\n",
            " \tExplore P: 1.0252\n",
            "\n",
            "Episode 724\n",
            " \tTotal reward: -111.60549926757812\n",
            " \tTraining loss: 0.7618\n",
            " \tExplore P: 1.0250\n",
            "\n",
            "Episode 725\n",
            " \tTotal reward: -107.68626403808594\n",
            " \tTraining loss: 1.1796\n",
            " \tExplore P: 1.0249\n",
            "\n",
            "Model saved\n",
            "Episode 726\n",
            " \tTotal reward: -115.96470642089844\n",
            " \tTraining loss: 2.2247\n",
            " \tExplore P: 1.0248\n",
            "\n",
            "Episode 727\n",
            " \tTotal reward: -98.57972717285156\n",
            " \tTraining loss: 2.1579\n",
            " \tExplore P: 1.0246\n",
            "\n",
            "Episode 728\n",
            " \tTotal reward: -100.60163879394531\n",
            " \tTraining loss: 0.8545\n",
            " \tExplore P: 1.0246\n",
            "\n",
            "Episode 729\n",
            " \tTotal reward: -115.318603515625\n",
            " \tTraining loss: 1.4866\n",
            " \tExplore P: 1.0244\n",
            "\n",
            "Episode 730\n",
            " \tTotal reward: -114.66714477539062\n",
            " \tTraining loss: 0.6690\n",
            " \tExplore P: 1.0242\n",
            "\n",
            "Model saved\n",
            "Episode 731\n",
            " \tTotal reward: -113.12071228027344\n",
            " \tTraining loss: 3.8665\n",
            " \tExplore P: 1.0241\n",
            "\n",
            "Episode 732\n",
            " \tTotal reward: -114.27310180664062\n",
            " \tTraining loss: 2.8584\n",
            " \tExplore P: 1.0240\n",
            "\n",
            "Episode 733\n",
            " \tTotal reward: -76.00798034667969\n",
            " \tTraining loss: 0.7943\n",
            " \tExplore P: 1.0239\n",
            "\n",
            "Episode 734\n",
            " \tTotal reward: -99.9429931640625\n",
            " \tTraining loss: 0.9603\n",
            " \tExplore P: 1.0238\n",
            "\n",
            "Episode 735\n",
            " \tTotal reward: -114.82479858398438\n",
            " \tTraining loss: 1.1419\n",
            " \tExplore P: 1.0236\n",
            "\n",
            "Model saved\n",
            "Episode 736\n",
            " \tTotal reward: -112.03280639648438\n",
            " \tTraining loss: 1.6533\n",
            " \tExplore P: 1.0235\n",
            "\n",
            "Episode 737\n",
            " \tTotal reward: -101.14582824707031\n",
            " \tTraining loss: 7.7583\n",
            " \tExplore P: 1.0234\n",
            "\n",
            "Episode 738\n",
            " \tTotal reward: -115.84162902832031\n",
            " \tTraining loss: 2.5074\n",
            " \tExplore P: 1.0233\n",
            "\n",
            "Episode 739\n",
            " \tTotal reward: -95.34947204589844\n",
            " \tTraining loss: 1.3826\n",
            " \tExplore P: 1.0233\n",
            "\n",
            "Episode 740\n",
            " \tTotal reward: -96.17289733886719\n",
            " \tTraining loss: 1.1897\n",
            " \tExplore P: 1.0231\n",
            "\n",
            "Model saved\n",
            "Episode 741\n",
            " \tTotal reward: -114.19664001464844\n",
            " \tTraining loss: 7.5011\n",
            " \tExplore P: 1.0230\n",
            "\n",
            "Episode 742\n",
            " \tTotal reward: -115.52375793457031\n",
            " \tTraining loss: 0.7373\n",
            " \tExplore P: 1.0229\n",
            "\n",
            "Episode 743\n",
            " \tTotal reward: -115.96992492675781\n",
            " \tTraining loss: 0.6327\n",
            " \tExplore P: 1.0227\n",
            "\n",
            "Episode 744\n",
            " \tTotal reward: -101.9281005859375\n",
            " \tTraining loss: 1.6333\n",
            " \tExplore P: 1.0226\n",
            "\n",
            "Episode 745\n",
            " \tTotal reward: -110.2066650390625\n",
            " \tTraining loss: 10.2637\n",
            " \tExplore P: 1.0225\n",
            "\n",
            "Model saved\n",
            "Episode 746\n",
            " \tTotal reward: -106.76217651367188\n",
            " \tTraining loss: 0.5431\n",
            " \tExplore P: 1.0224\n",
            "\n",
            "Episode 747\n",
            " \tTotal reward: -97.65858459472656\n",
            " \tTraining loss: 1.2455\n",
            " \tExplore P: 1.0223\n",
            "\n",
            "Episode 748\n",
            " \tTotal reward: -114.05403137207031\n",
            " \tTraining loss: 1.2905\n",
            " \tExplore P: 1.0223\n",
            "\n",
            "Episode 749\n",
            " \tTotal reward: -102.12796020507812\n",
            " \tTraining loss: 0.9659\n",
            " \tExplore P: 1.0221\n",
            "\n",
            "Episode 750\n",
            " \tTotal reward: -97.29090881347656\n",
            " \tTraining loss: 1.1913\n",
            " \tExplore P: 1.0220\n",
            "\n",
            "Model saved\n",
            "Episode 751\n",
            " \tTotal reward: -113.65460205078125\n",
            " \tTraining loss: 0.5247\n",
            " \tExplore P: 1.0219\n",
            "\n",
            "Episode 752\n",
            " \tTotal reward: -115.99787902832031\n",
            " \tTraining loss: 10.0319\n",
            " \tExplore P: 1.0218\n",
            "\n",
            "Episode 753\n",
            " \tTotal reward: -111.0469970703125\n",
            " \tTraining loss: 0.6552\n",
            " \tExplore P: 1.0217\n",
            "\n",
            "Episode 754\n",
            " \tTotal reward: -87.40693664550781\n",
            " \tTraining loss: 0.9899\n",
            " \tExplore P: 1.0216\n",
            "\n",
            "Episode 755\n",
            " \tTotal reward: -95.80184936523438\n",
            " \tTraining loss: 2.0854\n",
            " \tExplore P: 1.0215\n",
            "\n",
            "Model saved\n",
            "Episode 756\n",
            " \tTotal reward: -97.05001831054688\n",
            " \tTraining loss: 16.9364\n",
            " \tExplore P: 1.0215\n",
            "\n",
            "Episode 757\n",
            " \tTotal reward: -96.84559631347656\n",
            " \tTraining loss: 0.7510\n",
            " \tExplore P: 1.0214\n",
            "\n",
            "Episode 758\n",
            " \tTotal reward: -77.05101013183594\n",
            " \tTraining loss: 2.2277\n",
            " \tExplore P: 1.0213\n",
            "\n",
            "Episode 759\n",
            " \tTotal reward: -112.48361206054688\n",
            " \tTraining loss: 1.1065\n",
            " \tExplore P: 1.0212\n",
            "\n",
            "Episode 760\n",
            " \tTotal reward: -110.47090148925781\n",
            " \tTraining loss: 1.0317\n",
            " \tExplore P: 1.0211\n",
            "\n",
            "Model saved\n",
            "Episode 761\n",
            " \tTotal reward: -94.10919189453125\n",
            " \tTraining loss: 1.4237\n",
            " \tExplore P: 1.0210\n",
            "\n",
            "Episode 762\n",
            " \tTotal reward: -93.40647888183594\n",
            " \tTraining loss: 0.9880\n",
            " \tExplore P: 1.0210\n",
            "\n",
            "Episode 763\n",
            " \tTotal reward: -108.29988098144531\n",
            " \tTraining loss: 1.2076\n",
            " \tExplore P: 1.0209\n",
            "\n",
            "Episode 764\n",
            " \tTotal reward: -100.29605102539062\n",
            " \tTraining loss: 0.8397\n",
            " \tExplore P: 1.0209\n",
            "\n",
            "Episode 765\n",
            " \tTotal reward: -107.24240112304688\n",
            " \tTraining loss: 0.6147\n",
            " \tExplore P: 1.0207\n",
            "\n",
            "Model saved\n",
            "Episode 766\n",
            " \tTotal reward: -107.21138000488281\n",
            " \tTraining loss: 1.0704\n",
            " \tExplore P: 1.0206\n",
            "\n",
            "Episode 767\n",
            " \tTotal reward: -87.79412841796875\n",
            " \tTraining loss: 0.4512\n",
            " \tExplore P: 1.0205\n",
            "\n",
            "Episode 768\n",
            " \tTotal reward: -113.72474670410156\n",
            " \tTraining loss: 0.9696\n",
            " \tExplore P: 1.0204\n",
            "\n",
            "Episode 769\n",
            " \tTotal reward: -115.93302917480469\n",
            " \tTraining loss: 1.0240\n",
            " \tExplore P: 1.0204\n",
            "\n",
            "Episode 770\n",
            " \tTotal reward: -91.8658447265625\n",
            " \tTraining loss: 0.5871\n",
            " \tExplore P: 1.0203\n",
            "\n",
            "Model saved\n",
            "Episode 771\n",
            " \tTotal reward: -108.25177001953125\n",
            " \tTraining loss: 10.3829\n",
            " \tExplore P: 1.0202\n",
            "\n",
            "Episode 772\n",
            " \tTotal reward: -110.4840087890625\n",
            " \tTraining loss: 0.9216\n",
            " \tExplore P: 1.0200\n",
            "\n",
            "Episode 773\n",
            " \tTotal reward: -110.17794799804688\n",
            " \tTraining loss: 0.6300\n",
            " \tExplore P: 1.0200\n",
            "\n",
            "Episode 774\n",
            " \tTotal reward: -115.99713134765625\n",
            " \tTraining loss: 7.2578\n",
            " \tExplore P: 1.0199\n",
            "\n",
            "Episode 775\n",
            " \tTotal reward: -99.02377319335938\n",
            " \tTraining loss: 1.6462\n",
            " \tExplore P: 1.0198\n",
            "\n",
            "Model saved\n",
            "Episode 776\n",
            " \tTotal reward: -114.65666198730469\n",
            " \tTraining loss: 0.9096\n",
            " \tExplore P: 1.0198\n",
            "\n",
            "Episode 777\n",
            " \tTotal reward: -113.75120544433594\n",
            " \tTraining loss: 1.9147\n",
            " \tExplore P: 1.0197\n",
            "\n",
            "Episode 778\n",
            " \tTotal reward: -112.27342224121094\n",
            " \tTraining loss: 1.1610\n",
            " \tExplore P: 1.0197\n",
            "\n",
            "Episode 779\n",
            " \tTotal reward: -113.70208740234375\n",
            " \tTraining loss: 7.7046\n",
            " \tExplore P: 1.0196\n",
            "\n",
            "Episode 780\n",
            " \tTotal reward: -115.87675476074219\n",
            " \tTraining loss: 0.5532\n",
            " \tExplore P: 1.0194\n",
            "\n",
            "Model saved\n",
            "Episode 781\n",
            " \tTotal reward: -114.3212890625\n",
            " \tTraining loss: 1.5718\n",
            " \tExplore P: 1.0193\n",
            "\n",
            "Episode 782\n",
            " \tTotal reward: -89.24978637695312\n",
            " \tTraining loss: 0.5873\n",
            " \tExplore P: 1.0192\n",
            "\n",
            "Episode 783\n",
            " \tTotal reward: -112.72151184082031\n",
            " \tTraining loss: 1.5389\n",
            " \tExplore P: 1.0191\n",
            "\n",
            "Episode 784\n",
            " \tTotal reward: -115.99478149414062\n",
            " \tTraining loss: 1.3216\n",
            " \tExplore P: 1.0190\n",
            "\n",
            "Episode 785\n",
            " \tTotal reward: -99.76017761230469\n",
            " \tTraining loss: 12.8256\n",
            " \tExplore P: 1.0189\n",
            "\n",
            "Model saved\n",
            "Episode 786\n",
            " \tTotal reward: -115.99772644042969\n",
            " \tTraining loss: 1.2729\n",
            " \tExplore P: 1.0189\n",
            "\n",
            "Episode 787\n",
            " \tTotal reward: -113.81631469726562\n",
            " \tTraining loss: 1.0833\n",
            " \tExplore P: 1.0188\n",
            "\n",
            "Episode 788\n",
            " \tTotal reward: -104.51509094238281\n",
            " \tTraining loss: 1.2209\n",
            " \tExplore P: 1.0187\n",
            "\n",
            "Episode 789\n",
            " \tTotal reward: -113.00820922851562\n",
            " \tTraining loss: 0.6555\n",
            " \tExplore P: 1.0187\n",
            "\n",
            "Episode 790\n",
            " \tTotal reward: -87.65342712402344\n",
            " \tTraining loss: 0.7532\n",
            " \tExplore P: 1.0186\n",
            "\n",
            "Model saved\n",
            "Model updated\n",
            "Episode 791\n",
            " \tTotal reward: 106.51455688476562\n",
            " \tTraining loss: 1.9250\n",
            " \tExplore P: 1.0177\n",
            "\n",
            "Episode 792\n",
            " \tTotal reward: -104.59674072265625\n",
            " \tTraining loss: 2.0283\n",
            " \tExplore P: 1.0176\n",
            "\n",
            "Episode 793\n",
            " \tTotal reward: -115.77622985839844\n",
            " \tTraining loss: 0.9799\n",
            " \tExplore P: 1.0175\n",
            "\n",
            "Episode 794\n",
            " \tTotal reward: -103.51817321777344\n",
            " \tTraining loss: 8.9816\n",
            " \tExplore P: 1.0174\n",
            "\n",
            "Episode 795\n",
            " \tTotal reward: -106.46382141113281\n",
            " \tTraining loss: 1.2550\n",
            " \tExplore P: 1.0173\n",
            "\n",
            "Model saved\n",
            "Episode 796\n",
            " \tTotal reward: -115.99716186523438\n",
            " \tTraining loss: 5.4657\n",
            " \tExplore P: 1.0173\n",
            "\n",
            "Episode 797\n",
            " \tTotal reward: -71.28598022460938\n",
            " \tTraining loss: 0.6773\n",
            " \tExplore P: 1.0171\n",
            "\n",
            "Episode 798\n",
            " \tTotal reward: -107.04640197753906\n",
            " \tTraining loss: 12.0846\n",
            " \tExplore P: 1.0170\n",
            "\n",
            "Episode 799\n",
            " \tTotal reward: -115.91252136230469\n",
            " \tTraining loss: 3.3775\n",
            " \tExplore P: 1.0169\n",
            "\n",
            "Episode 800\n",
            " \tTotal reward: -107.06101989746094\n",
            " \tTraining loss: 2.1561\n",
            " \tExplore P: 1.0169\n",
            "\n",
            "Model saved\n",
            "Episode 801\n",
            " \tTotal reward: -115.78916931152344\n",
            " \tTraining loss: 2.1674\n",
            " \tExplore P: 1.0166\n",
            "\n",
            "Episode 802\n",
            " \tTotal reward: -102.71922302246094\n",
            " \tTraining loss: 1.2378\n",
            " \tExplore P: 1.0166\n",
            "\n",
            "Episode 803\n",
            " \tTotal reward: -101.52755737304688\n",
            " \tTraining loss: 1.0587\n",
            " \tExplore P: 1.0165\n",
            "\n",
            "Episode 804\n",
            " \tTotal reward: -115.99786376953125\n",
            " \tTraining loss: 0.7061\n",
            " \tExplore P: 1.0165\n",
            "\n",
            "Episode 805\n",
            " \tTotal reward: -90.92991638183594\n",
            " \tTraining loss: 1.5353\n",
            " \tExplore P: 1.0163\n",
            "\n",
            "Model saved\n",
            "Episode 806\n",
            " \tTotal reward: -109.76493835449219\n",
            " \tTraining loss: 4.3712\n",
            " \tExplore P: 1.0162\n",
            "\n",
            "Episode 807\n",
            " \tTotal reward: -92.41851806640625\n",
            " \tTraining loss: 1.5939\n",
            " \tExplore P: 1.0162\n",
            "\n",
            "Episode 808\n",
            " \tTotal reward: -112.90170288085938\n",
            " \tTraining loss: 2.2479\n",
            " \tExplore P: 1.0161\n",
            "\n",
            "Episode 809\n",
            " \tTotal reward: -113.75395202636719\n",
            " \tTraining loss: 1.0004\n",
            " \tExplore P: 1.0159\n",
            "\n",
            "Episode 810\n",
            " \tTotal reward: -109.31715393066406\n",
            " \tTraining loss: 0.9149\n",
            " \tExplore P: 1.0158\n",
            "\n",
            "Model saved\n",
            "Episode 811\n",
            " \tTotal reward: -107.06448364257812\n",
            " \tTraining loss: 0.7823\n",
            " \tExplore P: 1.0158\n",
            "\n",
            "Episode 812\n",
            " \tTotal reward: -86.99972534179688\n",
            " \tTraining loss: 1.6442\n",
            " \tExplore P: 1.0157\n",
            "\n",
            "Episode 813\n",
            " \tTotal reward: -113.97955322265625\n",
            " \tTraining loss: 0.5374\n",
            " \tExplore P: 1.0156\n",
            "\n",
            "Episode 814\n",
            " \tTotal reward: -110.70927429199219\n",
            " \tTraining loss: 1.4328\n",
            " \tExplore P: 1.0155\n",
            "\n",
            "Episode 815\n",
            " \tTotal reward: -98.28089904785156\n",
            " \tTraining loss: 1.2928\n",
            " \tExplore P: 1.0155\n",
            "\n",
            "Model saved\n",
            "Episode 816\n",
            " \tTotal reward: -114.89141845703125\n",
            " \tTraining loss: 1.3942\n",
            " \tExplore P: 1.0154\n",
            "\n",
            "Episode 817\n",
            " \tTotal reward: -115.99856567382812\n",
            " \tTraining loss: 1.5228\n",
            " \tExplore P: 1.0154\n",
            "\n",
            "Episode 818\n",
            " \tTotal reward: -115.20541381835938\n",
            " \tTraining loss: 1.1865\n",
            " \tExplore P: 1.0153\n",
            "\n",
            "Episode 819\n",
            " \tTotal reward: -111.49267578125\n",
            " \tTraining loss: 0.9269\n",
            " \tExplore P: 1.0152\n",
            "\n",
            "Episode 820\n",
            " \tTotal reward: -110.17364501953125\n",
            " \tTraining loss: 1.8623\n",
            " \tExplore P: 1.0152\n",
            "\n",
            "Model saved\n",
            "Episode 821\n",
            " \tTotal reward: -107.74464416503906\n",
            " \tTraining loss: 1.0860\n",
            " \tExplore P: 1.0151\n",
            "\n",
            "Episode 822\n",
            " \tTotal reward: -114.53535461425781\n",
            " \tTraining loss: 0.4447\n",
            " \tExplore P: 1.0150\n",
            "\n",
            "Episode 823\n",
            " \tTotal reward: -71.562744140625\n",
            " \tTraining loss: 0.9502\n",
            " \tExplore P: 1.0150\n",
            "\n",
            "Episode 824\n",
            " \tTotal reward: -106.74110412597656\n",
            " \tTraining loss: 1.1349\n",
            " \tExplore P: 1.0149\n",
            "\n",
            "Episode 825\n",
            " \tTotal reward: -93.25851440429688\n",
            " \tTraining loss: 2.1106\n",
            " \tExplore P: 1.0148\n",
            "\n",
            "Model saved\n",
            "Episode 826\n",
            " \tTotal reward: -94.34516906738281\n",
            " \tTraining loss: 1.6321\n",
            " \tExplore P: 1.0147\n",
            "\n",
            "Episode 827\n",
            " \tTotal reward: -92.91607666015625\n",
            " \tTraining loss: 2.3830\n",
            " \tExplore P: 1.0147\n",
            "\n",
            "Episode 828\n",
            " \tTotal reward: -100.48757934570312\n",
            " \tTraining loss: 1.0562\n",
            " \tExplore P: 1.0146\n",
            "\n",
            "Episode 829\n",
            " \tTotal reward: -80.86552429199219\n",
            " \tTraining loss: 2.2460\n",
            " \tExplore P: 1.0144\n",
            "\n",
            "Episode 830\n",
            " \tTotal reward: -112.970703125\n",
            " \tTraining loss: 1.3940\n",
            " \tExplore P: 1.0144\n",
            "\n",
            "Model saved\n",
            "Episode 831\n",
            " \tTotal reward: -115.90922546386719\n",
            " \tTraining loss: 1.0970\n",
            " \tExplore P: 1.0143\n",
            "\n",
            "Episode 832\n",
            " \tTotal reward: -85.81094360351562\n",
            " \tTraining loss: 1.4469\n",
            " \tExplore P: 1.0142\n",
            "\n",
            "Episode 833\n",
            " \tTotal reward: -93.84268188476562\n",
            " \tTraining loss: 1.1852\n",
            " \tExplore P: 1.0141\n",
            "\n",
            "Episode 834\n",
            " \tTotal reward: -115.99935913085938\n",
            " \tTraining loss: 4.0895\n",
            " \tExplore P: 1.0140\n",
            "\n",
            "Episode 835\n",
            " \tTotal reward: -109.47207641601562\n",
            " \tTraining loss: 3.0164\n",
            " \tExplore P: 1.0140\n",
            "\n",
            "Model saved\n",
            "Episode 836\n",
            " \tTotal reward: -85.29655456542969\n",
            " \tTraining loss: 2.0995\n",
            " \tExplore P: 1.0140\n",
            "\n",
            "Episode 837\n",
            " \tTotal reward: -100.86482238769531\n",
            " \tTraining loss: 1.7549\n",
            " \tExplore P: 1.0139\n",
            "\n",
            "Episode 838\n",
            " \tTotal reward: -84.07501220703125\n",
            " \tTraining loss: 8.8381\n",
            " \tExplore P: 1.0139\n",
            "\n",
            "Episode 839\n",
            " \tTotal reward: -114.9403076171875\n",
            " \tTraining loss: 10.0115\n",
            " \tExplore P: 1.0138\n",
            "\n",
            "Episode 840\n",
            " \tTotal reward: -107.31672668457031\n",
            " \tTraining loss: 0.5399\n",
            " \tExplore P: 1.0137\n",
            "\n",
            "Model saved\n",
            "Episode 841\n",
            " \tTotal reward: -92.09025573730469\n",
            " \tTraining loss: 0.9482\n",
            " \tExplore P: 1.0137\n",
            "\n",
            "Episode 842\n",
            " \tTotal reward: -108.77461242675781\n",
            " \tTraining loss: 1.4295\n",
            " \tExplore P: 1.0137\n",
            "\n",
            "Episode 843\n",
            " \tTotal reward: -103.54129028320312\n",
            " \tTraining loss: 1.9375\n",
            " \tExplore P: 1.0136\n",
            "\n",
            "Episode 844\n",
            " \tTotal reward: -112.77914428710938\n",
            " \tTraining loss: 3.2141\n",
            " \tExplore P: 1.0136\n",
            "\n",
            "Episode 845\n",
            " \tTotal reward: -113.24583435058594\n",
            " \tTraining loss: 1.2346\n",
            " \tExplore P: 1.0135\n",
            "\n",
            "Model saved\n",
            "Episode 846\n",
            " \tTotal reward: -105.20350646972656\n",
            " \tTraining loss: 1.2390\n",
            " \tExplore P: 1.0134\n",
            "\n",
            "Episode 847\n",
            " \tTotal reward: -115.25242614746094\n",
            " \tTraining loss: 1.3172\n",
            " \tExplore P: 1.0133\n",
            "\n",
            "Episode 848\n",
            " \tTotal reward: -115.62216186523438\n",
            " \tTraining loss: 2.8455\n",
            " \tExplore P: 1.0133\n",
            "\n",
            "Episode 849\n",
            " \tTotal reward: -100.95301818847656\n",
            " \tTraining loss: 1.2594\n",
            " \tExplore P: 1.0132\n",
            "\n",
            "Episode 850\n",
            " \tTotal reward: -110.32832336425781\n",
            " \tTraining loss: 0.8464\n",
            " \tExplore P: 1.0132\n",
            "\n",
            "Model saved\n",
            "Episode 851\n",
            " \tTotal reward: -111.39262390136719\n",
            " \tTraining loss: 1.1970\n",
            " \tExplore P: 1.0131\n",
            "\n",
            "Episode 852\n",
            " \tTotal reward: -111.81024169921875\n",
            " \tTraining loss: 0.8487\n",
            " \tExplore P: 1.0130\n",
            "\n",
            "Episode 853\n",
            " \tTotal reward: -69.07009887695312\n",
            " \tTraining loss: 17.0295\n",
            " \tExplore P: 1.0129\n",
            "\n",
            "Episode 854\n",
            " \tTotal reward: -107.43153381347656\n",
            " \tTraining loss: 0.9777\n",
            " \tExplore P: 1.0127\n",
            "\n",
            "Episode 855\n",
            " \tTotal reward: -110.87103271484375\n",
            " \tTraining loss: 0.9037\n",
            " \tExplore P: 1.0126\n",
            "\n",
            "Model saved\n",
            "Episode 856\n",
            " \tTotal reward: -114.20037841796875\n",
            " \tTraining loss: 1.0972\n",
            " \tExplore P: 1.0125\n",
            "\n",
            "Episode 857\n",
            " \tTotal reward: -95.3551025390625\n",
            " \tTraining loss: 0.9538\n",
            " \tExplore P: 1.0125\n",
            "\n",
            "Episode 858\n",
            " \tTotal reward: -96.09432983398438\n",
            " \tTraining loss: 0.8524\n",
            " \tExplore P: 1.0124\n",
            "\n",
            "Episode 859\n",
            " \tTotal reward: -113.9801025390625\n",
            " \tTraining loss: 2.3012\n",
            " \tExplore P: 1.0123\n",
            "\n",
            "Episode 860\n",
            " \tTotal reward: -113.76280212402344\n",
            " \tTraining loss: 1.8722\n",
            " \tExplore P: 1.0123\n",
            "\n",
            "Model saved\n",
            "Episode 861\n",
            " \tTotal reward: -103.49714660644531\n",
            " \tTraining loss: 0.7251\n",
            " \tExplore P: 1.0122\n",
            "\n",
            "Episode 862\n",
            " \tTotal reward: -107.72514343261719\n",
            " \tTraining loss: 0.9763\n",
            " \tExplore P: 1.0121\n",
            "\n",
            "Episode 863\n",
            " \tTotal reward: -115.96687316894531\n",
            " \tTraining loss: 0.8413\n",
            " \tExplore P: 1.0121\n",
            "\n",
            "Episode 864\n",
            " \tTotal reward: -93.53610229492188\n",
            " \tTraining loss: 1.3045\n",
            " \tExplore P: 1.0120\n",
            "\n",
            "Episode 865\n",
            " \tTotal reward: -115.97560119628906\n",
            " \tTraining loss: 0.5428\n",
            " \tExplore P: 1.0120\n",
            "\n",
            "Model saved\n",
            "Episode 866\n",
            " \tTotal reward: -113.02580261230469\n",
            " \tTraining loss: 1.2469\n",
            " \tExplore P: 1.0119\n",
            "\n",
            "Episode 867\n",
            " \tTotal reward: -115.99516296386719\n",
            " \tTraining loss: 2.1978\n",
            " \tExplore P: 1.0119\n",
            "\n",
            "Episode 868\n",
            " \tTotal reward: -97.23249816894531\n",
            " \tTraining loss: 0.9967\n",
            " \tExplore P: 1.0118\n",
            "\n",
            "Episode 869\n",
            " \tTotal reward: -106.94338989257812\n",
            " \tTraining loss: 1.5683\n",
            " \tExplore P: 1.0117\n",
            "\n",
            "Episode 870\n",
            " \tTotal reward: -105.54644775390625\n",
            " \tTraining loss: 0.8870\n",
            " \tExplore P: 1.0117\n",
            "\n",
            "Model saved\n",
            "Episode 871\n",
            " \tTotal reward: -102.7987060546875\n",
            " \tTraining loss: 1.0970\n",
            " \tExplore P: 1.0116\n",
            "\n",
            "Episode 872\n",
            " \tTotal reward: -109.09077453613281\n",
            " \tTraining loss: 1.3751\n",
            " \tExplore P: 1.0116\n",
            "\n",
            "Episode 873\n",
            " \tTotal reward: -107.77835083007812\n",
            " \tTraining loss: 0.6296\n",
            " \tExplore P: 1.0115\n",
            "\n",
            "Episode 874\n",
            " \tTotal reward: -110.25576782226562\n",
            " \tTraining loss: 2.9588\n",
            " \tExplore P: 1.0115\n",
            "\n",
            "Episode 875\n",
            " \tTotal reward: -106.41107177734375\n",
            " \tTraining loss: 1.1616\n",
            " \tExplore P: 1.0114\n",
            "\n",
            "Model saved\n",
            "Episode 876\n",
            " \tTotal reward: -110.59394836425781\n",
            " \tTraining loss: 1.0999\n",
            " \tExplore P: 1.0114\n",
            "\n",
            "Episode 877\n",
            " \tTotal reward: -106.38876342773438\n",
            " \tTraining loss: 2.9460\n",
            " \tExplore P: 1.0114\n",
            "\n",
            "Episode 878\n",
            " \tTotal reward: -114.34176635742188\n",
            " \tTraining loss: 2.2263\n",
            " \tExplore P: 1.0113\n",
            "\n",
            "Episode 879\n",
            " \tTotal reward: -106.94322204589844\n",
            " \tTraining loss: 2.2184\n",
            " \tExplore P: 1.0113\n",
            "\n",
            "Model updated\n",
            "Episode 880\n",
            " \tTotal reward: -86.9776611328125\n",
            " \tTraining loss: 0.8182\n",
            " \tExplore P: 1.0108\n",
            "\n",
            "Model saved\n",
            "Episode 881\n",
            " \tTotal reward: -115.99713134765625\n",
            " \tTraining loss: 1.3881\n",
            " \tExplore P: 1.0108\n",
            "\n",
            "Episode 882\n",
            " \tTotal reward: -112.51248168945312\n",
            " \tTraining loss: 1.2370\n",
            " \tExplore P: 1.0107\n",
            "\n",
            "Episode 883\n",
            " \tTotal reward: -114.49386596679688\n",
            " \tTraining loss: 0.9016\n",
            " \tExplore P: 1.0107\n",
            "\n",
            "Episode 884\n",
            " \tTotal reward: -103.06919860839844\n",
            " \tTraining loss: 8.2251\n",
            " \tExplore P: 1.0106\n",
            "\n",
            "Episode 885\n",
            " \tTotal reward: -107.32974243164062\n",
            " \tTraining loss: 1.9321\n",
            " \tExplore P: 1.0106\n",
            "\n",
            "Model saved\n",
            "Episode 886\n",
            " \tTotal reward: -59.30162048339844\n",
            " \tTraining loss: 1.5241\n",
            " \tExplore P: 1.0105\n",
            "\n",
            "Episode 887\n",
            " \tTotal reward: -107.19660949707031\n",
            " \tTraining loss: 0.9601\n",
            " \tExplore P: 1.0104\n",
            "\n",
            "Episode 888\n",
            " \tTotal reward: -106.43257141113281\n",
            " \tTraining loss: 1.0371\n",
            " \tExplore P: 1.0104\n",
            "\n",
            "Episode 889\n",
            " \tTotal reward: -101.60052490234375\n",
            " \tTraining loss: 2.2533\n",
            " \tExplore P: 1.0103\n",
            "\n",
            "Episode 890\n",
            " \tTotal reward: -111.28738403320312\n",
            " \tTraining loss: 0.8328\n",
            " \tExplore P: 1.0102\n",
            "\n",
            "Model saved\n",
            "Episode 891\n",
            " \tTotal reward: -94.63200378417969\n",
            " \tTraining loss: 0.8336\n",
            " \tExplore P: 1.0102\n",
            "\n",
            "Episode 892\n",
            " \tTotal reward: -113.19206237792969\n",
            " \tTraining loss: 1.2059\n",
            " \tExplore P: 1.0102\n",
            "\n",
            "Episode 893\n",
            " \tTotal reward: -115.20610046386719\n",
            " \tTraining loss: 2.1111\n",
            " \tExplore P: 1.0101\n",
            "\n",
            "Episode 894\n",
            " \tTotal reward: -91.23271179199219\n",
            " \tTraining loss: 1.7084\n",
            " \tExplore P: 1.0101\n",
            "\n",
            "Episode 895\n",
            " \tTotal reward: -115.56443786621094\n",
            " \tTraining loss: 1.0112\n",
            " \tExplore P: 1.0101\n",
            "\n",
            "Model saved\n",
            "Episode 896\n",
            " \tTotal reward: -114.79905700683594\n",
            " \tTraining loss: 1.2415\n",
            " \tExplore P: 1.0100\n",
            "\n",
            "Episode 897\n",
            " \tTotal reward: -85.65959167480469\n",
            " \tTraining loss: 1.2824\n",
            " \tExplore P: 1.0100\n",
            "\n",
            "Episode 898\n",
            " \tTotal reward: -115.97848510742188\n",
            " \tTraining loss: 2.4062\n",
            " \tExplore P: 1.0099\n",
            "\n",
            "Episode 899\n",
            " \tTotal reward: -88.29519653320312\n",
            " \tTraining loss: 0.9750\n",
            " \tExplore P: 1.0099\n",
            "\n",
            "Episode 900\n",
            " \tTotal reward: -106.272216796875\n",
            " \tTraining loss: 1.1115\n",
            " \tExplore P: 1.0098\n",
            "\n",
            "Model saved\n",
            "Episode 901\n",
            " \tTotal reward: -115.9403076171875\n",
            " \tTraining loss: 1.4541\n",
            " \tExplore P: 1.0098\n",
            "\n",
            "Episode 902\n",
            " \tTotal reward: -107.28218078613281\n",
            " \tTraining loss: 1.2715\n",
            " \tExplore P: 1.0098\n",
            "\n",
            "Episode 903\n",
            " \tTotal reward: -109.4141845703125\n",
            " \tTraining loss: 1.6844\n",
            " \tExplore P: 1.0097\n",
            "\n",
            "Episode 904\n",
            " \tTotal reward: -112.57464599609375\n",
            " \tTraining loss: 1.0893\n",
            " \tExplore P: 1.0097\n",
            "\n",
            "Episode 905\n",
            " \tTotal reward: -95.50735473632812\n",
            " \tTraining loss: 0.9380\n",
            " \tExplore P: 1.0097\n",
            "\n",
            "Model saved\n",
            "Episode 906\n",
            " \tTotal reward: -110.78547668457031\n",
            " \tTraining loss: 1.4767\n",
            " \tExplore P: 1.0096\n",
            "\n",
            "Episode 907\n",
            " \tTotal reward: -98.89054870605469\n",
            " \tTraining loss: 0.7928\n",
            " \tExplore P: 1.0096\n",
            "\n",
            "Episode 908\n",
            " \tTotal reward: -115.99642944335938\n",
            " \tTraining loss: 17.8363\n",
            " \tExplore P: 1.0095\n",
            "\n",
            "Episode 909\n",
            " \tTotal reward: -104.80221557617188\n",
            " \tTraining loss: 2.9648\n",
            " \tExplore P: 1.0095\n",
            "\n",
            "Episode 910\n",
            " \tTotal reward: -115.99882507324219\n",
            " \tTraining loss: 2.0125\n",
            " \tExplore P: 1.0094\n",
            "\n",
            "Model saved\n",
            "Episode 911\n",
            " \tTotal reward: -113.48649597167969\n",
            " \tTraining loss: 1.9913\n",
            " \tExplore P: 1.0094\n",
            "\n",
            "Episode 912\n",
            " \tTotal reward: -97.724609375\n",
            " \tTraining loss: 0.9408\n",
            " \tExplore P: 1.0094\n",
            "\n",
            "Episode 913\n",
            " \tTotal reward: -115.2945556640625\n",
            " \tTraining loss: 1.0770\n",
            " \tExplore P: 1.0094\n",
            "\n",
            "Episode 914\n",
            " \tTotal reward: -106.1376953125\n",
            " \tTraining loss: 2.3958\n",
            " \tExplore P: 1.0093\n",
            "\n",
            "Episode 915\n",
            " \tTotal reward: -41.76678466796875\n",
            " \tTraining loss: 0.9878\n",
            " \tExplore P: 1.0093\n",
            "\n",
            "Model saved\n",
            "Episode 916\n",
            " \tTotal reward: -115.92892456054688\n",
            " \tTraining loss: 0.8574\n",
            " \tExplore P: 1.0092\n",
            "\n",
            "Episode 917\n",
            " \tTotal reward: -87.75013732910156\n",
            " \tTraining loss: 1.2872\n",
            " \tExplore P: 1.0092\n",
            "\n",
            "Episode 918\n",
            " \tTotal reward: -94.70639038085938\n",
            " \tTraining loss: 7.4144\n",
            " \tExplore P: 1.0092\n",
            "\n",
            "Episode 919\n",
            " \tTotal reward: -115.99861145019531\n",
            " \tTraining loss: 7.9291\n",
            " \tExplore P: 1.0091\n",
            "\n",
            "Episode 920\n",
            " \tTotal reward: -109.63636779785156\n",
            " \tTraining loss: 0.8643\n",
            " \tExplore P: 1.0091\n",
            "\n",
            "Model saved\n",
            "Episode 921\n",
            " \tTotal reward: -93.4097900390625\n",
            " \tTraining loss: 1.7874\n",
            " \tExplore P: 1.0090\n",
            "\n",
            "Episode 922\n",
            " \tTotal reward: -113.65650939941406\n",
            " \tTraining loss: 3.2851\n",
            " \tExplore P: 1.0090\n",
            "\n",
            "Episode 923\n",
            " \tTotal reward: -113.1483154296875\n",
            " \tTraining loss: 1.6383\n",
            " \tExplore P: 1.0090\n",
            "\n",
            "Episode 924\n",
            " \tTotal reward: -111.94589233398438\n",
            " \tTraining loss: 3.1630\n",
            " \tExplore P: 1.0089\n",
            "\n",
            "Episode 925\n",
            " \tTotal reward: -113.31205749511719\n",
            " \tTraining loss: 1.2214\n",
            " \tExplore P: 1.0089\n",
            "\n",
            "Model saved\n",
            "Episode 926\n",
            " \tTotal reward: -54.37074279785156\n",
            " \tTraining loss: 0.6644\n",
            " \tExplore P: 1.0088\n",
            "\n",
            "Episode 927\n",
            " \tTotal reward: -115.73857116699219\n",
            " \tTraining loss: 1.6574\n",
            " \tExplore P: 1.0087\n",
            "\n",
            "Episode 928\n",
            " \tTotal reward: -114.22227478027344\n",
            " \tTraining loss: 1.7880\n",
            " \tExplore P: 1.0087\n",
            "\n",
            "Episode 929\n",
            " \tTotal reward: -109.11053466796875\n",
            " \tTraining loss: 6.2305\n",
            " \tExplore P: 1.0086\n",
            "\n",
            "Episode 930\n",
            " \tTotal reward: -93.01580810546875\n",
            " \tTraining loss: 1.1649\n",
            " \tExplore P: 1.0086\n",
            "\n",
            "Model saved\n",
            "Episode 931\n",
            " \tTotal reward: -115.64189147949219\n",
            " \tTraining loss: 1.2886\n",
            " \tExplore P: 1.0085\n",
            "\n",
            "Episode 932\n",
            " \tTotal reward: -115.41523742675781\n",
            " \tTraining loss: 2.7818\n",
            " \tExplore P: 1.0085\n",
            "\n",
            "Episode 933\n",
            " \tTotal reward: -115.9549560546875\n",
            " \tTraining loss: 1.8517\n",
            " \tExplore P: 1.0084\n",
            "\n",
            "Episode 934\n",
            " \tTotal reward: -112.24392700195312\n",
            " \tTraining loss: 1.0253\n",
            " \tExplore P: 1.0084\n",
            "\n",
            "Episode 935\n",
            " \tTotal reward: -114.72256469726562\n",
            " \tTraining loss: 1.5883\n",
            " \tExplore P: 1.0083\n",
            "\n",
            "Model saved\n",
            "Episode 936\n",
            " \tTotal reward: -107.83050537109375\n",
            " \tTraining loss: 9.8349\n",
            " \tExplore P: 1.0083\n",
            "\n",
            "Episode 937\n",
            " \tTotal reward: -107.09255981445312\n",
            " \tTraining loss: 1.3597\n",
            " \tExplore P: 1.0083\n",
            "\n",
            "Episode 938\n",
            " \tTotal reward: -113.43595886230469\n",
            " \tTraining loss: 1.5818\n",
            " \tExplore P: 1.0083\n",
            "\n",
            "Episode 939\n",
            " \tTotal reward: -115.21733093261719\n",
            " \tTraining loss: 1.2012\n",
            " \tExplore P: 1.0082\n",
            "\n",
            "Episode 940\n",
            " \tTotal reward: -99.44061279296875\n",
            " \tTraining loss: 9.8223\n",
            " \tExplore P: 1.0080\n",
            "\n",
            "Model saved\n",
            "Episode 941\n",
            " \tTotal reward: -115.95469665527344\n",
            " \tTraining loss: 1.5520\n",
            " \tExplore P: 1.0079\n",
            "\n",
            "Episode 942\n",
            " \tTotal reward: -113.12484741210938\n",
            " \tTraining loss: 1.5017\n",
            " \tExplore P: 1.0079\n",
            "\n",
            "Episode 943\n",
            " \tTotal reward: -108.03526306152344\n",
            " \tTraining loss: 1.2084\n",
            " \tExplore P: 1.0079\n",
            "\n",
            "Episode 944\n",
            " \tTotal reward: -102.76695251464844\n",
            " \tTraining loss: 1.9980\n",
            " \tExplore P: 1.0079\n",
            "\n",
            "Episode 945\n",
            " \tTotal reward: -86.07640075683594\n",
            " \tTraining loss: 1.8029\n",
            " \tExplore P: 1.0078\n",
            "\n",
            "Model saved\n",
            "Episode 946\n",
            " \tTotal reward: -105.20805358886719\n",
            " \tTraining loss: 2.4971\n",
            " \tExplore P: 1.0078\n",
            "\n",
            "Episode 947\n",
            " \tTotal reward: -111.16200256347656\n",
            " \tTraining loss: 1.3429\n",
            " \tExplore P: 1.0078\n",
            "\n",
            "Episode 948\n",
            " \tTotal reward: -102.20759582519531\n",
            " \tTraining loss: 1.2312\n",
            " \tExplore P: 1.0077\n",
            "\n",
            "Episode 949\n",
            " \tTotal reward: -96.84483337402344\n",
            " \tTraining loss: 1.1271\n",
            " \tExplore P: 1.0077\n",
            "\n",
            "Episode 950\n",
            " \tTotal reward: -103.360595703125\n",
            " \tTraining loss: 0.5386\n",
            " \tExplore P: 1.0077\n",
            "\n",
            "Model saved\n",
            "Episode 951\n",
            " \tTotal reward: -109.6177978515625\n",
            " \tTraining loss: 1.8090\n",
            " \tExplore P: 1.0076\n",
            "\n",
            "Episode 952\n",
            " \tTotal reward: -112.61451721191406\n",
            " \tTraining loss: 0.6972\n",
            " \tExplore P: 1.0076\n",
            "\n",
            "Episode 953\n",
            " \tTotal reward: -115.85150146484375\n",
            " \tTraining loss: 0.7476\n",
            " \tExplore P: 1.0076\n",
            "\n",
            "Episode 954\n",
            " \tTotal reward: -95.2757568359375\n",
            " \tTraining loss: 1.1147\n",
            " \tExplore P: 1.0075\n",
            "\n",
            "Episode 955\n",
            " \tTotal reward: -109.51359558105469\n",
            " \tTraining loss: 1.4444\n",
            " \tExplore P: 1.0075\n",
            "\n",
            "Model saved\n",
            "Episode 956\n",
            " \tTotal reward: -86.20872497558594\n",
            " \tTraining loss: 3.2787\n",
            " \tExplore P: 1.0074\n",
            "\n",
            "Episode 957\n",
            " \tTotal reward: -111.89443969726562\n",
            " \tTraining loss: 1.5828\n",
            " \tExplore P: 1.0074\n",
            "\n",
            "Episode 958\n",
            " \tTotal reward: -112.61283874511719\n",
            " \tTraining loss: 1.9987\n",
            " \tExplore P: 1.0073\n",
            "\n",
            "Episode 959\n",
            " \tTotal reward: -109.804443359375\n",
            " \tTraining loss: 0.8063\n",
            " \tExplore P: 1.0073\n",
            "\n",
            "Episode 960\n",
            " \tTotal reward: -88.18243408203125\n",
            " \tTraining loss: 1.1802\n",
            " \tExplore P: 1.0073\n",
            "\n",
            "Model saved\n",
            "Episode 961\n",
            " \tTotal reward: -107.65000915527344\n",
            " \tTraining loss: 0.9026\n",
            " \tExplore P: 1.0073\n",
            "\n",
            "Episode 962\n",
            " \tTotal reward: -102.8555908203125\n",
            " \tTraining loss: 0.5403\n",
            " \tExplore P: 1.0072\n",
            "\n",
            "Episode 963\n",
            " \tTotal reward: -93.51168823242188\n",
            " \tTraining loss: 0.6966\n",
            " \tExplore P: 1.0072\n",
            "\n",
            "Episode 964\n",
            " \tTotal reward: -97.49276733398438\n",
            " \tTraining loss: 1.0332\n",
            " \tExplore P: 1.0072\n",
            "\n",
            "Episode 965\n",
            " \tTotal reward: -87.82356262207031\n",
            " \tTraining loss: 0.9730\n",
            " \tExplore P: 1.0071\n",
            "\n",
            "Model saved\n",
            "Episode 966\n",
            " \tTotal reward: -108.60231018066406\n",
            " \tTraining loss: 1.1257\n",
            " \tExplore P: 1.0071\n",
            "\n",
            "Episode 967\n",
            " \tTotal reward: -101.32496643066406\n",
            " \tTraining loss: 0.7877\n",
            " \tExplore P: 1.0071\n",
            "\n",
            "Episode 968\n",
            " \tTotal reward: -98.12567138671875\n",
            " \tTraining loss: 1.3533\n",
            " \tExplore P: 1.0070\n",
            "\n",
            "Episode 969\n",
            " \tTotal reward: -81.15841674804688\n",
            " \tTraining loss: 2.6114\n",
            " \tExplore P: 1.0070\n",
            "\n",
            "Episode 970\n",
            " \tTotal reward: -110.62028503417969\n",
            " \tTraining loss: 0.9239\n",
            " \tExplore P: 1.0070\n",
            "\n",
            "Model saved\n",
            "Episode 971\n",
            " \tTotal reward: -88.63595581054688\n",
            " \tTraining loss: 1.1010\n",
            " \tExplore P: 1.0069\n",
            "\n",
            "Episode 972\n",
            " \tTotal reward: -114.90631103515625\n",
            " \tTraining loss: 0.9131\n",
            " \tExplore P: 1.0069\n",
            "\n",
            "Episode 973\n",
            " \tTotal reward: -67.80947875976562\n",
            " \tTraining loss: 1.0177\n",
            " \tExplore P: 1.0069\n",
            "\n",
            "Episode 974\n",
            " \tTotal reward: -115.96929931640625\n",
            " \tTraining loss: 1.0423\n",
            " \tExplore P: 1.0068\n",
            "\n",
            "Episode 975\n",
            " \tTotal reward: -91.33663940429688\n",
            " \tTraining loss: 2.8792\n",
            " \tExplore P: 1.0068\n",
            "\n",
            "Model saved\n",
            "Episode 976\n",
            " \tTotal reward: -115.99362182617188\n",
            " \tTraining loss: 2.6675\n",
            " \tExplore P: 1.0068\n",
            "\n",
            "Episode 977\n",
            " \tTotal reward: -108.19828796386719\n",
            " \tTraining loss: 1.7050\n",
            " \tExplore P: 1.0067\n",
            "\n",
            "Model updated\n",
            "Episode 978\n",
            " \tTotal reward: -110.08952331542969\n",
            " \tTraining loss: 1.2412\n",
            " \tExplore P: 1.0067\n",
            "\n",
            "Episode 979\n",
            " \tTotal reward: -102.85446166992188\n",
            " \tTraining loss: 1.0113\n",
            " \tExplore P: 1.0067\n",
            "\n",
            "Episode 980\n",
            " \tTotal reward: -108.92219543457031\n",
            " \tTraining loss: 2.4561\n",
            " \tExplore P: 1.0067\n",
            "\n",
            "Model saved\n",
            "Episode 981\n",
            " \tTotal reward: -106.38658142089844\n",
            " \tTraining loss: 1.2469\n",
            " \tExplore P: 1.0066\n",
            "\n",
            "Episode 982\n",
            " \tTotal reward: -106.54489135742188\n",
            " \tTraining loss: 0.9828\n",
            " \tExplore P: 1.0066\n",
            "\n",
            "Episode 983\n",
            " \tTotal reward: -102.88430786132812\n",
            " \tTraining loss: 0.8373\n",
            " \tExplore P: 1.0065\n",
            "\n",
            "Episode 984\n",
            " \tTotal reward: -115.97526550292969\n",
            " \tTraining loss: 2.1960\n",
            " \tExplore P: 1.0065\n",
            "\n",
            "Episode 985\n",
            " \tTotal reward: -110.16952514648438\n",
            " \tTraining loss: 3.3116\n",
            " \tExplore P: 1.0065\n",
            "\n",
            "Model saved\n",
            "Episode 986\n",
            " \tTotal reward: -78.76405334472656\n",
            " \tTraining loss: 1.9168\n",
            " \tExplore P: 1.0064\n",
            "\n",
            "Episode 987\n",
            " \tTotal reward: -88.73239135742188\n",
            " \tTraining loss: 1.4888\n",
            " \tExplore P: 1.0064\n",
            "\n",
            "Episode 988\n",
            " \tTotal reward: -107.80848693847656\n",
            " \tTraining loss: 2.2673\n",
            " \tExplore P: 1.0064\n",
            "\n",
            "Episode 989\n",
            " \tTotal reward: -101.39108276367188\n",
            " \tTraining loss: 4.1046\n",
            " \tExplore P: 1.0064\n",
            "\n",
            "Episode 990\n",
            " \tTotal reward: -112.69696044921875\n",
            " \tTraining loss: 7.2514\n",
            " \tExplore P: 1.0063\n",
            "\n",
            "Model saved\n",
            "Episode 991\n",
            " \tTotal reward: -105.43998718261719\n",
            " \tTraining loss: 6.0611\n",
            " \tExplore P: 1.0063\n",
            "\n",
            "Episode 992\n",
            " \tTotal reward: -101.430908203125\n",
            " \tTraining loss: 1.3575\n",
            " \tExplore P: 1.0063\n",
            "\n",
            "Episode 993\n",
            " \tTotal reward: -115.98219299316406\n",
            " \tTraining loss: 0.8369\n",
            " \tExplore P: 1.0063\n",
            "\n",
            "Episode 994\n",
            " \tTotal reward: -101.32081604003906\n",
            " \tTraining loss: 1.8896\n",
            " \tExplore P: 1.0062\n",
            "\n",
            "Episode 995\n",
            " \tTotal reward: -66.1287841796875\n",
            " \tTraining loss: 1.1116\n",
            " \tExplore P: 1.0062\n",
            "\n",
            "Model saved\n",
            "Episode 996\n",
            " \tTotal reward: -110.27444458007812\n",
            " \tTraining loss: 3.8166\n",
            " \tExplore P: 1.0061\n",
            "\n",
            "Episode 997\n",
            " \tTotal reward: -115.98469543457031\n",
            " \tTraining loss: 4.2417\n",
            " \tExplore P: 1.0060\n",
            "\n",
            "Episode 998\n",
            " \tTotal reward: -114.58609008789062\n",
            " \tTraining loss: 4.1059\n",
            " \tExplore P: 1.0060\n",
            "\n",
            "Episode 999\n",
            " \tTotal reward: -115.12680053710938\n",
            " \tTraining loss: 3.5473\n",
            " \tExplore P: 1.0060\n",
            "\n",
            "Episode 1000\n",
            " \tTotal reward: -115.19862365722656\n",
            " \tTraining loss: 1.0536\n",
            " \tExplore P: 1.0060\n",
            "\n",
            "Model saved\n",
            "Episode 1001\n",
            " \tTotal reward: -107.89790344238281\n",
            " \tTraining loss: 11.5459\n",
            " \tExplore P: 1.0060\n",
            "\n",
            "Episode 1002\n",
            " \tTotal reward: -110.50559997558594\n",
            " \tTraining loss: 0.7533\n",
            " \tExplore P: 1.0059\n",
            "\n",
            "Episode 1003\n",
            " \tTotal reward: -84.43519592285156\n",
            " \tTraining loss: 0.7778\n",
            " \tExplore P: 1.0059\n",
            "\n",
            "Episode 1004\n",
            " \tTotal reward: -103.49636840820312\n",
            " \tTraining loss: 1.4726\n",
            " \tExplore P: 1.0059\n",
            "\n",
            "Episode 1005\n",
            " \tTotal reward: -115.97254943847656\n",
            " \tTraining loss: 1.8953\n",
            " \tExplore P: 1.0059\n",
            "\n",
            "Model saved\n",
            "Episode 1006\n",
            " \tTotal reward: -105.16319274902344\n",
            " \tTraining loss: 0.6977\n",
            " \tExplore P: 1.0058\n",
            "\n",
            "Episode 1007\n",
            " \tTotal reward: -109.52073669433594\n",
            " \tTraining loss: 13.6528\n",
            " \tExplore P: 1.0058\n",
            "\n",
            "Episode 1008\n",
            " \tTotal reward: -90.73063659667969\n",
            " \tTraining loss: 1.5373\n",
            " \tExplore P: 1.0058\n",
            "\n",
            "Episode 1009\n",
            " \tTotal reward: -115.74519348144531\n",
            " \tTraining loss: 2.0306\n",
            " \tExplore P: 1.0057\n",
            "\n",
            "Episode 1010\n",
            " \tTotal reward: -113.25244140625\n",
            " \tTraining loss: 1.0079\n",
            " \tExplore P: 1.0057\n",
            "\n",
            "Model saved\n",
            "Episode 1011\n",
            " \tTotal reward: -113.32170104980469\n",
            " \tTraining loss: 1.9340\n",
            " \tExplore P: 1.0056\n",
            "\n",
            "Episode 1012\n",
            " \tTotal reward: -113.85432434082031\n",
            " \tTraining loss: 10.3328\n",
            " \tExplore P: 1.0056\n",
            "\n",
            "Episode 1013\n",
            " \tTotal reward: -115.99861145019531\n",
            " \tTraining loss: 1.4820\n",
            " \tExplore P: 1.0056\n",
            "\n",
            "Episode 1014\n",
            " \tTotal reward: -94.51585388183594\n",
            " \tTraining loss: 1.4562\n",
            " \tExplore P: 1.0056\n",
            "\n",
            "Episode 1015\n",
            " \tTotal reward: -105.00782775878906\n",
            " \tTraining loss: 5.0299\n",
            " \tExplore P: 1.0055\n",
            "\n",
            "Model saved\n",
            "Episode 1016\n",
            " \tTotal reward: -95.96492004394531\n",
            " \tTraining loss: 1.1504\n",
            " \tExplore P: 1.0055\n",
            "\n",
            "Episode 1017\n",
            " \tTotal reward: -90.88365173339844\n",
            " \tTraining loss: 1.5249\n",
            " \tExplore P: 1.0055\n",
            "\n",
            "Episode 1018\n",
            " \tTotal reward: -113.16825866699219\n",
            " \tTraining loss: 5.5169\n",
            " \tExplore P: 1.0054\n",
            "\n",
            "Episode 1019\n",
            " \tTotal reward: -101.20681762695312\n",
            " \tTraining loss: 0.8497\n",
            " \tExplore P: 1.0054\n",
            "\n",
            "Episode 1020\n",
            " \tTotal reward: -113.28376770019531\n",
            " \tTraining loss: 0.7427\n",
            " \tExplore P: 1.0054\n",
            "\n",
            "Model saved\n",
            "Episode 1021\n",
            " \tTotal reward: -101.60545349121094\n",
            " \tTraining loss: 3.2922\n",
            " \tExplore P: 1.0054\n",
            "\n",
            "Episode 1022\n",
            " \tTotal reward: -115.944580078125\n",
            " \tTraining loss: 2.4171\n",
            " \tExplore P: 1.0053\n",
            "\n",
            "Episode 1023\n",
            " \tTotal reward: -112.52531433105469\n",
            " \tTraining loss: 0.8932\n",
            " \tExplore P: 1.0053\n",
            "\n",
            "Episode 1024\n",
            " \tTotal reward: -85.02153015136719\n",
            " \tTraining loss: 2.2102\n",
            " \tExplore P: 1.0053\n",
            "\n",
            "Episode 1025\n",
            " \tTotal reward: -115.61170959472656\n",
            " \tTraining loss: 8.8227\n",
            " \tExplore P: 1.0053\n",
            "\n",
            "Model saved\n",
            "Episode 1026\n",
            " \tTotal reward: -114.59303283691406\n",
            " \tTraining loss: 1.0907\n",
            " \tExplore P: 1.0052\n",
            "\n",
            "Episode 1027\n",
            " \tTotal reward: -99.09255981445312\n",
            " \tTraining loss: 0.9885\n",
            " \tExplore P: 1.0052\n",
            "\n",
            "Episode 1028\n",
            " \tTotal reward: -99.14996337890625\n",
            " \tTraining loss: 3.6512\n",
            " \tExplore P: 1.0052\n",
            "\n",
            "Episode 1029\n",
            " \tTotal reward: -115.90975952148438\n",
            " \tTraining loss: 3.9346\n",
            " \tExplore P: 1.0051\n",
            "\n",
            "Episode 1030\n",
            " \tTotal reward: -112.8392333984375\n",
            " \tTraining loss: 1.1617\n",
            " \tExplore P: 1.0051\n",
            "\n",
            "Model saved\n",
            "Episode 1031\n",
            " \tTotal reward: -110.62576293945312\n",
            " \tTraining loss: 2.2665\n",
            " \tExplore P: 1.0051\n",
            "\n",
            "Episode 1032\n",
            " \tTotal reward: -99.61102294921875\n",
            " \tTraining loss: 3.7628\n",
            " \tExplore P: 1.0051\n",
            "\n",
            "Episode 1033\n",
            " \tTotal reward: -98.14205932617188\n",
            " \tTraining loss: 0.6061\n",
            " \tExplore P: 1.0050\n",
            "\n",
            "Episode 1034\n",
            " \tTotal reward: -104.04815673828125\n",
            " \tTraining loss: 2.0512\n",
            " \tExplore P: 1.0050\n",
            "\n",
            "Episode 1035\n",
            " \tTotal reward: -87.3201904296875\n",
            " \tTraining loss: 5.4592\n",
            " \tExplore P: 1.0050\n",
            "\n",
            "Model saved\n",
            "Episode 1036\n",
            " \tTotal reward: -113.87481689453125\n",
            " \tTraining loss: 2.4771\n",
            " \tExplore P: 1.0049\n",
            "\n",
            "Episode 1037\n",
            " \tTotal reward: -113.7305908203125\n",
            " \tTraining loss: 0.8163\n",
            " \tExplore P: 1.0049\n",
            "\n",
            "Episode 1038\n",
            " \tTotal reward: -102.16062927246094\n",
            " \tTraining loss: 1.8400\n",
            " \tExplore P: 1.0049\n",
            "\n",
            "Episode 1039\n",
            " \tTotal reward: -109.28038024902344\n",
            " \tTraining loss: 2.0742\n",
            " \tExplore P: 1.0049\n",
            "\n",
            "Episode 1040\n",
            " \tTotal reward: -112.34674072265625\n",
            " \tTraining loss: 1.1823\n",
            " \tExplore P: 1.0049\n",
            "\n",
            "Model saved\n",
            "Episode 1041\n",
            " \tTotal reward: -115.9757080078125\n",
            " \tTraining loss: 1.9491\n",
            " \tExplore P: 1.0048\n",
            "\n",
            "Episode 1042\n",
            " \tTotal reward: -80.88262939453125\n",
            " \tTraining loss: 1.0355\n",
            " \tExplore P: 1.0048\n",
            "\n",
            "Episode 1043\n",
            " \tTotal reward: -105.14295959472656\n",
            " \tTraining loss: 1.3949\n",
            " \tExplore P: 1.0048\n",
            "\n",
            "Episode 1044\n",
            " \tTotal reward: -104.67060852050781\n",
            " \tTraining loss: 9.3706\n",
            " \tExplore P: 1.0048\n",
            "\n",
            "Episode 1045\n",
            " \tTotal reward: -97.35165405273438\n",
            " \tTraining loss: 1.6295\n",
            " \tExplore P: 1.0048\n",
            "\n",
            "Model saved\n",
            "Episode 1046\n",
            " \tTotal reward: -93.55854797363281\n",
            " \tTraining loss: 2.8539\n",
            " \tExplore P: 1.0047\n",
            "\n",
            "Episode 1047\n",
            " \tTotal reward: -106.55097961425781\n",
            " \tTraining loss: 5.9667\n",
            " \tExplore P: 1.0047\n",
            "\n",
            "Episode 1048\n",
            " \tTotal reward: -109.21525573730469\n",
            " \tTraining loss: 1.1040\n",
            " \tExplore P: 1.0047\n",
            "\n",
            "Episode 1049\n",
            " \tTotal reward: -112.60537719726562\n",
            " \tTraining loss: 7.3442\n",
            " \tExplore P: 1.0047\n",
            "\n",
            "Episode 1050\n",
            " \tTotal reward: -111.90797424316406\n",
            " \tTraining loss: 1.5642\n",
            " \tExplore P: 1.0047\n",
            "\n",
            "Model saved\n",
            "Episode 1051\n",
            " \tTotal reward: -98.87773132324219\n",
            " \tTraining loss: 1.0898\n",
            " \tExplore P: 1.0046\n",
            "\n",
            "Episode 1052\n",
            " \tTotal reward: -102.10612487792969\n",
            " \tTraining loss: 0.7252\n",
            " \tExplore P: 1.0046\n",
            "\n",
            "Episode 1053\n",
            " \tTotal reward: -98.85200500488281\n",
            " \tTraining loss: 1.2712\n",
            " \tExplore P: 1.0046\n",
            "\n",
            "Episode 1054\n",
            " \tTotal reward: -105.64527893066406\n",
            " \tTraining loss: 1.2212\n",
            " \tExplore P: 1.0046\n",
            "\n",
            "Episode 1055\n",
            " \tTotal reward: -115.21955871582031\n",
            " \tTraining loss: 1.2279\n",
            " \tExplore P: 1.0045\n",
            "\n",
            "Model saved\n",
            "Episode 1056\n",
            " \tTotal reward: -101.54962158203125\n",
            " \tTraining loss: 3.2468\n",
            " \tExplore P: 1.0045\n",
            "\n",
            "Episode 1057\n",
            " \tTotal reward: -99.95941162109375\n",
            " \tTraining loss: 0.9669\n",
            " \tExplore P: 1.0045\n",
            "\n",
            "Episode 1058\n",
            " \tTotal reward: -115.80389404296875\n",
            " \tTraining loss: 2.0696\n",
            " \tExplore P: 1.0045\n",
            "\n",
            "Episode 1059\n",
            " \tTotal reward: -97.50587463378906\n",
            " \tTraining loss: 1.1533\n",
            " \tExplore P: 1.0044\n",
            "\n",
            "Episode 1060\n",
            " \tTotal reward: -113.52622985839844\n",
            " \tTraining loss: 1.9686\n",
            " \tExplore P: 1.0044\n",
            "\n",
            "Model saved\n",
            "Episode 1061\n",
            " \tTotal reward: -106.94622802734375\n",
            " \tTraining loss: 8.0880\n",
            " \tExplore P: 1.0044\n",
            "\n",
            "Episode 1062\n",
            " \tTotal reward: -115.11166381835938\n",
            " \tTraining loss: 0.6204\n",
            " \tExplore P: 1.0044\n",
            "\n",
            "Episode 1063\n",
            " \tTotal reward: -101.48345947265625\n",
            " \tTraining loss: 0.9325\n",
            " \tExplore P: 1.0044\n",
            "\n",
            "Episode 1064\n",
            " \tTotal reward: -115.95079040527344\n",
            " \tTraining loss: 3.1781\n",
            " \tExplore P: 1.0043\n",
            "\n",
            "Episode 1065\n",
            " \tTotal reward: -99.04293823242188\n",
            " \tTraining loss: 9.0669\n",
            " \tExplore P: 1.0043\n",
            "\n",
            "Model saved\n",
            "Episode 1066\n",
            " \tTotal reward: -46.11723327636719\n",
            " \tTraining loss: 0.5971\n",
            " \tExplore P: 1.0043\n",
            "\n",
            "Episode 1067\n",
            " \tTotal reward: -83.00520324707031\n",
            " \tTraining loss: 1.3874\n",
            " \tExplore P: 1.0042\n",
            "\n",
            "Episode 1068\n",
            " \tTotal reward: -57.23530578613281\n",
            " \tTraining loss: 0.9591\n",
            " \tExplore P: 1.0042\n",
            "\n",
            "Episode 1069\n",
            " \tTotal reward: -100.0174560546875\n",
            " \tTraining loss: 1.0096\n",
            " \tExplore P: 1.0042\n",
            "\n",
            "Episode 1070\n",
            " \tTotal reward: -101.65074157714844\n",
            " \tTraining loss: 1.9552\n",
            " \tExplore P: 1.0042\n",
            "\n",
            "Model saved\n",
            "Episode 1071\n",
            " \tTotal reward: -81.77787780761719\n",
            " \tTraining loss: 2.1851\n",
            " \tExplore P: 1.0041\n",
            "\n",
            "Episode 1072\n",
            " \tTotal reward: -107.34840393066406\n",
            " \tTraining loss: 4.9348\n",
            " \tExplore P: 1.0041\n",
            "\n",
            "Episode 1073\n",
            " \tTotal reward: -115.95480346679688\n",
            " \tTraining loss: 2.6392\n",
            " \tExplore P: 1.0041\n",
            "\n",
            "Episode 1074\n",
            " \tTotal reward: -102.17503356933594\n",
            " \tTraining loss: 7.4208\n",
            " \tExplore P: 1.0041\n",
            "\n",
            "Episode 1075\n",
            " \tTotal reward: -106.13792419433594\n",
            " \tTraining loss: 4.2500\n",
            " \tExplore P: 1.0041\n",
            "\n",
            "Model saved\n",
            "Model updated\n",
            "Episode 1076\n",
            " \tTotal reward: -107.91270446777344\n",
            " \tTraining loss: 7.5035\n",
            " \tExplore P: 1.0041\n",
            "\n",
            "Episode 1077\n",
            " \tTotal reward: -102.39207458496094\n",
            " \tTraining loss: 1.3782\n",
            " \tExplore P: 1.0041\n",
            "\n",
            "Episode 1078\n",
            " \tTotal reward: -87.41520690917969\n",
            " \tTraining loss: 1.1573\n",
            " \tExplore P: 1.0040\n",
            "\n",
            "Episode 1079\n",
            " \tTotal reward: -87.66610717773438\n",
            " \tTraining loss: 3.5306\n",
            " \tExplore P: 1.0040\n",
            "\n",
            "Episode 1080\n",
            " \tTotal reward: -98.7532958984375\n",
            " \tTraining loss: 15.0682\n",
            " \tExplore P: 1.0040\n",
            "\n",
            "Model saved\n",
            "Episode 1081\n",
            " \tTotal reward: -91.65718078613281\n",
            " \tTraining loss: 7.7040\n",
            " \tExplore P: 1.0040\n",
            "\n",
            "Episode 1082\n",
            " \tTotal reward: -81.517822265625\n",
            " \tTraining loss: 3.8745\n",
            " \tExplore P: 1.0040\n",
            "\n",
            "Episode 1083\n",
            " \tTotal reward: -96.3218994140625\n",
            " \tTraining loss: 2.3724\n",
            " \tExplore P: 1.0039\n",
            "\n",
            "Episode 1084\n",
            " \tTotal reward: -109.84400939941406\n",
            " \tTraining loss: 1.9763\n",
            " \tExplore P: 1.0039\n",
            "\n",
            "Episode 1085\n",
            " \tTotal reward: -115.88392639160156\n",
            " \tTraining loss: 0.9506\n",
            " \tExplore P: 1.0039\n",
            "\n",
            "Model saved\n",
            "Episode 1086\n",
            " \tTotal reward: -114.12873840332031\n",
            " \tTraining loss: 0.9779\n",
            " \tExplore P: 1.0039\n",
            "\n",
            "Episode 1087\n",
            " \tTotal reward: -68.24966430664062\n",
            " \tTraining loss: 1.5183\n",
            " \tExplore P: 1.0039\n",
            "\n",
            "Episode 1088\n",
            " \tTotal reward: -115.221435546875\n",
            " \tTraining loss: 1.1643\n",
            " \tExplore P: 1.0038\n",
            "\n",
            "Episode 1089\n",
            " \tTotal reward: -111.42462158203125\n",
            " \tTraining loss: 2.0331\n",
            " \tExplore P: 1.0038\n",
            "\n",
            "Episode 1090\n",
            " \tTotal reward: 7.6886749267578125\n",
            " \tTraining loss: 1.2221\n",
            " \tExplore P: 1.0036\n",
            "\n",
            "Model saved\n",
            "Episode 1091\n",
            " \tTotal reward: -113.70108032226562\n",
            " \tTraining loss: 1.2371\n",
            " \tExplore P: 1.0036\n",
            "\n",
            "Episode 1092\n",
            " \tTotal reward: -107.51602172851562\n",
            " \tTraining loss: 1.7147\n",
            " \tExplore P: 1.0036\n",
            "\n",
            "Episode 1093\n",
            " \tTotal reward: -114.90478515625\n",
            " \tTraining loss: 3.4660\n",
            " \tExplore P: 1.0036\n",
            "\n",
            "Episode 1094\n",
            " \tTotal reward: -92.18930053710938\n",
            " \tTraining loss: 1.3572\n",
            " \tExplore P: 1.0036\n",
            "\n",
            "Episode 1095\n",
            " \tTotal reward: -115.23443603515625\n",
            " \tTraining loss: 2.2973\n",
            " \tExplore P: 1.0035\n",
            "\n",
            "Model saved\n",
            "Episode 1096\n",
            " \tTotal reward: -108.36074829101562\n",
            " \tTraining loss: 1.9586\n",
            " \tExplore P: 1.0035\n",
            "\n",
            "Episode 1097\n",
            " \tTotal reward: -99.18153381347656\n",
            " \tTraining loss: 6.2520\n",
            " \tExplore P: 1.0035\n",
            "\n",
            "Episode 1098\n",
            " \tTotal reward: -105.00047302246094\n",
            " \tTraining loss: 1.9676\n",
            " \tExplore P: 1.0035\n",
            "\n",
            "Episode 1099\n",
            " \tTotal reward: -86.17143249511719\n",
            " \tTraining loss: 1.1481\n",
            " \tExplore P: 1.0035\n",
            "\n",
            "Episode 1100\n",
            " \tTotal reward: -111.11717224121094\n",
            " \tTraining loss: 1.0525\n",
            " \tExplore P: 1.0035\n",
            "\n",
            "Model saved\n",
            "Episode 1101\n",
            " \tTotal reward: -108.60321044921875\n",
            " \tTraining loss: 2.9312\n",
            " \tExplore P: 1.0034\n",
            "\n",
            "Episode 1102\n",
            " \tTotal reward: -93.44001770019531\n",
            " \tTraining loss: 1.4523\n",
            " \tExplore P: 1.0034\n",
            "\n",
            "Episode 1103\n",
            " \tTotal reward: -87.94963073730469\n",
            " \tTraining loss: 1.3792\n",
            " \tExplore P: 1.0034\n",
            "\n",
            "Episode 1104\n",
            " \tTotal reward: -111.23866271972656\n",
            " \tTraining loss: 2.4054\n",
            " \tExplore P: 1.0034\n",
            "\n",
            "Episode 1105\n",
            " \tTotal reward: -105.02528381347656\n",
            " \tTraining loss: 1.3538\n",
            " \tExplore P: 1.0034\n",
            "\n",
            "Model saved\n",
            "Episode 1106\n",
            " \tTotal reward: -115.70558166503906\n",
            " \tTraining loss: 0.7887\n",
            " \tExplore P: 1.0034\n",
            "\n",
            "Episode 1107\n",
            " \tTotal reward: -47.629608154296875\n",
            " \tTraining loss: 1.2492\n",
            " \tExplore P: 1.0033\n",
            "\n",
            "Episode 1108\n",
            " \tTotal reward: -115.98565673828125\n",
            " \tTraining loss: 0.8766\n",
            " \tExplore P: 1.0033\n",
            "\n",
            "Episode 1109\n",
            " \tTotal reward: -110.31338500976562\n",
            " \tTraining loss: 1.0935\n",
            " \tExplore P: 1.0033\n",
            "\n",
            "Episode 1110\n",
            " \tTotal reward: -97.68350219726562\n",
            " \tTraining loss: 3.6231\n",
            " \tExplore P: 1.0033\n",
            "\n",
            "Model saved\n",
            "Episode 1111\n",
            " \tTotal reward: -115.28842163085938\n",
            " \tTraining loss: 1.2396\n",
            " \tExplore P: 1.0033\n",
            "\n",
            "Episode 1112\n",
            " \tTotal reward: -114.34628295898438\n",
            " \tTraining loss: 1.4315\n",
            " \tExplore P: 1.0033\n",
            "\n",
            "Episode 1113\n",
            " \tTotal reward: -114.42025756835938\n",
            " \tTraining loss: 1.0480\n",
            " \tExplore P: 1.0033\n",
            "\n",
            "Episode 1114\n",
            " \tTotal reward: -64.5302734375\n",
            " \tTraining loss: 1.0904\n",
            " \tExplore P: 1.0032\n",
            "\n",
            "Episode 1115\n",
            " \tTotal reward: -115.98442077636719\n",
            " \tTraining loss: 1.2227\n",
            " \tExplore P: 1.0032\n",
            "\n",
            "Model saved\n",
            "Episode 1116\n",
            " \tTotal reward: -115.2218017578125\n",
            " \tTraining loss: 0.8117\n",
            " \tExplore P: 1.0032\n",
            "\n",
            "Episode 1117\n",
            " \tTotal reward: -110.07310485839844\n",
            " \tTraining loss: 0.9634\n",
            " \tExplore P: 1.0032\n",
            "\n",
            "Episode 1118\n",
            " \tTotal reward: -82.14036560058594\n",
            " \tTraining loss: 1.8226\n",
            " \tExplore P: 1.0032\n",
            "\n",
            "Episode 1119\n",
            " \tTotal reward: -103.61109924316406\n",
            " \tTraining loss: 1.2584\n",
            " \tExplore P: 1.0031\n",
            "\n",
            "Episode 1120\n",
            " \tTotal reward: -114.47537231445312\n",
            " \tTraining loss: 3.7795\n",
            " \tExplore P: 1.0031\n",
            "\n",
            "Model saved\n",
            "Episode 1121\n",
            " \tTotal reward: -113.41775512695312\n",
            " \tTraining loss: 2.7277\n",
            " \tExplore P: 1.0031\n",
            "\n",
            "Episode 1122\n",
            " \tTotal reward: -113.13529968261719\n",
            " \tTraining loss: 1.3360\n",
            " \tExplore P: 1.0031\n",
            "\n",
            "Episode 1123\n",
            " \tTotal reward: -110.88505554199219\n",
            " \tTraining loss: 0.7431\n",
            " \tExplore P: 1.0030\n",
            "\n",
            "Episode 1124\n",
            " \tTotal reward: -109.76849365234375\n",
            " \tTraining loss: 0.9358\n",
            " \tExplore P: 1.0030\n",
            "\n",
            "Episode 1125\n",
            " \tTotal reward: -109.18086242675781\n",
            " \tTraining loss: 1.3835\n",
            " \tExplore P: 1.0030\n",
            "\n",
            "Model saved\n",
            "Episode 1126\n",
            " \tTotal reward: -115.267333984375\n",
            " \tTraining loss: 1.1023\n",
            " \tExplore P: 1.0030\n",
            "\n",
            "Episode 1127\n",
            " \tTotal reward: -100.75169372558594\n",
            " \tTraining loss: 15.5598\n",
            " \tExplore P: 1.0030\n",
            "\n",
            "Episode 1128\n",
            " \tTotal reward: -97.07124328613281\n",
            " \tTraining loss: 0.8240\n",
            " \tExplore P: 1.0030\n",
            "\n",
            "Episode 1129\n",
            " \tTotal reward: -115.85389709472656\n",
            " \tTraining loss: 1.3882\n",
            " \tExplore P: 1.0030\n",
            "\n",
            "Episode 1130\n",
            " \tTotal reward: -113.33665466308594\n",
            " \tTraining loss: 0.5366\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Model saved\n",
            "Episode 1131\n",
            " \tTotal reward: -88.355224609375\n",
            " \tTraining loss: 1.0569\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Episode 1132\n",
            " \tTotal reward: -115.98731994628906\n",
            " \tTraining loss: 1.8049\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Episode 1133\n",
            " \tTotal reward: -101.6473388671875\n",
            " \tTraining loss: 1.2900\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Episode 1134\n",
            " \tTotal reward: -115.99858093261719\n",
            " \tTraining loss: 1.1223\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Episode 1135\n",
            " \tTotal reward: -94.81971740722656\n",
            " \tTraining loss: 0.9989\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Model saved\n",
            "Episode 1136\n",
            " \tTotal reward: -115.98080444335938\n",
            " \tTraining loss: 1.8261\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Episode 1137\n",
            " \tTotal reward: -97.01866149902344\n",
            " \tTraining loss: 0.6847\n",
            " \tExplore P: 1.0029\n",
            "\n",
            "Episode 1138\n",
            " \tTotal reward: -109.71755981445312\n",
            " \tTraining loss: 13.9211\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Episode 1139\n",
            " \tTotal reward: -113.27728271484375\n",
            " \tTraining loss: 1.1237\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Episode 1140\n",
            " \tTotal reward: -98.99313354492188\n",
            " \tTraining loss: 3.4230\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Model saved\n",
            "Episode 1141\n",
            " \tTotal reward: -106.88735961914062\n",
            " \tTraining loss: 1.2937\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Episode 1142\n",
            " \tTotal reward: -110.40928649902344\n",
            " \tTraining loss: 1.0788\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Episode 1143\n",
            " \tTotal reward: -103.20953369140625\n",
            " \tTraining loss: 0.7426\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Episode 1144\n",
            " \tTotal reward: -115.89683532714844\n",
            " \tTraining loss: 0.6950\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Episode 1145\n",
            " \tTotal reward: -86.16133117675781\n",
            " \tTraining loss: 0.7596\n",
            " \tExplore P: 1.0028\n",
            "\n",
            "Model saved\n",
            "Episode 1146\n",
            " \tTotal reward: -112.18646240234375\n",
            " \tTraining loss: 1.0842\n",
            " \tExplore P: 1.0027\n",
            "\n",
            "Episode 1147\n",
            " \tTotal reward: -69.83050537109375\n",
            " \tTraining loss: 0.9029\n",
            " \tExplore P: 1.0027\n",
            "\n",
            "Episode 1148\n",
            " \tTotal reward: -115.97682189941406\n",
            " \tTraining loss: 1.6885\n",
            " \tExplore P: 1.0027\n",
            "\n",
            "Episode 1149\n",
            " \tTotal reward: -114.87217712402344\n",
            " \tTraining loss: 1.3853\n",
            " \tExplore P: 1.0027\n",
            "\n",
            "Episode 1150\n",
            " \tTotal reward: -84.65884399414062\n",
            " \tTraining loss: 2.7451\n",
            " \tExplore P: 1.0027\n",
            "\n",
            "Model saved\n",
            "Episode 1151\n",
            " \tTotal reward: -99.52081298828125\n",
            " \tTraining loss: 1.1416\n",
            " \tExplore P: 1.0026\n",
            "\n",
            "Episode 1152\n",
            " \tTotal reward: -115.977294921875\n",
            " \tTraining loss: 0.9222\n",
            " \tExplore P: 1.0026\n",
            "\n",
            "Episode 1153\n",
            " \tTotal reward: -85.17701721191406\n",
            " \tTraining loss: 1.0080\n",
            " \tExplore P: 1.0026\n",
            "\n",
            "Episode 1154\n",
            " \tTotal reward: -105.70510864257812\n",
            " \tTraining loss: 0.7704\n",
            " \tExplore P: 1.0026\n",
            "\n",
            "Episode 1155\n",
            " \tTotal reward: -96.12332153320312\n",
            " \tTraining loss: 0.8165\n",
            " \tExplore P: 1.0026\n",
            "\n",
            "Model saved\n",
            "Episode 1156\n",
            " \tTotal reward: -110.77578735351562\n",
            " \tTraining loss: 2.5768\n",
            " \tExplore P: 1.0026\n",
            "\n",
            "Episode 1157\n",
            " \tTotal reward: -113.9327392578125\n",
            " \tTraining loss: 0.8154\n",
            " \tExplore P: 1.0026\n",
            "\n",
            "Episode 1158\n",
            " \tTotal reward: -115.23776245117188\n",
            " \tTraining loss: 0.8224\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Episode 1159\n",
            " \tTotal reward: -115.97492980957031\n",
            " \tTraining loss: 1.3190\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Episode 1160\n",
            " \tTotal reward: -94.19094848632812\n",
            " \tTraining loss: 0.6878\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Model saved\n",
            "Episode 1161\n",
            " \tTotal reward: -111.39105224609375\n",
            " \tTraining loss: 12.5271\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Episode 1162\n",
            " \tTotal reward: -101.02479553222656\n",
            " \tTraining loss: 2.6348\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Model updated\n",
            "Episode 1163\n",
            " \tTotal reward: -105.07273864746094\n",
            " \tTraining loss: 0.9415\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Episode 1164\n",
            " \tTotal reward: -91.11920166015625\n",
            " \tTraining loss: 3.4633\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Episode 1165\n",
            " \tTotal reward: -87.95819091796875\n",
            " \tTraining loss: 3.0746\n",
            " \tExplore P: 1.0025\n",
            "\n",
            "Model saved\n",
            "Episode 1166\n",
            " \tTotal reward: -105.44645690917969\n",
            " \tTraining loss: 1.6913\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1167\n",
            " \tTotal reward: -113.43621826171875\n",
            " \tTraining loss: 1.3065\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1168\n",
            " \tTotal reward: -94.13941955566406\n",
            " \tTraining loss: 1.9393\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1169\n",
            " \tTotal reward: -109.58160400390625\n",
            " \tTraining loss: 3.3077\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1170\n",
            " \tTotal reward: -82.50657653808594\n",
            " \tTraining loss: 1.7733\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Model saved\n",
            "Episode 1171\n",
            " \tTotal reward: -115.97718811035156\n",
            " \tTraining loss: 3.6899\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1172\n",
            " \tTotal reward: -113.29177856445312\n",
            " \tTraining loss: 1.4609\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1173\n",
            " \tTotal reward: -115.96365356445312\n",
            " \tTraining loss: 1.7469\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1174\n",
            " \tTotal reward: -106.32769775390625\n",
            " \tTraining loss: 1.8030\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Episode 1175\n",
            " \tTotal reward: -27.068038940429688\n",
            " \tTraining loss: 4.1761\n",
            " \tExplore P: 1.0024\n",
            "\n",
            "Model saved\n",
            "Episode 1176\n",
            " \tTotal reward: -106.27810668945312\n",
            " \tTraining loss: 2.2868\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1177\n",
            " \tTotal reward: -114.78315734863281\n",
            " \tTraining loss: 1.7584\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1178\n",
            " \tTotal reward: -107.11735534667969\n",
            " \tTraining loss: 3.3842\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1179\n",
            " \tTotal reward: -115.87312316894531\n",
            " \tTraining loss: 1.4957\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1180\n",
            " \tTotal reward: -110.58131408691406\n",
            " \tTraining loss: 1.8004\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Model saved\n",
            "Episode 1181\n",
            " \tTotal reward: -114.72488403320312\n",
            " \tTraining loss: 1.5600\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1182\n",
            " \tTotal reward: -112.63534545898438\n",
            " \tTraining loss: 1.3732\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1183\n",
            " \tTotal reward: -108.09963989257812\n",
            " \tTraining loss: 0.9121\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1184\n",
            " \tTotal reward: -115.99784851074219\n",
            " \tTraining loss: 2.5616\n",
            " \tExplore P: 1.0023\n",
            "\n",
            "Episode 1185\n",
            " \tTotal reward: -98.84263610839844\n",
            " \tTraining loss: 0.9091\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Model saved\n",
            "Episode 1186\n",
            " \tTotal reward: -115.94010925292969\n",
            " \tTraining loss: 0.7962\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Episode 1187\n",
            " \tTotal reward: -115.97322082519531\n",
            " \tTraining loss: 7.1544\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Episode 1188\n",
            " \tTotal reward: -112.97531127929688\n",
            " \tTraining loss: 1.3184\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Episode 1189\n",
            " \tTotal reward: -83.20491027832031\n",
            " \tTraining loss: 1.9830\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Episode 1190\n",
            " \tTotal reward: -109.97499084472656\n",
            " \tTraining loss: 1.1433\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Model saved\n",
            "Episode 1191\n",
            " \tTotal reward: -115.92758178710938\n",
            " \tTraining loss: 5.4279\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Episode 1192\n",
            " \tTotal reward: -110.24613952636719\n",
            " \tTraining loss: 2.6925\n",
            " \tExplore P: 1.0022\n",
            "\n",
            "Episode 1193\n",
            " \tTotal reward: -115.73503112792969\n",
            " \tTraining loss: 1.2768\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Episode 1194\n",
            " \tTotal reward: -114.20443725585938\n",
            " \tTraining loss: 1.5096\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Episode 1195\n",
            " \tTotal reward: -107.98637390136719\n",
            " \tTraining loss: 16.8558\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Model saved\n",
            "Episode 1196\n",
            " \tTotal reward: -86.66224670410156\n",
            " \tTraining loss: 3.3164\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Episode 1197\n",
            " \tTotal reward: -115.2315673828125\n",
            " \tTraining loss: 1.1921\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Episode 1198\n",
            " \tTotal reward: -113.23977661132812\n",
            " \tTraining loss: 3.5518\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Episode 1199\n",
            " \tTotal reward: -94.84727478027344\n",
            " \tTraining loss: 6.3687\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Episode 1200\n",
            " \tTotal reward: -89.07293701171875\n",
            " \tTraining loss: 1.4452\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Model saved\n",
            "Episode 1201\n",
            " \tTotal reward: -104.35060119628906\n",
            " \tTraining loss: 1.7852\n",
            " \tExplore P: 1.0021\n",
            "\n",
            "Episode 1202\n",
            " \tTotal reward: -115.94944763183594\n",
            " \tTraining loss: 0.9709\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1203\n",
            " \tTotal reward: -111.64559936523438\n",
            " \tTraining loss: 2.0097\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1204\n",
            " \tTotal reward: -108.85159301757812\n",
            " \tTraining loss: 2.8265\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1205\n",
            " \tTotal reward: -115.08468627929688\n",
            " \tTraining loss: 1.3739\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Model saved\n",
            "Episode 1206\n",
            " \tTotal reward: -115.92172241210938\n",
            " \tTraining loss: 0.9945\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1207\n",
            " \tTotal reward: -115.75682067871094\n",
            " \tTraining loss: 1.9607\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1208\n",
            " \tTotal reward: -91.820556640625\n",
            " \tTraining loss: 1.7300\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1209\n",
            " \tTotal reward: -97.81181335449219\n",
            " \tTraining loss: 1.2399\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1210\n",
            " \tTotal reward: -115.18760681152344\n",
            " \tTraining loss: 1.6085\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Model saved\n",
            "Episode 1211\n",
            " \tTotal reward: -106.66592407226562\n",
            " \tTraining loss: 1.1826\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1212\n",
            " \tTotal reward: -93.42132568359375\n",
            " \tTraining loss: 1.3507\n",
            " \tExplore P: 1.0020\n",
            "\n",
            "Episode 1213\n",
            " \tTotal reward: -115.97576904296875\n",
            " \tTraining loss: 0.8655\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1214\n",
            " \tTotal reward: -102.65626525878906\n",
            " \tTraining loss: 1.3494\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1215\n",
            " \tTotal reward: -111.65888977050781\n",
            " \tTraining loss: 1.0134\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Model saved\n",
            "Episode 1216\n",
            " \tTotal reward: -90.49468994140625\n",
            " \tTraining loss: 1.1868\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1217\n",
            " \tTotal reward: -115.97859191894531\n",
            " \tTraining loss: 1.3678\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1218\n",
            " \tTotal reward: -110.39816284179688\n",
            " \tTraining loss: 0.8779\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1219\n",
            " \tTotal reward: -93.83193969726562\n",
            " \tTraining loss: 1.4020\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1220\n",
            " \tTotal reward: -102.95164489746094\n",
            " \tTraining loss: 1.0483\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Model saved\n",
            "Episode 1221\n",
            " \tTotal reward: -112.95108032226562\n",
            " \tTraining loss: 1.3313\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1222\n",
            " \tTotal reward: -115.57797241210938\n",
            " \tTraining loss: 1.4951\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1223\n",
            " \tTotal reward: -99.03121948242188\n",
            " \tTraining loss: 1.1924\n",
            " \tExplore P: 1.0019\n",
            "\n",
            "Episode 1224\n",
            " \tTotal reward: -94.61239624023438\n",
            " \tTraining loss: 1.1670\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1225\n",
            " \tTotal reward: -68.83438110351562\n",
            " \tTraining loss: 1.4656\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Model saved\n",
            "Episode 1226\n",
            " \tTotal reward: -112.05844116210938\n",
            " \tTraining loss: 1.6424\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1227\n",
            " \tTotal reward: -112.04891967773438\n",
            " \tTraining loss: 1.2744\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1228\n",
            " \tTotal reward: -108.3463134765625\n",
            " \tTraining loss: 1.6711\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1229\n",
            " \tTotal reward: -110.03453063964844\n",
            " \tTraining loss: 1.1216\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1230\n",
            " \tTotal reward: -93.82284545898438\n",
            " \tTraining loss: 0.8637\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Model saved\n",
            "Episode 1231\n",
            " \tTotal reward: -114.83949279785156\n",
            " \tTraining loss: 1.2704\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1232\n",
            " \tTotal reward: -98.34190368652344\n",
            " \tTraining loss: 1.1588\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1233\n",
            " \tTotal reward: -106.34257507324219\n",
            " \tTraining loss: 1.8553\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1234\n",
            " \tTotal reward: -115.95677185058594\n",
            " \tTraining loss: 0.6900\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1235\n",
            " \tTotal reward: -109.83059692382812\n",
            " \tTraining loss: 0.7978\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Model saved\n",
            "Episode 1236\n",
            " \tTotal reward: -110.41242980957031\n",
            " \tTraining loss: 1.3961\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1237\n",
            " \tTotal reward: -115.51849365234375\n",
            " \tTraining loss: 1.3166\n",
            " \tExplore P: 1.0018\n",
            "\n",
            "Episode 1238\n",
            " \tTotal reward: -112.68666076660156\n",
            " \tTraining loss: 1.1121\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1239\n",
            " \tTotal reward: -63.47050476074219\n",
            " \tTraining loss: 0.7004\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1240\n",
            " \tTotal reward: -115.94354248046875\n",
            " \tTraining loss: 2.5856\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Model saved\n",
            "Episode 1241\n",
            " \tTotal reward: -113.35469055175781\n",
            " \tTraining loss: 1.6465\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1242\n",
            " \tTotal reward: -112.95283508300781\n",
            " \tTraining loss: 1.0971\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1243\n",
            " \tTotal reward: -102.57177734375\n",
            " \tTraining loss: 5.6889\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1244\n",
            " \tTotal reward: -92.50776672363281\n",
            " \tTraining loss: 0.8864\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1245\n",
            " \tTotal reward: -92.23980712890625\n",
            " \tTraining loss: 0.7323\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Model saved\n",
            "Episode 1246\n",
            " \tTotal reward: -107.05776977539062\n",
            " \tTraining loss: 0.8215\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1247\n",
            " \tTotal reward: -107.43928527832031\n",
            " \tTraining loss: 0.9675\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1248\n",
            " \tTotal reward: -75.44349670410156\n",
            " \tTraining loss: 4.9338\n",
            " \tExplore P: 1.0017\n",
            "\n",
            "Episode 1249\n",
            " \tTotal reward: -87.4892578125\n",
            " \tTraining loss: 11.6014\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1250\n",
            " \tTotal reward: -77.5316162109375\n",
            " \tTraining loss: 1.7110\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Model saved\n",
            "Episode 1251\n",
            " \tTotal reward: -115.60853576660156\n",
            " \tTraining loss: 1.0156\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1252\n",
            " \tTotal reward: -89.36363220214844\n",
            " \tTraining loss: 8.1220\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1253\n",
            " \tTotal reward: -108.91876220703125\n",
            " \tTraining loss: 1.5891\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1254\n",
            " \tTotal reward: -115.99369812011719\n",
            " \tTraining loss: 1.3283\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1255\n",
            " \tTotal reward: -109.02064514160156\n",
            " \tTraining loss: 0.9621\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Model saved\n",
            "Episode 1256\n",
            " \tTotal reward: -115.99806213378906\n",
            " \tTraining loss: 2.4338\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1257\n",
            " \tTotal reward: -115.62594604492188\n",
            " \tTraining loss: 0.9307\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1258\n",
            " \tTotal reward: -113.33555603027344\n",
            " \tTraining loss: 1.3681\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1259\n",
            " \tTotal reward: -94.80302429199219\n",
            " \tTraining loss: 2.4803\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1260\n",
            " \tTotal reward: -115.08009338378906\n",
            " \tTraining loss: 0.9639\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Model saved\n",
            "Episode 1261\n",
            " \tTotal reward: -109.77395629882812\n",
            " \tTraining loss: 1.4411\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1262\n",
            " \tTotal reward: -115.10850524902344\n",
            " \tTraining loss: 1.2632\n",
            " \tExplore P: 1.0016\n",
            "\n",
            "Episode 1263\n",
            " \tTotal reward: -113.3382568359375\n",
            " \tTraining loss: 1.6473\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1264\n",
            " \tTotal reward: -109.72312927246094\n",
            " \tTraining loss: 1.9786\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1265\n",
            " \tTotal reward: -109.317138671875\n",
            " \tTraining loss: 1.5390\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Model saved\n",
            "Episode 1266\n",
            " \tTotal reward: -108.928466796875\n",
            " \tTraining loss: 1.5720\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1267\n",
            " \tTotal reward: -109.27204895019531\n",
            " \tTraining loss: 1.6445\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1268\n",
            " \tTotal reward: -114.726806640625\n",
            " \tTraining loss: 0.9529\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1269\n",
            " \tTotal reward: -93.40638732910156\n",
            " \tTraining loss: 1.6752\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Model updated\n",
            "Episode 1270\n",
            " \tTotal reward: -97.46615600585938\n",
            " \tTraining loss: 2.8481\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Model saved\n",
            "Episode 1271\n",
            " \tTotal reward: -114.31413269042969\n",
            " \tTraining loss: 9.4302\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1272\n",
            " \tTotal reward: -115.99937438964844\n",
            " \tTraining loss: 1.5204\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1273\n",
            " \tTotal reward: -110.34030151367188\n",
            " \tTraining loss: 1.7213\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1274\n",
            " \tTotal reward: -108.56863403320312\n",
            " \tTraining loss: 2.0213\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1275\n",
            " \tTotal reward: -114.51496887207031\n",
            " \tTraining loss: 3.3605\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Model saved\n",
            "Episode 1276\n",
            " \tTotal reward: -102.01242065429688\n",
            " \tTraining loss: 2.2842\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1277\n",
            " \tTotal reward: -115.3314208984375\n",
            " \tTraining loss: 4.1806\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1278\n",
            " \tTotal reward: -115.21774291992188\n",
            " \tTraining loss: 1.6257\n",
            " \tExplore P: 1.0015\n",
            "\n",
            "Episode 1279\n",
            " \tTotal reward: -100.92332458496094\n",
            " \tTraining loss: 1.3795\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1280\n",
            " \tTotal reward: -111.89103698730469\n",
            " \tTraining loss: 4.3381\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Model saved\n",
            "Episode 1281\n",
            " \tTotal reward: -115.17335510253906\n",
            " \tTraining loss: 1.7848\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1282\n",
            " \tTotal reward: -115.99571228027344\n",
            " \tTraining loss: 2.7706\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1283\n",
            " \tTotal reward: -114.95149230957031\n",
            " \tTraining loss: 4.7703\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1284\n",
            " \tTotal reward: -93.28311157226562\n",
            " \tTraining loss: 2.2240\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1285\n",
            " \tTotal reward: -110.48876953125\n",
            " \tTraining loss: 1.3541\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Model saved\n",
            "Episode 1286\n",
            " \tTotal reward: -92.31013488769531\n",
            " \tTraining loss: 1.1568\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1287\n",
            " \tTotal reward: -115.97560119628906\n",
            " \tTraining loss: 1.2821\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1288\n",
            " \tTotal reward: -108.54304504394531\n",
            " \tTraining loss: 1.8794\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1289\n",
            " \tTotal reward: -104.90895080566406\n",
            " \tTraining loss: 0.8941\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1290\n",
            " \tTotal reward: -113.84088134765625\n",
            " \tTraining loss: 1.2574\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Model saved\n",
            "Episode 1291\n",
            " \tTotal reward: -97.31855773925781\n",
            " \tTraining loss: 1.7019\n",
            " \tExplore P: 1.0014\n",
            "\n",
            "Episode 1292\n",
            " \tTotal reward: -111.73977661132812\n",
            " \tTraining loss: 2.4316\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1293\n",
            " \tTotal reward: -92.06767272949219\n",
            " \tTraining loss: 1.1641\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1294\n",
            " \tTotal reward: -113.33544921875\n",
            " \tTraining loss: 0.9150\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1295\n",
            " \tTotal reward: -90.27377319335938\n",
            " \tTraining loss: 1.0624\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Model saved\n",
            "Episode 1296\n",
            " \tTotal reward: -108.68971252441406\n",
            " \tTraining loss: 3.2302\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1297\n",
            " \tTotal reward: -115.92718505859375\n",
            " \tTraining loss: 1.5534\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1298\n",
            " \tTotal reward: -112.51083374023438\n",
            " \tTraining loss: 3.3419\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1299\n",
            " \tTotal reward: -64.78811645507812\n",
            " \tTraining loss: 1.8002\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1300\n",
            " \tTotal reward: -106.47525024414062\n",
            " \tTraining loss: 4.8879\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Model saved\n",
            "Episode 1301\n",
            " \tTotal reward: -94.75123596191406\n",
            " \tTraining loss: 1.3528\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1302\n",
            " \tTotal reward: -103.27653503417969\n",
            " \tTraining loss: 3.5522\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1303\n",
            " \tTotal reward: -98.2020263671875\n",
            " \tTraining loss: 1.3293\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1304\n",
            " \tTotal reward: -115.95002746582031\n",
            " \tTraining loss: 1.0998\n",
            " \tExplore P: 1.0013\n",
            "\n",
            "Episode 1305\n",
            " \tTotal reward: -104.33505249023438\n",
            " \tTraining loss: 0.9805\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Model saved\n",
            "Episode 1306\n",
            " \tTotal reward: -108.65716552734375\n",
            " \tTraining loss: 1.3671\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1307\n",
            " \tTotal reward: -109.63360595703125\n",
            " \tTraining loss: 0.9070\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1308\n",
            " \tTotal reward: -102.14663696289062\n",
            " \tTraining loss: 0.8180\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1309\n",
            " \tTotal reward: -92.81150817871094\n",
            " \tTraining loss: 6.8757\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1310\n",
            " \tTotal reward: -108.83346557617188\n",
            " \tTraining loss: 0.8445\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Model saved\n",
            "Episode 1311\n",
            " \tTotal reward: -107.25106811523438\n",
            " \tTraining loss: 11.1833\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1312\n",
            " \tTotal reward: -101.81718444824219\n",
            " \tTraining loss: 2.1197\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1313\n",
            " \tTotal reward: -115.97633361816406\n",
            " \tTraining loss: 0.9489\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1314\n",
            " \tTotal reward: -109.35282897949219\n",
            " \tTraining loss: 1.1032\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1315\n",
            " \tTotal reward: -93.38111877441406\n",
            " \tTraining loss: 1.5689\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Model saved\n",
            "Episode 1316\n",
            " \tTotal reward: -114.93528747558594\n",
            " \tTraining loss: 1.0724\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1317\n",
            " \tTotal reward: -113.19027709960938\n",
            " \tTraining loss: 1.3095\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1318\n",
            " \tTotal reward: -106.94644165039062\n",
            " \tTraining loss: 8.7668\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1319\n",
            " \tTotal reward: -87.51016235351562\n",
            " \tTraining loss: 1.0380\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Episode 1320\n",
            " \tTotal reward: -104.03169250488281\n",
            " \tTraining loss: 0.8577\n",
            " \tExplore P: 1.0012\n",
            "\n",
            "Model saved\n",
            "Episode 1321\n",
            " \tTotal reward: -109.24893188476562\n",
            " \tTraining loss: 0.7353\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1322\n",
            " \tTotal reward: -111.80867004394531\n",
            " \tTraining loss: 2.3319\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1323\n",
            " \tTotal reward: -108.66099548339844\n",
            " \tTraining loss: 4.0028\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1324\n",
            " \tTotal reward: -108.31843566894531\n",
            " \tTraining loss: 1.5611\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1325\n",
            " \tTotal reward: -105.57807922363281\n",
            " \tTraining loss: 2.6358\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Model saved\n",
            "Episode 1326\n",
            " \tTotal reward: -100.56747436523438\n",
            " \tTraining loss: 15.9489\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1327\n",
            " \tTotal reward: -110.5555419921875\n",
            " \tTraining loss: 9.9729\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1328\n",
            " \tTotal reward: -99.75051879882812\n",
            " \tTraining loss: 2.1586\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1329\n",
            " \tTotal reward: -62.58872985839844\n",
            " \tTraining loss: 1.9100\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1330\n",
            " \tTotal reward: -115.97369384765625\n",
            " \tTraining loss: 12.8383\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Model saved\n",
            "Episode 1331\n",
            " \tTotal reward: -109.91752624511719\n",
            " \tTraining loss: 1.0331\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1332\n",
            " \tTotal reward: -113.84576416015625\n",
            " \tTraining loss: 0.8310\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1333\n",
            " \tTotal reward: -110.53079223632812\n",
            " \tTraining loss: 1.0212\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1334\n",
            " \tTotal reward: -98.1865234375\n",
            " \tTraining loss: 2.4760\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1335\n",
            " \tTotal reward: -113.95210266113281\n",
            " \tTraining loss: 2.0268\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Model saved\n",
            "Episode 1336\n",
            " \tTotal reward: -115.98545837402344\n",
            " \tTraining loss: 1.1567\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1337\n",
            " \tTotal reward: -114.78727722167969\n",
            " \tTraining loss: 0.8517\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1338\n",
            " \tTotal reward: -91.20448303222656\n",
            " \tTraining loss: 10.0083\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1339\n",
            " \tTotal reward: -113.86428833007812\n",
            " \tTraining loss: 1.0609\n",
            " \tExplore P: 1.0011\n",
            "\n",
            "Episode 1340\n",
            " \tTotal reward: -110.79742431640625\n",
            " \tTraining loss: 1.4512\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Model saved\n",
            "Episode 1341\n",
            " \tTotal reward: -115.93022155761719\n",
            " \tTraining loss: 0.9262\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1342\n",
            " \tTotal reward: -115.93487548828125\n",
            " \tTraining loss: 1.0132\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1343\n",
            " \tTotal reward: -106.00985717773438\n",
            " \tTraining loss: 0.9759\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1344\n",
            " \tTotal reward: -96.6138916015625\n",
            " \tTraining loss: 1.1949\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1345\n",
            " \tTotal reward: -108.73316955566406\n",
            " \tTraining loss: 1.6918\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Model saved\n",
            "Episode 1346\n",
            " \tTotal reward: -115.75283813476562\n",
            " \tTraining loss: 2.1987\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1347\n",
            " \tTotal reward: -97.61625671386719\n",
            " \tTraining loss: 1.0650\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1348\n",
            " \tTotal reward: -82.64297485351562\n",
            " \tTraining loss: 1.0682\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1349\n",
            " \tTotal reward: -110.96481323242188\n",
            " \tTraining loss: 0.9350\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1350\n",
            " \tTotal reward: -104.29376220703125\n",
            " \tTraining loss: 1.4144\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Model saved\n",
            "Episode 1351\n",
            " \tTotal reward: -107.98291015625\n",
            " \tTraining loss: 1.6616\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1352\n",
            " \tTotal reward: -114.05433654785156\n",
            " \tTraining loss: 2.9581\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1353\n",
            " \tTotal reward: -99.86158752441406\n",
            " \tTraining loss: 2.6714\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1354\n",
            " \tTotal reward: -115.20106506347656\n",
            " \tTraining loss: 0.9522\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1355\n",
            " \tTotal reward: -93.32763671875\n",
            " \tTraining loss: 1.7286\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Model saved\n",
            "Episode 1356\n",
            " \tTotal reward: -113.04267883300781\n",
            " \tTraining loss: 1.3939\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1357\n",
            " \tTotal reward: -109.60212707519531\n",
            " \tTraining loss: 1.8857\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1358\n",
            " \tTotal reward: -111.30525207519531\n",
            " \tTraining loss: 1.6439\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1359\n",
            " \tTotal reward: -94.03318786621094\n",
            " \tTraining loss: 1.0257\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1360\n",
            " \tTotal reward: -109.67927551269531\n",
            " \tTraining loss: 1.0461\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Model saved\n",
            "Episode 1361\n",
            " \tTotal reward: -114.01409912109375\n",
            " \tTraining loss: 1.0680\n",
            " \tExplore P: 1.0010\n",
            "\n",
            "Episode 1362\n",
            " \tTotal reward: -111.59571838378906\n",
            " \tTraining loss: 0.9088\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1363\n",
            " \tTotal reward: -109.5860595703125\n",
            " \tTraining loss: 0.8981\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1364\n",
            " \tTotal reward: -114.81181335449219\n",
            " \tTraining loss: 18.9971\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1365\n",
            " \tTotal reward: -114.04924011230469\n",
            " \tTraining loss: 1.5094\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Model saved\n",
            "Episode 1366\n",
            " \tTotal reward: -107.78097534179688\n",
            " \tTraining loss: 0.6768\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1367\n",
            " \tTotal reward: -115.99859619140625\n",
            " \tTraining loss: 0.7623\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1368\n",
            " \tTotal reward: -105.48077392578125\n",
            " \tTraining loss: 1.3460\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1369\n",
            " \tTotal reward: -112.65364074707031\n",
            " \tTraining loss: 0.8883\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1370\n",
            " \tTotal reward: -105.89675903320312\n",
            " \tTraining loss: 0.8963\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Model saved\n",
            "Model updated\n",
            "Episode 1371\n",
            " \tTotal reward: -113.854736328125\n",
            " \tTraining loss: 1.7134\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1372\n",
            " \tTotal reward: -113.64431762695312\n",
            " \tTraining loss: 1.5799\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1373\n",
            " \tTotal reward: -103.52227783203125\n",
            " \tTraining loss: 3.9710\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1374\n",
            " \tTotal reward: -115.97560119628906\n",
            " \tTraining loss: 1.3549\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1375\n",
            " \tTotal reward: -93.37815856933594\n",
            " \tTraining loss: 1.2968\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Model saved\n",
            "Episode 1376\n",
            " \tTotal reward: -97.57984924316406\n",
            " \tTraining loss: 2.3356\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1377\n",
            " \tTotal reward: -114.67515563964844\n",
            " \tTraining loss: 2.0061\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1378\n",
            " \tTotal reward: -110.3897705078125\n",
            " \tTraining loss: 4.9661\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1379\n",
            " \tTotal reward: -102.08012390136719\n",
            " \tTraining loss: 3.5602\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1380\n",
            " \tTotal reward: -90.91224670410156\n",
            " \tTraining loss: 9.9723\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Model saved\n",
            "Episode 1381\n",
            " \tTotal reward: -91.68441772460938\n",
            " \tTraining loss: 1.1032\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1382\n",
            " \tTotal reward: -112.01919555664062\n",
            " \tTraining loss: 1.5211\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1383\n",
            " \tTotal reward: -112.10147094726562\n",
            " \tTraining loss: 1.1609\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1384\n",
            " \tTotal reward: -106.97111511230469\n",
            " \tTraining loss: 1.7589\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1385\n",
            " \tTotal reward: -101.07997131347656\n",
            " \tTraining loss: 1.4589\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Model saved\n",
            "Episode 1386\n",
            " \tTotal reward: -115.95591735839844\n",
            " \tTraining loss: 3.5291\n",
            " \tExplore P: 1.0009\n",
            "\n",
            "Episode 1387\n",
            " \tTotal reward: -68.9375\n",
            " \tTraining loss: 19.4082\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1388\n",
            " \tTotal reward: -113.17845153808594\n",
            " \tTraining loss: 0.7763\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1389\n",
            " \tTotal reward: -95.67770385742188\n",
            " \tTraining loss: 1.5386\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1390\n",
            " \tTotal reward: -107.25778198242188\n",
            " \tTraining loss: 0.9630\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Model saved\n",
            "Episode 1391\n",
            " \tTotal reward: -115.05203247070312\n",
            " \tTraining loss: 2.1935\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1392\n",
            " \tTotal reward: -95.24177551269531\n",
            " \tTraining loss: 4.7923\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1393\n",
            " \tTotal reward: -110.98928833007812\n",
            " \tTraining loss: 4.6586\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1394\n",
            " \tTotal reward: -105.32333374023438\n",
            " \tTraining loss: 2.3821\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1395\n",
            " \tTotal reward: -114.10098266601562\n",
            " \tTraining loss: 0.9316\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Model saved\n",
            "Episode 1396\n",
            " \tTotal reward: -108.57958984375\n",
            " \tTraining loss: 1.4636\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1397\n",
            " \tTotal reward: -109.10293579101562\n",
            " \tTraining loss: 5.2172\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1398\n",
            " \tTotal reward: -106.05740356445312\n",
            " \tTraining loss: 1.3446\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1399\n",
            " \tTotal reward: -94.30752563476562\n",
            " \tTraining loss: 2.1400\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1400\n",
            " \tTotal reward: -111.51565551757812\n",
            " \tTraining loss: 0.8079\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Model saved\n",
            "Episode 1401\n",
            " \tTotal reward: -111.2581787109375\n",
            " \tTraining loss: 1.7579\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1402\n",
            " \tTotal reward: -112.39373779296875\n",
            " \tTraining loss: 1.9623\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1403\n",
            " \tTotal reward: -83.68251037597656\n",
            " \tTraining loss: 1.8916\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1404\n",
            " \tTotal reward: -113.50222778320312\n",
            " \tTraining loss: 2.4056\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1405\n",
            " \tTotal reward: -115.97494506835938\n",
            " \tTraining loss: 0.8694\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Model saved\n",
            "Episode 1406\n",
            " \tTotal reward: -114.58682250976562\n",
            " \tTraining loss: 0.9146\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1407\n",
            " \tTotal reward: -109.42427062988281\n",
            " \tTraining loss: 6.5395\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1408\n",
            " \tTotal reward: -105.17079162597656\n",
            " \tTraining loss: 1.6682\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1409\n",
            " \tTotal reward: -104.75053405761719\n",
            " \tTraining loss: 12.8444\n",
            " \tExplore P: 1.0008\n",
            "\n",
            "Episode 1410\n",
            " \tTotal reward: -85.866943359375\n",
            " \tTraining loss: 2.1593\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Model saved\n",
            "Episode 1411\n",
            " \tTotal reward: -94.88079833984375\n",
            " \tTraining loss: 1.1630\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1412\n",
            " \tTotal reward: -75.87701416015625\n",
            " \tTraining loss: 1.6906\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1413\n",
            " \tTotal reward: -111.4007568359375\n",
            " \tTraining loss: 0.6696\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1414\n",
            " \tTotal reward: -115.66201782226562\n",
            " \tTraining loss: 1.4215\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1415\n",
            " \tTotal reward: -115.88615417480469\n",
            " \tTraining loss: 3.9847\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Model saved\n",
            "Episode 1416\n",
            " \tTotal reward: -104.62525939941406\n",
            " \tTraining loss: 7.1229\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1417\n",
            " \tTotal reward: -111.39166259765625\n",
            " \tTraining loss: 3.3594\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1418\n",
            " \tTotal reward: -79.316162109375\n",
            " \tTraining loss: 1.2785\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1419\n",
            " \tTotal reward: -99.80720520019531\n",
            " \tTraining loss: 2.9190\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1420\n",
            " \tTotal reward: -111.86784362792969\n",
            " \tTraining loss: 2.2738\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Model saved\n",
            "Episode 1421\n",
            " \tTotal reward: -109.70553588867188\n",
            " \tTraining loss: 10.1555\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1422\n",
            " \tTotal reward: -93.68209838867188\n",
            " \tTraining loss: 2.2172\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1423\n",
            " \tTotal reward: -112.95048522949219\n",
            " \tTraining loss: 1.5526\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1424\n",
            " \tTotal reward: -101.69325256347656\n",
            " \tTraining loss: 0.8769\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1425\n",
            " \tTotal reward: -95.793212890625\n",
            " \tTraining loss: 4.1055\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Model saved\n",
            "Episode 1426\n",
            " \tTotal reward: -104.86361694335938\n",
            " \tTraining loss: 1.6314\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1427\n",
            " \tTotal reward: -112.59771728515625\n",
            " \tTraining loss: 1.6792\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1428\n",
            " \tTotal reward: -100.9244384765625\n",
            " \tTraining loss: 2.2637\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1429\n",
            " \tTotal reward: -115.25389099121094\n",
            " \tTraining loss: 1.0981\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1430\n",
            " \tTotal reward: -81.91168212890625\n",
            " \tTraining loss: 0.8623\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Model saved\n",
            "Episode 1431\n",
            " \tTotal reward: -115.92707824707031\n",
            " \tTraining loss: 0.6158\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1432\n",
            " \tTotal reward: -91.58232116699219\n",
            " \tTraining loss: 1.3413\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1433\n",
            " \tTotal reward: -106.00691223144531\n",
            " \tTraining loss: 1.3266\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1434\n",
            " \tTotal reward: -103.50093078613281\n",
            " \tTraining loss: 1.4630\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1435\n",
            " \tTotal reward: -103.9881591796875\n",
            " \tTraining loss: 0.7571\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Model saved\n",
            "Episode 1436\n",
            " \tTotal reward: -115.97718811035156\n",
            " \tTraining loss: 1.3621\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1437\n",
            " \tTotal reward: -115.62918090820312\n",
            " \tTraining loss: 3.3904\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1438\n",
            " \tTotal reward: -110.50906372070312\n",
            " \tTraining loss: 1.6299\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1439\n",
            " \tTotal reward: -115.99858093261719\n",
            " \tTraining loss: 0.8263\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1440\n",
            " \tTotal reward: -94.72782897949219\n",
            " \tTraining loss: 4.9198\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Model saved\n",
            "Episode 1441\n",
            " \tTotal reward: -106.96414184570312\n",
            " \tTraining loss: 2.2738\n",
            " \tExplore P: 1.0007\n",
            "\n",
            "Episode 1442\n",
            " \tTotal reward: -106.17495727539062\n",
            " \tTraining loss: 1.5998\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1443\n",
            " \tTotal reward: -101.06657409667969\n",
            " \tTraining loss: 1.5246\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1444\n",
            " \tTotal reward: -100.31727600097656\n",
            " \tTraining loss: 0.9508\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1445\n",
            " \tTotal reward: -104.49761962890625\n",
            " \tTraining loss: 1.7388\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model saved\n",
            "Episode 1446\n",
            " \tTotal reward: -101.08531188964844\n",
            " \tTraining loss: 1.3624\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1447\n",
            " \tTotal reward: -111.55036926269531\n",
            " \tTraining loss: 1.8798\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1448\n",
            " \tTotal reward: -105.556884765625\n",
            " \tTraining loss: 1.2671\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1449\n",
            " \tTotal reward: -108.80718994140625\n",
            " \tTraining loss: 0.9318\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1450\n",
            " \tTotal reward: -115.94618225097656\n",
            " \tTraining loss: 1.0622\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model saved\n",
            "Episode 1451\n",
            " \tTotal reward: -109.55668640136719\n",
            " \tTraining loss: 1.1382\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1452\n",
            " \tTotal reward: -115.99501037597656\n",
            " \tTraining loss: 2.9619\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1453\n",
            " \tTotal reward: -115.97560119628906\n",
            " \tTraining loss: 1.3748\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1454\n",
            " \tTotal reward: -115.94754028320312\n",
            " \tTraining loss: 2.2103\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1455\n",
            " \tTotal reward: -97.9991455078125\n",
            " \tTraining loss: 14.3500\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model saved\n",
            "Episode 1456\n",
            " \tTotal reward: -98.56986999511719\n",
            " \tTraining loss: 1.1411\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1457\n",
            " \tTotal reward: -93.296142578125\n",
            " \tTraining loss: 1.0917\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1458\n",
            " \tTotal reward: -115.93927001953125\n",
            " \tTraining loss: 1.0475\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1459\n",
            " \tTotal reward: -113.05979919433594\n",
            " \tTraining loss: 2.2757\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1460\n",
            " \tTotal reward: -115.19857788085938\n",
            " \tTraining loss: 0.9843\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model saved\n",
            "Episode 1461\n",
            " \tTotal reward: -114.15109252929688\n",
            " \tTraining loss: 1.6526\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1462\n",
            " \tTotal reward: -113.74784851074219\n",
            " \tTraining loss: 0.9101\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1463\n",
            " \tTotal reward: -115.9755859375\n",
            " \tTraining loss: 1.3677\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1464\n",
            " \tTotal reward: -96.82600402832031\n",
            " \tTraining loss: 22.8301\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1465\n",
            " \tTotal reward: -115.85267639160156\n",
            " \tTraining loss: 7.8067\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model saved\n",
            "Episode 1466\n",
            " \tTotal reward: -95.60671997070312\n",
            " \tTraining loss: 0.7818\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1467\n",
            " \tTotal reward: -106.06686401367188\n",
            " \tTraining loss: 0.7825\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1468\n",
            " \tTotal reward: -109.29478454589844\n",
            " \tTraining loss: 0.9971\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1469\n",
            " \tTotal reward: -105.13739013671875\n",
            " \tTraining loss: 2.1358\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1470\n",
            " \tTotal reward: -113.00660705566406\n",
            " \tTraining loss: 0.5827\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model saved\n",
            "Episode 1471\n",
            " \tTotal reward: -107.88450622558594\n",
            " \tTraining loss: 1.5482\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1472\n",
            " \tTotal reward: -109.04251098632812\n",
            " \tTraining loss: 0.9838\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1473\n",
            " \tTotal reward: -115.22642517089844\n",
            " \tTraining loss: 2.7232\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1474\n",
            " \tTotal reward: -108.81779479980469\n",
            " \tTraining loss: 0.9013\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1475\n",
            " \tTotal reward: -106.76976013183594\n",
            " \tTraining loss: 1.0842\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model saved\n",
            "Episode 1476\n",
            " \tTotal reward: -115.00167846679688\n",
            " \tTraining loss: 1.1857\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Model updated\n",
            "Episode 1477\n",
            " \tTotal reward: -113.49057006835938\n",
            " \tTraining loss: 2.9186\n",
            " \tExplore P: 1.0006\n",
            "\n",
            "Episode 1478\n",
            " \tTotal reward: -106.72665405273438\n",
            " \tTraining loss: 3.5578\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1479\n",
            " \tTotal reward: -74.64358520507812\n",
            " \tTraining loss: 1.0553\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1480\n",
            " \tTotal reward: -106.92909240722656\n",
            " \tTraining loss: 6.2334\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1481\n",
            " \tTotal reward: -83.54966735839844\n",
            " \tTraining loss: 1.8942\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1482\n",
            " \tTotal reward: -115.99790954589844\n",
            " \tTraining loss: 2.2107\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1483\n",
            " \tTotal reward: -115.84042358398438\n",
            " \tTraining loss: 3.5372\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1484\n",
            " \tTotal reward: -94.82893371582031\n",
            " \tTraining loss: 1.8223\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1485\n",
            " \tTotal reward: -113.96141052246094\n",
            " \tTraining loss: 2.4091\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1486\n",
            " \tTotal reward: -115.99644470214844\n",
            " \tTraining loss: 1.2035\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1487\n",
            " \tTotal reward: -99.94660949707031\n",
            " \tTraining loss: 2.8829\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1488\n",
            " \tTotal reward: -102.89186096191406\n",
            " \tTraining loss: 3.7520\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1489\n",
            " \tTotal reward: -113.84977722167969\n",
            " \tTraining loss: 2.2833\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1490\n",
            " \tTotal reward: -111.41847229003906\n",
            " \tTraining loss: 1.1113\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1491\n",
            " \tTotal reward: -115.93023681640625\n",
            " \tTraining loss: 1.9289\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1492\n",
            " \tTotal reward: -103.71414184570312\n",
            " \tTraining loss: 1.9878\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1493\n",
            " \tTotal reward: -105.57421875\n",
            " \tTraining loss: 2.9664\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1494\n",
            " \tTotal reward: -109.0189208984375\n",
            " \tTraining loss: 1.6094\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1495\n",
            " \tTotal reward: -113.87007141113281\n",
            " \tTraining loss: 1.7528\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1496\n",
            " \tTotal reward: -103.02676391601562\n",
            " \tTraining loss: 3.9443\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1497\n",
            " \tTotal reward: -109.53770446777344\n",
            " \tTraining loss: 2.3153\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1498\n",
            " \tTotal reward: -106.35272216796875\n",
            " \tTraining loss: 5.1515\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1499\n",
            " \tTotal reward: -93.23640441894531\n",
            " \tTraining loss: 1.0888\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1500\n",
            " \tTotal reward: -107.85740661621094\n",
            " \tTraining loss: 0.8281\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1501\n",
            " \tTotal reward: -104.23727416992188\n",
            " \tTraining loss: 0.9198\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1502\n",
            " \tTotal reward: -64.22613525390625\n",
            " \tTraining loss: 1.5201\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1503\n",
            " \tTotal reward: -108.29820251464844\n",
            " \tTraining loss: 1.4322\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1504\n",
            " \tTotal reward: -95.12124633789062\n",
            " \tTraining loss: 1.4513\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1505\n",
            " \tTotal reward: -110.38275146484375\n",
            " \tTraining loss: 1.5194\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1506\n",
            " \tTotal reward: -108.73634338378906\n",
            " \tTraining loss: 2.3310\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1507\n",
            " \tTotal reward: -73.68623352050781\n",
            " \tTraining loss: 7.7090\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1508\n",
            " \tTotal reward: -109.52886962890625\n",
            " \tTraining loss: 2.5659\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1509\n",
            " \tTotal reward: -115.99444580078125\n",
            " \tTraining loss: 3.0985\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1510\n",
            " \tTotal reward: -101.95378112792969\n",
            " \tTraining loss: 1.6654\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1511\n",
            " \tTotal reward: -97.96197509765625\n",
            " \tTraining loss: 1.4779\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1512\n",
            " \tTotal reward: -112.28401184082031\n",
            " \tTraining loss: 10.3785\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1513\n",
            " \tTotal reward: -113.18612670898438\n",
            " \tTraining loss: 9.8672\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1514\n",
            " \tTotal reward: -115.99937438964844\n",
            " \tTraining loss: 0.9145\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1515\n",
            " \tTotal reward: -88.45919799804688\n",
            " \tTraining loss: 1.2806\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Model saved\n",
            "Episode 1516\n",
            " \tTotal reward: -113.09417724609375\n",
            " \tTraining loss: 1.1125\n",
            " \tExplore P: 1.0005\n",
            "\n",
            "Episode 1517\n",
            " \tTotal reward: -97.34053039550781\n",
            " \tTraining loss: 1.7992\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1518\n",
            " \tTotal reward: -113.89341735839844\n",
            " \tTraining loss: 1.0849\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1519\n",
            " \tTotal reward: -98.74436950683594\n",
            " \tTraining loss: 1.2573\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1520\n",
            " \tTotal reward: -109.45765686035156\n",
            " \tTraining loss: 0.7053\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1521\n",
            " \tTotal reward: -115.97987365722656\n",
            " \tTraining loss: 1.4437\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1522\n",
            " \tTotal reward: -105.76048278808594\n",
            " \tTraining loss: 1.0194\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1523\n",
            " \tTotal reward: -111.85560607910156\n",
            " \tTraining loss: 1.5490\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1524\n",
            " \tTotal reward: -99.64361572265625\n",
            " \tTraining loss: 1.7081\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1525\n",
            " \tTotal reward: -99.35111999511719\n",
            " \tTraining loss: 1.2291\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1526\n",
            " \tTotal reward: -115.94648742675781\n",
            " \tTraining loss: 1.0674\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1527\n",
            " \tTotal reward: -111.706787109375\n",
            " \tTraining loss: 3.2970\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1528\n",
            " \tTotal reward: -101.45623779296875\n",
            " \tTraining loss: 23.8057\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1529\n",
            " \tTotal reward: -112.58546447753906\n",
            " \tTraining loss: 12.2196\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1530\n",
            " \tTotal reward: -98.82159423828125\n",
            " \tTraining loss: 1.0856\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1531\n",
            " \tTotal reward: -101.10050964355469\n",
            " \tTraining loss: 1.0185\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1532\n",
            " \tTotal reward: -108.34039306640625\n",
            " \tTraining loss: 1.6714\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1533\n",
            " \tTotal reward: -112.40802001953125\n",
            " \tTraining loss: 1.3256\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1534\n",
            " \tTotal reward: -113.73703002929688\n",
            " \tTraining loss: 1.1342\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1535\n",
            " \tTotal reward: -74.92742919921875\n",
            " \tTraining loss: 0.9585\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1536\n",
            " \tTotal reward: -113.49075317382812\n",
            " \tTraining loss: 1.8355\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1537\n",
            " \tTotal reward: -106.92276000976562\n",
            " \tTraining loss: 1.4795\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1538\n",
            " \tTotal reward: -115.75643920898438\n",
            " \tTraining loss: 1.0940\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1539\n",
            " \tTotal reward: -113.01530456542969\n",
            " \tTraining loss: 2.5814\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1540\n",
            " \tTotal reward: -100.0631103515625\n",
            " \tTraining loss: 1.5383\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1541\n",
            " \tTotal reward: -110.7789306640625\n",
            " \tTraining loss: 1.2110\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1542\n",
            " \tTotal reward: -104.25796508789062\n",
            " \tTraining loss: 0.8069\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1543\n",
            " \tTotal reward: -73.09162902832031\n",
            " \tTraining loss: 1.1126\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1544\n",
            " \tTotal reward: -100.56939697265625\n",
            " \tTraining loss: 0.9852\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1545\n",
            " \tTotal reward: -77.21934509277344\n",
            " \tTraining loss: 1.6731\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1546\n",
            " \tTotal reward: -113.21456909179688\n",
            " \tTraining loss: 1.0425\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1547\n",
            " \tTotal reward: -100.02243041992188\n",
            " \tTraining loss: 1.0189\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1548\n",
            " \tTotal reward: -115.93252563476562\n",
            " \tTraining loss: 1.7409\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1549\n",
            " \tTotal reward: -108.45970153808594\n",
            " \tTraining loss: 0.8455\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1550\n",
            " \tTotal reward: -108.96171569824219\n",
            " \tTraining loss: 2.7033\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1551\n",
            " \tTotal reward: -105.16105651855469\n",
            " \tTraining loss: 1.4379\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1552\n",
            " \tTotal reward: -115.99784851074219\n",
            " \tTraining loss: 1.6611\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1553\n",
            " \tTotal reward: -110.48677062988281\n",
            " \tTraining loss: 0.9631\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1554\n",
            " \tTotal reward: -89.40182495117188\n",
            " \tTraining loss: 2.9007\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1555\n",
            " \tTotal reward: -108.4412841796875\n",
            " \tTraining loss: 1.7136\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1556\n",
            " \tTotal reward: -114.30143737792969\n",
            " \tTraining loss: 1.0918\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1557\n",
            " \tTotal reward: -115.96305847167969\n",
            " \tTraining loss: 3.8704\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1558\n",
            " \tTotal reward: -112.43795776367188\n",
            " \tTraining loss: 1.3287\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1559\n",
            " \tTotal reward: -115.90730285644531\n",
            " \tTraining loss: 0.9498\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1560\n",
            " \tTotal reward: -110.04965209960938\n",
            " \tTraining loss: 1.3652\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1561\n",
            " \tTotal reward: -107.48548889160156\n",
            " \tTraining loss: 2.5624\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1562\n",
            " \tTotal reward: -88.49153137207031\n",
            " \tTraining loss: 0.9639\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1563\n",
            " \tTotal reward: -93.44013977050781\n",
            " \tTraining loss: 2.4845\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1564\n",
            " \tTotal reward: -90.12709045410156\n",
            " \tTraining loss: 1.9981\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1565\n",
            " \tTotal reward: -115.9757080078125\n",
            " \tTraining loss: 1.9038\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Model saved\n",
            "Episode 1566\n",
            " \tTotal reward: -112.70939636230469\n",
            " \tTraining loss: 1.2561\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1567\n",
            " \tTotal reward: -104.17247009277344\n",
            " \tTraining loss: 0.5867\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1568\n",
            " \tTotal reward: -109.70193481445312\n",
            " \tTraining loss: 1.0665\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1569\n",
            " \tTotal reward: -94.70458984375\n",
            " \tTraining loss: 0.7657\n",
            " \tExplore P: 1.0004\n",
            "\n",
            "Episode 1570\n",
            " \tTotal reward: -109.01786804199219\n",
            " \tTraining loss: 1.5368\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1571\n",
            " \tTotal reward: -113.69442749023438\n",
            " \tTraining loss: 2.9367\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1572\n",
            " \tTotal reward: -115.97560119628906\n",
            " \tTraining loss: 0.8345\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1573\n",
            " \tTotal reward: -114.39620971679688\n",
            " \tTraining loss: 9.4890\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1574\n",
            " \tTotal reward: -111.1588134765625\n",
            " \tTraining loss: 0.9766\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1575\n",
            " \tTotal reward: -106.63645935058594\n",
            " \tTraining loss: 0.8851\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1576\n",
            " \tTotal reward: -115.04380798339844\n",
            " \tTraining loss: 1.1436\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1577\n",
            " \tTotal reward: -108.64967346191406\n",
            " \tTraining loss: 1.5994\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1578\n",
            " \tTotal reward: -115.83518981933594\n",
            " \tTraining loss: 1.3137\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model updated\n",
            "Episode 1579\n",
            " \tTotal reward: -115.98599243164062\n",
            " \tTraining loss: 1.6017\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1580\n",
            " \tTotal reward: -115.21994018554688\n",
            " \tTraining loss: 3.0018\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1581\n",
            " \tTotal reward: -107.87258911132812\n",
            " \tTraining loss: 4.9420\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1582\n",
            " \tTotal reward: -97.59237670898438\n",
            " \tTraining loss: 1.7592\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1583\n",
            " \tTotal reward: -115.88571166992188\n",
            " \tTraining loss: 1.8795\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1584\n",
            " \tTotal reward: -93.48623657226562\n",
            " \tTraining loss: 2.7233\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1585\n",
            " \tTotal reward: -81.87188720703125\n",
            " \tTraining loss: 1.6784\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1586\n",
            " \tTotal reward: -113.88133239746094\n",
            " \tTraining loss: 2.5196\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1587\n",
            " \tTotal reward: -99.4080810546875\n",
            " \tTraining loss: 2.5473\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1588\n",
            " \tTotal reward: -107.39860534667969\n",
            " \tTraining loss: 20.9030\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1589\n",
            " \tTotal reward: -102.83816528320312\n",
            " \tTraining loss: 1.4202\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1590\n",
            " \tTotal reward: -115.84342956542969\n",
            " \tTraining loss: 1.3059\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1591\n",
            " \tTotal reward: -108.75613403320312\n",
            " \tTraining loss: 1.7578\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1592\n",
            " \tTotal reward: -114.22032165527344\n",
            " \tTraining loss: 1.5042\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1593\n",
            " \tTotal reward: -108.28213500976562\n",
            " \tTraining loss: 1.1513\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1594\n",
            " \tTotal reward: -110.35569763183594\n",
            " \tTraining loss: 1.2630\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1595\n",
            " \tTotal reward: -70.3802490234375\n",
            " \tTraining loss: 1.5452\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1596\n",
            " \tTotal reward: -111.98599243164062\n",
            " \tTraining loss: 1.6321\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1597\n",
            " \tTotal reward: -114.53558349609375\n",
            " \tTraining loss: 1.4111\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1598\n",
            " \tTotal reward: -79.96389770507812\n",
            " \tTraining loss: 1.4913\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1599\n",
            " \tTotal reward: -92.88111877441406\n",
            " \tTraining loss: 1.4265\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1600\n",
            " \tTotal reward: -105.89936828613281\n",
            " \tTraining loss: 1.5633\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1601\n",
            " \tTotal reward: -111.26466369628906\n",
            " \tTraining loss: 0.9200\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1602\n",
            " \tTotal reward: -115.97569274902344\n",
            " \tTraining loss: 1.1668\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1603\n",
            " \tTotal reward: -102.53707885742188\n",
            " \tTraining loss: 1.1874\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1604\n",
            " \tTotal reward: -92.17567443847656\n",
            " \tTraining loss: 1.7087\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1605\n",
            " \tTotal reward: -115.98403930664062\n",
            " \tTraining loss: 1.6004\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1606\n",
            " \tTotal reward: -103.589599609375\n",
            " \tTraining loss: 2.5965\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1607\n",
            " \tTotal reward: -112.45184326171875\n",
            " \tTraining loss: 1.2840\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1608\n",
            " \tTotal reward: -105.05699157714844\n",
            " \tTraining loss: 1.8142\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1609\n",
            " \tTotal reward: -114.92025756835938\n",
            " \tTraining loss: 0.8024\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1610\n",
            " \tTotal reward: -110.28314208984375\n",
            " \tTraining loss: 1.0822\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1611\n",
            " \tTotal reward: -115.70404052734375\n",
            " \tTraining loss: 2.1720\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1612\n",
            " \tTotal reward: -111.39546203613281\n",
            " \tTraining loss: 1.3692\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1613\n",
            " \tTotal reward: -99.84709167480469\n",
            " \tTraining loss: 1.6118\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1614\n",
            " \tTotal reward: -115.98066711425781\n",
            " \tTraining loss: 1.0154\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1615\n",
            " \tTotal reward: -115.97715759277344\n",
            " \tTraining loss: 1.3456\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1616\n",
            " \tTotal reward: -76.15182495117188\n",
            " \tTraining loss: 0.8934\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1617\n",
            " \tTotal reward: -83.114501953125\n",
            " \tTraining loss: 2.0425\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1618\n",
            " \tTotal reward: -115.99858093261719\n",
            " \tTraining loss: 12.2372\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1619\n",
            " \tTotal reward: -88.40351867675781\n",
            " \tTraining loss: 1.0927\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1620\n",
            " \tTotal reward: -108.65765380859375\n",
            " \tTraining loss: 1.0496\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1621\n",
            " \tTotal reward: -107.01559448242188\n",
            " \tTraining loss: 5.4406\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1622\n",
            " \tTotal reward: -106.81381225585938\n",
            " \tTraining loss: 1.2471\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1623\n",
            " \tTotal reward: -114.91915893554688\n",
            " \tTraining loss: 1.2867\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1624\n",
            " \tTotal reward: -102.28237915039062\n",
            " \tTraining loss: 1.2692\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1625\n",
            " \tTotal reward: -108.06556701660156\n",
            " \tTraining loss: 1.0792\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1626\n",
            " \tTotal reward: -89.726806640625\n",
            " \tTraining loss: 0.6531\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1627\n",
            " \tTotal reward: -91.24153137207031\n",
            " \tTraining loss: 2.2360\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1628\n",
            " \tTotal reward: -115.12901306152344\n",
            " \tTraining loss: 1.3102\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1629\n",
            " \tTotal reward: -114.93701171875\n",
            " \tTraining loss: 2.1280\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1630\n",
            " \tTotal reward: -108.10040283203125\n",
            " \tTraining loss: 1.1919\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1631\n",
            " \tTotal reward: -86.29278564453125\n",
            " \tTraining loss: 2.0618\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1632\n",
            " \tTotal reward: -115.57177734375\n",
            " \tTraining loss: 1.8840\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1633\n",
            " \tTotal reward: -105.37332153320312\n",
            " \tTraining loss: 1.5453\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1634\n",
            " \tTotal reward: -109.14547729492188\n",
            " \tTraining loss: 1.6511\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1635\n",
            " \tTotal reward: -92.97239685058594\n",
            " \tTraining loss: 1.3871\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1636\n",
            " \tTotal reward: -115.76884460449219\n",
            " \tTraining loss: 1.4789\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1637\n",
            " \tTotal reward: -107.64430236816406\n",
            " \tTraining loss: 2.5121\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1638\n",
            " \tTotal reward: -74.97900390625\n",
            " \tTraining loss: 1.1202\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1639\n",
            " \tTotal reward: -113.6287841796875\n",
            " \tTraining loss: 1.7306\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1640\n",
            " \tTotal reward: -106.47023010253906\n",
            " \tTraining loss: 1.2710\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Model saved\n",
            "Episode 1641\n",
            " \tTotal reward: -100.70286560058594\n",
            " \tTraining loss: 2.2925\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1642\n",
            " \tTotal reward: -110.54193115234375\n",
            " \tTraining loss: 1.0531\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1643\n",
            " \tTotal reward: -111.146728515625\n",
            " \tTraining loss: 1.0628\n",
            " \tExplore P: 1.0003\n",
            "\n",
            "Episode 1644\n",
            " \tTotal reward: -112.35826110839844\n",
            " \tTraining loss: 2.0569\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1645\n",
            " \tTotal reward: -108.2105712890625\n",
            " \tTraining loss: 1.2930\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1646\n",
            " \tTotal reward: -108.5823974609375\n",
            " \tTraining loss: 1.0848\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1647\n",
            " \tTotal reward: -103.38780212402344\n",
            " \tTraining loss: 1.1655\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1648\n",
            " \tTotal reward: -107.6109619140625\n",
            " \tTraining loss: 0.6306\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1649\n",
            " \tTotal reward: -86.36219787597656\n",
            " \tTraining loss: 1.2882\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1650\n",
            " \tTotal reward: -105.90910339355469\n",
            " \tTraining loss: 1.2504\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1651\n",
            " \tTotal reward: -100.96409606933594\n",
            " \tTraining loss: 1.2970\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1652\n",
            " \tTotal reward: -113.83297729492188\n",
            " \tTraining loss: 1.3577\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1653\n",
            " \tTotal reward: -74.74745178222656\n",
            " \tTraining loss: 1.1417\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1654\n",
            " \tTotal reward: -115.21670532226562\n",
            " \tTraining loss: 1.2920\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1655\n",
            " \tTotal reward: -113.45529174804688\n",
            " \tTraining loss: 0.9359\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1656\n",
            " \tTotal reward: -94.46435546875\n",
            " \tTraining loss: 1.5621\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1657\n",
            " \tTotal reward: -99.5347900390625\n",
            " \tTraining loss: 1.3175\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1658\n",
            " \tTotal reward: -115.94630432128906\n",
            " \tTraining loss: 7.6989\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1659\n",
            " \tTotal reward: -96.89007568359375\n",
            " \tTraining loss: 2.7834\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1660\n",
            " \tTotal reward: -105.18513488769531\n",
            " \tTraining loss: 4.2523\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1661\n",
            " \tTotal reward: -103.31951904296875\n",
            " \tTraining loss: 0.7838\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1662\n",
            " \tTotal reward: -108.78436279296875\n",
            " \tTraining loss: 1.4938\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1663\n",
            " \tTotal reward: -104.76641845703125\n",
            " \tTraining loss: 1.1715\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1664\n",
            " \tTotal reward: -112.22113037109375\n",
            " \tTraining loss: 3.5490\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1665\n",
            " \tTotal reward: -112.71360778808594\n",
            " \tTraining loss: 0.7921\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1666\n",
            " \tTotal reward: -108.39739990234375\n",
            " \tTraining loss: 1.5948\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1667\n",
            " \tTotal reward: -113.606201171875\n",
            " \tTraining loss: 0.8171\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1668\n",
            " \tTotal reward: -96.93907165527344\n",
            " \tTraining loss: 1.9314\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1669\n",
            " \tTotal reward: -115.98231506347656\n",
            " \tTraining loss: 1.1838\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1670\n",
            " \tTotal reward: -111.63978576660156\n",
            " \tTraining loss: 1.1037\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1671\n",
            " \tTotal reward: -113.35923767089844\n",
            " \tTraining loss: 2.4140\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1672\n",
            " \tTotal reward: -114.40501403808594\n",
            " \tTraining loss: 1.2556\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1673\n",
            " \tTotal reward: -107.03201293945312\n",
            " \tTraining loss: 1.5234\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1674\n",
            " \tTotal reward: -106.73310852050781\n",
            " \tTraining loss: 1.7712\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1675\n",
            " \tTotal reward: -113.84469604492188\n",
            " \tTraining loss: 1.6203\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1676\n",
            " \tTotal reward: -114.49110412597656\n",
            " \tTraining loss: 1.1341\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1677\n",
            " \tTotal reward: -115.35417175292969\n",
            " \tTraining loss: 0.7643\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1678\n",
            " \tTotal reward: -115.99867248535156\n",
            " \tTraining loss: 1.1649\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1679\n",
            " \tTotal reward: -93.96983337402344\n",
            " \tTraining loss: 6.3993\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1680\n",
            " \tTotal reward: -114.94367980957031\n",
            " \tTraining loss: 0.8825\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1681\n",
            " \tTotal reward: -95.33354187011719\n",
            " \tTraining loss: 6.4645\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1682\n",
            " \tTotal reward: -115.97587585449219\n",
            " \tTraining loss: 1.1879\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1683\n",
            " \tTotal reward: -113.82496643066406\n",
            " \tTraining loss: 1.3055\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1684\n",
            " \tTotal reward: -99.39564514160156\n",
            " \tTraining loss: 0.8192\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1685\n",
            " \tTotal reward: -112.43205261230469\n",
            " \tTraining loss: 0.7808\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1686\n",
            " \tTotal reward: -89.91905212402344\n",
            " \tTraining loss: 1.2809\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1687\n",
            " \tTotal reward: -109.07888793945312\n",
            " \tTraining loss: 1.4528\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1688\n",
            " \tTotal reward: -94.12623596191406\n",
            " \tTraining loss: 0.7729\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model updated\n",
            "Episode 1689\n",
            " \tTotal reward: -115.99856567382812\n",
            " \tTraining loss: 3.3923\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1690\n",
            " \tTotal reward: -101.8863525390625\n",
            " \tTraining loss: 2.2853\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1691\n",
            " \tTotal reward: -115.97563171386719\n",
            " \tTraining loss: 3.1374\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1692\n",
            " \tTotal reward: -104.82984924316406\n",
            " \tTraining loss: 2.8763\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1693\n",
            " \tTotal reward: -110.840576171875\n",
            " \tTraining loss: 4.7152\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1694\n",
            " \tTotal reward: -86.17514038085938\n",
            " \tTraining loss: 3.1606\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1695\n",
            " \tTotal reward: -89.89347839355469\n",
            " \tTraining loss: 10.8614\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1696\n",
            " \tTotal reward: -109.35598754882812\n",
            " \tTraining loss: 2.2919\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1697\n",
            " \tTotal reward: -115.80128479003906\n",
            " \tTraining loss: 5.6297\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1698\n",
            " \tTotal reward: -115.20222473144531\n",
            " \tTraining loss: 14.4731\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1699\n",
            " \tTotal reward: -110.81060791015625\n",
            " \tTraining loss: 4.2019\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1700\n",
            " \tTotal reward: -107.34669494628906\n",
            " \tTraining loss: 2.2825\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1701\n",
            " \tTotal reward: -110.31930541992188\n",
            " \tTraining loss: 2.7108\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1702\n",
            " \tTotal reward: -89.59640502929688\n",
            " \tTraining loss: 1.3300\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1703\n",
            " \tTotal reward: -87.41317749023438\n",
            " \tTraining loss: 7.0909\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1704\n",
            " \tTotal reward: -110.30619812011719\n",
            " \tTraining loss: 1.9401\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1705\n",
            " \tTotal reward: -83.67466735839844\n",
            " \tTraining loss: 14.5050\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1706\n",
            " \tTotal reward: -113.17184448242188\n",
            " \tTraining loss: 2.3001\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1707\n",
            " \tTotal reward: -104.27413940429688\n",
            " \tTraining loss: 18.0189\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1708\n",
            " \tTotal reward: -112.56015014648438\n",
            " \tTraining loss: 2.6026\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1709\n",
            " \tTotal reward: -104.53396606445312\n",
            " \tTraining loss: 1.1650\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1710\n",
            " \tTotal reward: -103.37348937988281\n",
            " \tTraining loss: 3.5114\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1711\n",
            " \tTotal reward: -110.96951293945312\n",
            " \tTraining loss: 1.0416\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1712\n",
            " \tTotal reward: -90.49739074707031\n",
            " \tTraining loss: 2.3421\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1713\n",
            " \tTotal reward: -103.78988647460938\n",
            " \tTraining loss: 3.8105\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1714\n",
            " \tTotal reward: -109.59349060058594\n",
            " \tTraining loss: 1.4551\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1715\n",
            " \tTotal reward: -97.39118957519531\n",
            " \tTraining loss: 13.1321\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1716\n",
            " \tTotal reward: -108.7003173828125\n",
            " \tTraining loss: 1.2108\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1717\n",
            " \tTotal reward: -115.89512634277344\n",
            " \tTraining loss: 1.9187\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1718\n",
            " \tTotal reward: -115.90542602539062\n",
            " \tTraining loss: 1.4055\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1719\n",
            " \tTotal reward: -101.86724853515625\n",
            " \tTraining loss: 0.9111\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1720\n",
            " \tTotal reward: -113.99505615234375\n",
            " \tTraining loss: 6.0272\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1721\n",
            " \tTotal reward: -111.52842712402344\n",
            " \tTraining loss: 1.9794\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1722\n",
            " \tTotal reward: -115.54191589355469\n",
            " \tTraining loss: 2.0787\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1723\n",
            " \tTotal reward: -115.90965270996094\n",
            " \tTraining loss: 1.5221\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1724\n",
            " \tTotal reward: -97.89627075195312\n",
            " \tTraining loss: 0.6957\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1725\n",
            " \tTotal reward: -112.61134338378906\n",
            " \tTraining loss: 0.8855\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1726\n",
            " \tTotal reward: -115.92236328125\n",
            " \tTraining loss: 1.2451\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1727\n",
            " \tTotal reward: -83.33671569824219\n",
            " \tTraining loss: 1.6039\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1728\n",
            " \tTotal reward: -115.85078430175781\n",
            " \tTraining loss: 1.0762\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1729\n",
            " \tTotal reward: -98.92218017578125\n",
            " \tTraining loss: 2.1575\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1730\n",
            " \tTotal reward: -95.24520874023438\n",
            " \tTraining loss: 17.7115\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1731\n",
            " \tTotal reward: -110.83360290527344\n",
            " \tTraining loss: 1.6346\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1732\n",
            " \tTotal reward: -98.42570495605469\n",
            " \tTraining loss: 3.5093\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1733\n",
            " \tTotal reward: -111.52783203125\n",
            " \tTraining loss: 1.1837\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1734\n",
            " \tTotal reward: -102.18972778320312\n",
            " \tTraining loss: 20.5847\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1735\n",
            " \tTotal reward: -109.35328674316406\n",
            " \tTraining loss: 1.4486\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1736\n",
            " \tTotal reward: -83.3741455078125\n",
            " \tTraining loss: 1.3588\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1737\n",
            " \tTotal reward: -86.08226013183594\n",
            " \tTraining loss: 1.6719\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1738\n",
            " \tTotal reward: -111.15869140625\n",
            " \tTraining loss: 1.1822\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1739\n",
            " \tTotal reward: -108.62835693359375\n",
            " \tTraining loss: 1.6098\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1740\n",
            " \tTotal reward: -105.15249633789062\n",
            " \tTraining loss: 1.4141\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1741\n",
            " \tTotal reward: -115.98858642578125\n",
            " \tTraining loss: 1.3472\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1742\n",
            " \tTotal reward: -101.61309814453125\n",
            " \tTraining loss: 1.4932\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1743\n",
            " \tTotal reward: -115.98355102539062\n",
            " \tTraining loss: 1.5267\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1744\n",
            " \tTotal reward: -107.43104553222656\n",
            " \tTraining loss: 1.1395\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1745\n",
            " \tTotal reward: -115.51792907714844\n",
            " \tTraining loss: 1.4481\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Model saved\n",
            "Episode 1746\n",
            " \tTotal reward: -105.41108703613281\n",
            " \tTraining loss: 1.2200\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1747\n",
            " \tTotal reward: -115.93690490722656\n",
            " \tTraining loss: 1.3335\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1748\n",
            " \tTotal reward: -112.42149353027344\n",
            " \tTraining loss: 3.2422\n",
            " \tExplore P: 1.0002\n",
            "\n",
            "Episode 1749\n",
            " \tTotal reward: -98.55661010742188\n",
            " \tTraining loss: 2.5667\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1750\n",
            " \tTotal reward: -96.57020568847656\n",
            " \tTraining loss: 1.4817\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1751\n",
            " \tTotal reward: -115.97560119628906\n",
            " \tTraining loss: 1.2495\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1752\n",
            " \tTotal reward: -115.41935729980469\n",
            " \tTraining loss: 1.0429\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1753\n",
            " \tTotal reward: -103.59109497070312\n",
            " \tTraining loss: 2.5849\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1754\n",
            " \tTotal reward: -113.86219787597656\n",
            " \tTraining loss: 10.7364\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1755\n",
            " \tTotal reward: -102.61068725585938\n",
            " \tTraining loss: 1.1098\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1756\n",
            " \tTotal reward: -111.06137084960938\n",
            " \tTraining loss: 1.9547\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1757\n",
            " \tTotal reward: -75.17324829101562\n",
            " \tTraining loss: 1.3928\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1758\n",
            " \tTotal reward: -101.38702392578125\n",
            " \tTraining loss: 1.3988\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1759\n",
            " \tTotal reward: -69.6280517578125\n",
            " \tTraining loss: 0.9558\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1760\n",
            " \tTotal reward: -100.44160461425781\n",
            " \tTraining loss: 1.2260\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1761\n",
            " \tTotal reward: -115.98161315917969\n",
            " \tTraining loss: 2.7868\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1762\n",
            " \tTotal reward: -104.946533203125\n",
            " \tTraining loss: 1.8835\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1763\n",
            " \tTotal reward: -112.05938720703125\n",
            " \tTraining loss: 1.3006\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1764\n",
            " \tTotal reward: -85.73626708984375\n",
            " \tTraining loss: 3.8035\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1765\n",
            " \tTotal reward: -85.08392333984375\n",
            " \tTraining loss: 1.2029\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1766\n",
            " \tTotal reward: -98.29656982421875\n",
            " \tTraining loss: 2.2941\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1767\n",
            " \tTotal reward: -101.16514587402344\n",
            " \tTraining loss: 0.9352\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1768\n",
            " \tTotal reward: -111.65451049804688\n",
            " \tTraining loss: 2.0015\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1769\n",
            " \tTotal reward: -97.05659484863281\n",
            " \tTraining loss: 1.4694\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1770\n",
            " \tTotal reward: -107.48475646972656\n",
            " \tTraining loss: 1.1133\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1771\n",
            " \tTotal reward: -111.87744140625\n",
            " \tTraining loss: 16.0182\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1772\n",
            " \tTotal reward: -104.4107666015625\n",
            " \tTraining loss: 1.5961\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1773\n",
            " \tTotal reward: -115.29486083984375\n",
            " \tTraining loss: 1.3174\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1774\n",
            " \tTotal reward: -81.29080200195312\n",
            " \tTraining loss: 0.8563\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1775\n",
            " \tTotal reward: -114.50064086914062\n",
            " \tTraining loss: 1.6500\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1776\n",
            " \tTotal reward: -92.91700744628906\n",
            " \tTraining loss: 0.7265\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1777\n",
            " \tTotal reward: -115.80096435546875\n",
            " \tTraining loss: 1.0697\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1778\n",
            " \tTotal reward: -100.49713134765625\n",
            " \tTraining loss: 1.2222\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1779\n",
            " \tTotal reward: -92.74070739746094\n",
            " \tTraining loss: 0.7630\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1780\n",
            " \tTotal reward: -77.55091857910156\n",
            " \tTraining loss: 1.4058\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1781\n",
            " \tTotal reward: -113.34201049804688\n",
            " \tTraining loss: 0.8223\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1782\n",
            " \tTotal reward: -84.73710632324219\n",
            " \tTraining loss: 1.3753\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1783\n",
            " \tTotal reward: -107.80364990234375\n",
            " \tTraining loss: 1.2462\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1784\n",
            " \tTotal reward: -82.94424438476562\n",
            " \tTraining loss: 1.2888\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1785\n",
            " \tTotal reward: -115.9998779296875\n",
            " \tTraining loss: 1.1479\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1786\n",
            " \tTotal reward: -90.57440185546875\n",
            " \tTraining loss: 1.3847\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1787\n",
            " \tTotal reward: -94.34083557128906\n",
            " \tTraining loss: 0.6494\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1788\n",
            " \tTotal reward: -92.30044555664062\n",
            " \tTraining loss: 9.2405\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1789\n",
            " \tTotal reward: -109.07809448242188\n",
            " \tTraining loss: 1.9295\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1790\n",
            " \tTotal reward: -115.90928649902344\n",
            " \tTraining loss: 2.5338\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1791\n",
            " \tTotal reward: -88.16099548339844\n",
            " \tTraining loss: 0.8748\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1792\n",
            " \tTotal reward: -96.30622863769531\n",
            " \tTraining loss: 1.2686\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model updated\n",
            "Episode 1793\n",
            " \tTotal reward: -93.75311279296875\n",
            " \tTraining loss: 10.1523\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1794\n",
            " \tTotal reward: -104.56126403808594\n",
            " \tTraining loss: 4.2194\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1795\n",
            " \tTotal reward: -115.49508666992188\n",
            " \tTraining loss: 9.9762\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1796\n",
            " \tTotal reward: -110.67958068847656\n",
            " \tTraining loss: 1.7012\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1797\n",
            " \tTotal reward: -108.07730102539062\n",
            " \tTraining loss: 4.2295\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1798\n",
            " \tTotal reward: -91.62594604492188\n",
            " \tTraining loss: 2.3709\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1799\n",
            " \tTotal reward: -110.01559448242188\n",
            " \tTraining loss: 1.2872\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1800\n",
            " \tTotal reward: -111.65013122558594\n",
            " \tTraining loss: 3.8284\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1801\n",
            " \tTotal reward: -115.32487487792969\n",
            " \tTraining loss: 2.9823\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1802\n",
            " \tTotal reward: -96.85809326171875\n",
            " \tTraining loss: 1.9500\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1803\n",
            " \tTotal reward: -103.10501098632812\n",
            " \tTraining loss: 3.9084\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1804\n",
            " \tTotal reward: -115.17347717285156\n",
            " \tTraining loss: 3.4203\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1805\n",
            " \tTotal reward: -88.91828918457031\n",
            " \tTraining loss: 2.1974\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1806\n",
            " \tTotal reward: -72.63081359863281\n",
            " \tTraining loss: 1.6295\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1807\n",
            " \tTotal reward: -108.4854736328125\n",
            " \tTraining loss: 2.8044\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1808\n",
            " \tTotal reward: -100.5128173828125\n",
            " \tTraining loss: 1.7973\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1809\n",
            " \tTotal reward: -109.96571350097656\n",
            " \tTraining loss: 1.3260\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1810\n",
            " \tTotal reward: -108.74029541015625\n",
            " \tTraining loss: 1.6960\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1811\n",
            " \tTotal reward: -83.87396240234375\n",
            " \tTraining loss: 1.5978\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1812\n",
            " \tTotal reward: -96.84713745117188\n",
            " \tTraining loss: 1.2538\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1813\n",
            " \tTotal reward: -106.25520324707031\n",
            " \tTraining loss: 1.1730\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1814\n",
            " \tTotal reward: -100.24075317382812\n",
            " \tTraining loss: 3.2668\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1815\n",
            " \tTotal reward: -115.9971923828125\n",
            " \tTraining loss: 1.3038\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1816\n",
            " \tTotal reward: -100.01455688476562\n",
            " \tTraining loss: 1.6154\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1817\n",
            " \tTotal reward: -90.17440795898438\n",
            " \tTraining loss: 1.3642\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1818\n",
            " \tTotal reward: -115.81617736816406\n",
            " \tTraining loss: 1.6173\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1819\n",
            " \tTotal reward: -91.11024475097656\n",
            " \tTraining loss: 1.3446\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1820\n",
            " \tTotal reward: -106.97477722167969\n",
            " \tTraining loss: 1.6387\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1821\n",
            " \tTotal reward: -108.10293579101562\n",
            " \tTraining loss: 2.0579\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1822\n",
            " \tTotal reward: -115.98258972167969\n",
            " \tTraining loss: 1.5518\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1823\n",
            " \tTotal reward: -115.80085754394531\n",
            " \tTraining loss: 1.6621\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1824\n",
            " \tTotal reward: -111.93864440917969\n",
            " \tTraining loss: 1.5003\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1825\n",
            " \tTotal reward: -99.00288391113281\n",
            " \tTraining loss: 1.0096\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1826\n",
            " \tTotal reward: -13.508834838867188\n",
            " \tTraining loss: 0.9413\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1827\n",
            " \tTotal reward: -94.14973449707031\n",
            " \tTraining loss: 1.5589\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1828\n",
            " \tTotal reward: -105.19430541992188\n",
            " \tTraining loss: 11.6318\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1829\n",
            " \tTotal reward: -112.61709594726562\n",
            " \tTraining loss: 0.7358\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1830\n",
            " \tTotal reward: -97.93687438964844\n",
            " \tTraining loss: 1.7869\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1831\n",
            " \tTotal reward: -112.87251281738281\n",
            " \tTraining loss: 1.6551\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1832\n",
            " \tTotal reward: -101.08528137207031\n",
            " \tTraining loss: 1.5384\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1833\n",
            " \tTotal reward: -91.67948913574219\n",
            " \tTraining loss: 1.0074\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1834\n",
            " \tTotal reward: -115.94427490234375\n",
            " \tTraining loss: 0.8908\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1835\n",
            " \tTotal reward: -107.90480041503906\n",
            " \tTraining loss: 2.8591\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1836\n",
            " \tTotal reward: -115.49978637695312\n",
            " \tTraining loss: 1.1414\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1837\n",
            " \tTotal reward: -102.37092590332031\n",
            " \tTraining loss: 0.6531\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1838\n",
            " \tTotal reward: -113.5765380859375\n",
            " \tTraining loss: 1.3890\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1839\n",
            " \tTotal reward: -112.23197937011719\n",
            " \tTraining loss: 1.3514\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1840\n",
            " \tTotal reward: -82.57209777832031\n",
            " \tTraining loss: 3.6141\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1841\n",
            " \tTotal reward: -99.13987731933594\n",
            " \tTraining loss: 1.1793\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1842\n",
            " \tTotal reward: -110.05502319335938\n",
            " \tTraining loss: 0.8909\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1843\n",
            " \tTotal reward: -106.07220458984375\n",
            " \tTraining loss: 0.7953\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1844\n",
            " \tTotal reward: -74.93801879882812\n",
            " \tTraining loss: 1.4691\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1845\n",
            " \tTotal reward: -86.44223022460938\n",
            " \tTraining loss: 0.8194\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1846\n",
            " \tTotal reward: -115.81208801269531\n",
            " \tTraining loss: 0.8728\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1847\n",
            " \tTotal reward: -115.979736328125\n",
            " \tTraining loss: 0.8884\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1848\n",
            " \tTotal reward: -115.17085266113281\n",
            " \tTraining loss: 1.2530\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1849\n",
            " \tTotal reward: -82.37942504882812\n",
            " \tTraining loss: 4.1709\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1850\n",
            " \tTotal reward: -99.46737670898438\n",
            " \tTraining loss: 0.9847\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1851\n",
            " \tTotal reward: -103.67251586914062\n",
            " \tTraining loss: 1.2848\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1852\n",
            " \tTotal reward: -115.93011474609375\n",
            " \tTraining loss: 1.0444\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1853\n",
            " \tTotal reward: -115.98207092285156\n",
            " \tTraining loss: 1.6151\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1854\n",
            " \tTotal reward: -115.97569274902344\n",
            " \tTraining loss: 0.9518\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1855\n",
            " \tTotal reward: -110.28427124023438\n",
            " \tTraining loss: 1.3119\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1856\n",
            " \tTotal reward: -107.98262023925781\n",
            " \tTraining loss: 2.5869\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1857\n",
            " \tTotal reward: -100.61418151855469\n",
            " \tTraining loss: 3.2738\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1858\n",
            " \tTotal reward: -102.7681884765625\n",
            " \tTraining loss: 1.8638\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1859\n",
            " \tTotal reward: -100.01535034179688\n",
            " \tTraining loss: 1.4946\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1860\n",
            " \tTotal reward: -94.8685302734375\n",
            " \tTraining loss: 1.2358\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1861\n",
            " \tTotal reward: -96.33241271972656\n",
            " \tTraining loss: 0.8511\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1862\n",
            " \tTotal reward: -114.92109680175781\n",
            " \tTraining loss: 1.0188\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1863\n",
            " \tTotal reward: -108.79371643066406\n",
            " \tTraining loss: 3.3215\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1864\n",
            " \tTotal reward: -111.67585754394531\n",
            " \tTraining loss: 1.2035\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1865\n",
            " \tTotal reward: -105.69142150878906\n",
            " \tTraining loss: 1.2008\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1866\n",
            " \tTotal reward: -94.05816650390625\n",
            " \tTraining loss: 2.3031\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1867\n",
            " \tTotal reward: -103.49720764160156\n",
            " \tTraining loss: 0.9046\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1868\n",
            " \tTotal reward: -115.38175964355469\n",
            " \tTraining loss: 1.2225\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1869\n",
            " \tTotal reward: -99.33961486816406\n",
            " \tTraining loss: 0.7230\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1870\n",
            " \tTotal reward: -110.03681945800781\n",
            " \tTraining loss: 1.5208\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1871\n",
            " \tTotal reward: -114.20353698730469\n",
            " \tTraining loss: 1.6199\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1872\n",
            " \tTotal reward: -115.97273254394531\n",
            " \tTraining loss: 2.5220\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1873\n",
            " \tTotal reward: -98.70906066894531\n",
            " \tTraining loss: 0.8069\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1874\n",
            " \tTotal reward: -99.98199462890625\n",
            " \tTraining loss: 1.3884\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1875\n",
            " \tTotal reward: -91.99794006347656\n",
            " \tTraining loss: 1.1861\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1876\n",
            " \tTotal reward: -105.16696166992188\n",
            " \tTraining loss: 2.6673\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1877\n",
            " \tTotal reward: -97.90560913085938\n",
            " \tTraining loss: 0.6360\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1878\n",
            " \tTotal reward: -113.87501525878906\n",
            " \tTraining loss: 1.3825\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1879\n",
            " \tTotal reward: -101.55625915527344\n",
            " \tTraining loss: 1.6612\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1880\n",
            " \tTotal reward: -115.97892761230469\n",
            " \tTraining loss: 1.0415\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1881\n",
            " \tTotal reward: -115.99928283691406\n",
            " \tTraining loss: 2.3258\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1882\n",
            " \tTotal reward: -115.97843933105469\n",
            " \tTraining loss: 16.2015\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1883\n",
            " \tTotal reward: -113.83256530761719\n",
            " \tTraining loss: 6.5344\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1884\n",
            " \tTotal reward: -94.10643005371094\n",
            " \tTraining loss: 1.0317\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model updated\n",
            "Episode 1885\n",
            " \tTotal reward: -110.81973266601562\n",
            " \tTraining loss: 20.4307\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1886\n",
            " \tTotal reward: -84.04315185546875\n",
            " \tTraining loss: 7.7333\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1887\n",
            " \tTotal reward: -112.02713012695312\n",
            " \tTraining loss: 1.9019\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1888\n",
            " \tTotal reward: -100.31852722167969\n",
            " \tTraining loss: 7.8895\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1889\n",
            " \tTotal reward: -82.82521057128906\n",
            " \tTraining loss: 1.7131\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1890\n",
            " \tTotal reward: -92.75283813476562\n",
            " \tTraining loss: 3.7433\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1891\n",
            " \tTotal reward: -111.76229858398438\n",
            " \tTraining loss: 2.2304\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1892\n",
            " \tTotal reward: -97.28695678710938\n",
            " \tTraining loss: 1.0871\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1893\n",
            " \tTotal reward: -109.95928955078125\n",
            " \tTraining loss: 2.3411\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1894\n",
            " \tTotal reward: -95.30073547363281\n",
            " \tTraining loss: 1.9352\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1895\n",
            " \tTotal reward: -114.92283630371094\n",
            " \tTraining loss: 1.4876\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1896\n",
            " \tTotal reward: -101.76025390625\n",
            " \tTraining loss: 3.2667\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1897\n",
            " \tTotal reward: -115.48471069335938\n",
            " \tTraining loss: 1.4223\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1898\n",
            " \tTotal reward: -102.90853881835938\n",
            " \tTraining loss: 1.4095\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1899\n",
            " \tTotal reward: -83.44381713867188\n",
            " \tTraining loss: 0.9672\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1900\n",
            " \tTotal reward: -97.48175048828125\n",
            " \tTraining loss: 1.0507\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1901\n",
            " \tTotal reward: -106.18881225585938\n",
            " \tTraining loss: 2.4452\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1902\n",
            " \tTotal reward: -101.79151916503906\n",
            " \tTraining loss: 1.8085\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1903\n",
            " \tTotal reward: -56.8189697265625\n",
            " \tTraining loss: 2.0386\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1904\n",
            " \tTotal reward: -112.83099365234375\n",
            " \tTraining loss: 1.6084\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1905\n",
            " \tTotal reward: -115.98031616210938\n",
            " \tTraining loss: 1.0306\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1906\n",
            " \tTotal reward: -81.35110473632812\n",
            " \tTraining loss: 1.2476\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1907\n",
            " \tTotal reward: -106.59956359863281\n",
            " \tTraining loss: 1.0518\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1908\n",
            " \tTotal reward: -84.90655517578125\n",
            " \tTraining loss: 3.0004\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1909\n",
            " \tTotal reward: -108.89837646484375\n",
            " \tTraining loss: 1.7309\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1910\n",
            " \tTotal reward: -115.10993957519531\n",
            " \tTraining loss: 2.1419\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1911\n",
            " \tTotal reward: -78.47047424316406\n",
            " \tTraining loss: 1.7600\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1912\n",
            " \tTotal reward: -95.91012573242188\n",
            " \tTraining loss: 1.5246\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1913\n",
            " \tTotal reward: -108.60687255859375\n",
            " \tTraining loss: 2.2250\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1914\n",
            " \tTotal reward: -114.52049255371094\n",
            " \tTraining loss: 2.0518\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1915\n",
            " \tTotal reward: -114.62399291992188\n",
            " \tTraining loss: 4.7189\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1916\n",
            " \tTotal reward: -104.41130065917969\n",
            " \tTraining loss: 1.1222\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1917\n",
            " \tTotal reward: -111.09355163574219\n",
            " \tTraining loss: 1.5552\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1918\n",
            " \tTotal reward: -94.23757934570312\n",
            " \tTraining loss: 1.9125\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1919\n",
            " \tTotal reward: -113.896484375\n",
            " \tTraining loss: 0.7989\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1920\n",
            " \tTotal reward: -47.41859436035156\n",
            " \tTraining loss: 1.2945\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1921\n",
            " \tTotal reward: -115.92758178710938\n",
            " \tTraining loss: 1.4967\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1922\n",
            " \tTotal reward: -115.08932495117188\n",
            " \tTraining loss: 1.2950\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1923\n",
            " \tTotal reward: -97.9034423828125\n",
            " \tTraining loss: 1.1724\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1924\n",
            " \tTotal reward: -114.57369995117188\n",
            " \tTraining loss: 1.6200\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1925\n",
            " \tTotal reward: -79.72273254394531\n",
            " \tTraining loss: 1.7233\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1926\n",
            " \tTotal reward: -83.59814453125\n",
            " \tTraining loss: 0.8213\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1927\n",
            " \tTotal reward: -97.23014831542969\n",
            " \tTraining loss: 2.3567\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1928\n",
            " \tTotal reward: -115.69656372070312\n",
            " \tTraining loss: 1.4348\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1929\n",
            " \tTotal reward: -114.85133361816406\n",
            " \tTraining loss: 1.1031\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1930\n",
            " \tTotal reward: -80.47767639160156\n",
            " \tTraining loss: 1.5154\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1931\n",
            " \tTotal reward: -115.18051147460938\n",
            " \tTraining loss: 1.6314\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1932\n",
            " \tTotal reward: -115.127197265625\n",
            " \tTraining loss: 1.0514\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1933\n",
            " \tTotal reward: -93.5623779296875\n",
            " \tTraining loss: 1.8293\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1934\n",
            " \tTotal reward: -109.76971435546875\n",
            " \tTraining loss: 0.9006\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1935\n",
            " \tTotal reward: -115.97560119628906\n",
            " \tTraining loss: 1.2401\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1936\n",
            " \tTotal reward: -86.31355285644531\n",
            " \tTraining loss: 1.4518\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1937\n",
            " \tTotal reward: -111.18246459960938\n",
            " \tTraining loss: 20.7265\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1938\n",
            " \tTotal reward: -115.99859619140625\n",
            " \tTraining loss: 0.7096\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1939\n",
            " \tTotal reward: -110.24494934082031\n",
            " \tTraining loss: 1.6969\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1940\n",
            " \tTotal reward: -86.28822326660156\n",
            " \tTraining loss: 0.8536\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1941\n",
            " \tTotal reward: -101.27677917480469\n",
            " \tTraining loss: 3.0163\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1942\n",
            " \tTotal reward: -62.52117919921875\n",
            " \tTraining loss: 0.7952\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1943\n",
            " \tTotal reward: -113.82225036621094\n",
            " \tTraining loss: 1.9021\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1944\n",
            " \tTotal reward: -113.91133117675781\n",
            " \tTraining loss: 1.5097\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1945\n",
            " \tTotal reward: -115.97543334960938\n",
            " \tTraining loss: 2.8284\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1946\n",
            " \tTotal reward: -99.12611389160156\n",
            " \tTraining loss: 1.4230\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1947\n",
            " \tTotal reward: -111.8580322265625\n",
            " \tTraining loss: 1.6169\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1948\n",
            " \tTotal reward: -112.31690979003906\n",
            " \tTraining loss: 0.8172\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1949\n",
            " \tTotal reward: -111.218994140625\n",
            " \tTraining loss: 0.5764\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1950\n",
            " \tTotal reward: -110.88075256347656\n",
            " \tTraining loss: 0.8651\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1951\n",
            " \tTotal reward: -109.27926635742188\n",
            " \tTraining loss: 1.5499\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1952\n",
            " \tTotal reward: -96.64755249023438\n",
            " \tTraining loss: 1.6544\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1953\n",
            " \tTotal reward: -108.42745971679688\n",
            " \tTraining loss: 1.8447\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1954\n",
            " \tTotal reward: -94.77880859375\n",
            " \tTraining loss: 0.9082\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1955\n",
            " \tTotal reward: -114.04493713378906\n",
            " \tTraining loss: 0.9435\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1956\n",
            " \tTotal reward: -113.13597106933594\n",
            " \tTraining loss: 2.3939\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1957\n",
            " \tTotal reward: -103.14387512207031\n",
            " \tTraining loss: 1.6457\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1958\n",
            " \tTotal reward: -108.47872924804688\n",
            " \tTraining loss: 1.2655\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1959\n",
            " \tTotal reward: -107.02346801757812\n",
            " \tTraining loss: 2.1880\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Episode 1960\n",
            " \tTotal reward: -114.24685668945312\n",
            " \tTraining loss: 1.9460\n",
            " \tExplore P: 1.0001\n",
            "\n",
            "Model saved\n",
            "Episode 1961\n",
            " \tTotal reward: -105.21200561523438\n",
            " \tTraining loss: 0.5966\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1962\n",
            " \tTotal reward: -115.5872802734375\n",
            " \tTraining loss: 1.1292\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1963\n",
            " \tTotal reward: -99.48373413085938\n",
            " \tTraining loss: 1.6819\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1964\n",
            " \tTotal reward: -107.82756042480469\n",
            " \tTraining loss: 2.6320\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1965\n",
            " \tTotal reward: -112.18492126464844\n",
            " \tTraining loss: 1.0675\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 1966\n",
            " \tTotal reward: -101.46781921386719\n",
            " \tTraining loss: 1.6950\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1967\n",
            " \tTotal reward: -113.40887451171875\n",
            " \tTraining loss: 1.8202\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1968\n",
            " \tTotal reward: -109.97955322265625\n",
            " \tTraining loss: 2.7296\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1969\n",
            " \tTotal reward: -102.10417175292969\n",
            " \tTraining loss: 0.8270\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1970\n",
            " \tTotal reward: -105.99287414550781\n",
            " \tTraining loss: 0.9080\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 1971\n",
            " \tTotal reward: -115.99716186523438\n",
            " \tTraining loss: 1.7294\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1972\n",
            " \tTotal reward: -88.77976989746094\n",
            " \tTraining loss: 3.6020\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1973\n",
            " \tTotal reward: -108.0634765625\n",
            " \tTraining loss: 2.2390\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1974\n",
            " \tTotal reward: -97.2698974609375\n",
            " \tTraining loss: 1.6132\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1975\n",
            " \tTotal reward: -89.62066650390625\n",
            " \tTraining loss: 1.0396\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 1976\n",
            " \tTotal reward: -107.32810974121094\n",
            " \tTraining loss: 2.7096\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1977\n",
            " \tTotal reward: -87.7891845703125\n",
            " \tTraining loss: 1.1474\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1978\n",
            " \tTotal reward: -111.91986083984375\n",
            " \tTraining loss: 0.9664\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1979\n",
            " \tTotal reward: -86.73585510253906\n",
            " \tTraining loss: 1.2117\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model updated\n",
            "Episode 1980\n",
            " \tTotal reward: -100.20901489257812\n",
            " \tTraining loss: 1.9419\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 1981\n",
            " \tTotal reward: -115.80322265625\n",
            " \tTraining loss: 2.3069\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1982\n",
            " \tTotal reward: -109.62265014648438\n",
            " \tTraining loss: 1.7444\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1983\n",
            " \tTotal reward: -115.95817565917969\n",
            " \tTraining loss: 2.4819\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1984\n",
            " \tTotal reward: -103.37081909179688\n",
            " \tTraining loss: 2.3024\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1985\n",
            " \tTotal reward: -97.6204833984375\n",
            " \tTraining loss: 3.0887\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 1986\n",
            " \tTotal reward: -104.60487365722656\n",
            " \tTraining loss: 3.7871\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1987\n",
            " \tTotal reward: -111.65812683105469\n",
            " \tTraining loss: 1.5536\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1988\n",
            " \tTotal reward: -115.99571228027344\n",
            " \tTraining loss: 2.0331\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1989\n",
            " \tTotal reward: -112.33695983886719\n",
            " \tTraining loss: 1.6805\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1990\n",
            " \tTotal reward: -115.99501037597656\n",
            " \tTraining loss: 2.5938\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 1991\n",
            " \tTotal reward: -104.38130187988281\n",
            " \tTraining loss: 1.2061\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1992\n",
            " \tTotal reward: -106.33412170410156\n",
            " \tTraining loss: 1.5243\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1993\n",
            " \tTotal reward: -113.50331115722656\n",
            " \tTraining loss: 1.8834\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1994\n",
            " \tTotal reward: -107.53761291503906\n",
            " \tTraining loss: 2.0104\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1995\n",
            " \tTotal reward: -100.14956665039062\n",
            " \tTraining loss: 1.0702\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 1996\n",
            " \tTotal reward: -115.40695190429688\n",
            " \tTraining loss: 1.0022\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1997\n",
            " \tTotal reward: -110.05613708496094\n",
            " \tTraining loss: 3.2377\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1998\n",
            " \tTotal reward: -97.2947998046875\n",
            " \tTraining loss: 2.1432\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 1999\n",
            " \tTotal reward: -115.84675598144531\n",
            " \tTraining loss: 2.1915\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2000\n",
            " \tTotal reward: -107.07661437988281\n",
            " \tTraining loss: 0.9339\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2001\n",
            " \tTotal reward: -98.78668212890625\n",
            " \tTraining loss: 1.1765\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2002\n",
            " \tTotal reward: -111.62564086914062\n",
            " \tTraining loss: 3.5341\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2003\n",
            " \tTotal reward: -97.87713623046875\n",
            " \tTraining loss: 1.5406\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2004\n",
            " \tTotal reward: -111.91862487792969\n",
            " \tTraining loss: 1.0579\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2005\n",
            " \tTotal reward: -109.97711181640625\n",
            " \tTraining loss: 2.3769\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2006\n",
            " \tTotal reward: -115.16830444335938\n",
            " \tTraining loss: 1.3298\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2007\n",
            " \tTotal reward: -93.14143371582031\n",
            " \tTraining loss: 1.2952\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2008\n",
            " \tTotal reward: -91.21670532226562\n",
            " \tTraining loss: 0.9392\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2009\n",
            " \tTotal reward: -112.14082336425781\n",
            " \tTraining loss: 1.6380\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2010\n",
            " \tTotal reward: -106.33839416503906\n",
            " \tTraining loss: 2.1831\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2011\n",
            " \tTotal reward: -115.73171997070312\n",
            " \tTraining loss: 1.0655\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2012\n",
            " \tTotal reward: -112.86439514160156\n",
            " \tTraining loss: 2.6731\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2013\n",
            " \tTotal reward: -115.99105834960938\n",
            " \tTraining loss: 1.1400\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2014\n",
            " \tTotal reward: -112.28395080566406\n",
            " \tTraining loss: 1.6884\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2015\n",
            " \tTotal reward: -114.34536743164062\n",
            " \tTraining loss: 4.6866\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2016\n",
            " \tTotal reward: -98.46969604492188\n",
            " \tTraining loss: 2.6856\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2017\n",
            " \tTotal reward: -48.06074523925781\n",
            " \tTraining loss: 0.8446\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2018\n",
            " \tTotal reward: -107.787353515625\n",
            " \tTraining loss: 1.1632\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2019\n",
            " \tTotal reward: -106.16934204101562\n",
            " \tTraining loss: 1.1519\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2020\n",
            " \tTotal reward: -113.3372802734375\n",
            " \tTraining loss: 1.1042\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2021\n",
            " \tTotal reward: -114.612060546875\n",
            " \tTraining loss: 1.5315\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2022\n",
            " \tTotal reward: -115.86575317382812\n",
            " \tTraining loss: 1.1642\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2023\n",
            " \tTotal reward: -114.20072937011719\n",
            " \tTraining loss: 1.4066\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2024\n",
            " \tTotal reward: -114.47789001464844\n",
            " \tTraining loss: 2.1268\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2025\n",
            " \tTotal reward: -115.7711181640625\n",
            " \tTraining loss: 1.1221\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2026\n",
            " \tTotal reward: -114.73185729980469\n",
            " \tTraining loss: 1.2820\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2027\n",
            " \tTotal reward: -80.12742614746094\n",
            " \tTraining loss: 1.3776\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2028\n",
            " \tTotal reward: -109.71060180664062\n",
            " \tTraining loss: 0.6821\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2029\n",
            " \tTotal reward: -115.22416687011719\n",
            " \tTraining loss: 35.0745\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2030\n",
            " \tTotal reward: -106.12089538574219\n",
            " \tTraining loss: 0.9067\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2031\n",
            " \tTotal reward: -103.3360595703125\n",
            " \tTraining loss: 2.1034\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2032\n",
            " \tTotal reward: -90.88908386230469\n",
            " \tTraining loss: 1.5390\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2033\n",
            " \tTotal reward: -110.2679443359375\n",
            " \tTraining loss: 1.7659\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2034\n",
            " \tTotal reward: -95.16903686523438\n",
            " \tTraining loss: 1.2981\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2035\n",
            " \tTotal reward: -115.97589111328125\n",
            " \tTraining loss: 1.2091\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2036\n",
            " \tTotal reward: -107.65641784667969\n",
            " \tTraining loss: 1.6705\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2037\n",
            " \tTotal reward: -108.22100830078125\n",
            " \tTraining loss: 0.9292\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2038\n",
            " \tTotal reward: -93.86247253417969\n",
            " \tTraining loss: 0.8420\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2039\n",
            " \tTotal reward: -110.86688232421875\n",
            " \tTraining loss: 2.3155\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2040\n",
            " \tTotal reward: -112.62370300292969\n",
            " \tTraining loss: 1.2928\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Model saved\n",
            "Episode 2041\n",
            " \tTotal reward: -103.5858154296875\n",
            " \tTraining loss: 1.7812\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2042\n",
            " \tTotal reward: -112.19049072265625\n",
            " \tTraining loss: 3.4959\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2043\n",
            " \tTotal reward: -100.34031677246094\n",
            " \tTraining loss: 1.4375\n",
            " \tExplore P: 1.0000\n",
            "\n",
            "Episode 2044\n",
            " \tTotal reward: -114.56752014160156\n",
            " \tTraining loss: 1.5611\n",
            " \tExplore P: 1.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu73uMMSzoRU",
        "colab_type": "raw"
      },
      "source": [
        "Episode: 0 Total reward: -89.26519775390625 Training loss: 0.6859 Explore P: 0.9919\n",
        "Model Saved\n",
        "Episode: 1 Total reward: -115.13249206542969 Training loss: 1.0044 Explore P: 0.9872\n",
        "Episode: 2 Total reward: -88.6678466796875 Training loss: 30.3196 Explore P: 0.9760\n",
        "Episode: 3 Total reward: -79.75584411621094 Training loss: 0.4285 Explore P: 0.9687\n",
        "Episode: 4 Total reward: -112.888916015625 Training loss: 17.6260 Explore P: 0.9616\n",
        "Episode: 5 Total reward: -72.01809692382812 Training loss: 0.3325 Explore P: 0.9566\n",
        "Model Saved\n",
        "Episode: 6 Total reward: -91.27947998046875 Training loss: 11.2775 Explore P: 0.9489\n",
        "Episode: 7 Total reward: -96.70150756835938 Training loss: 0.8339 Explore P: 0.9412\n",
        "Episode: 8 Total reward: -89.98007202148438 Training loss: 10.4413 Explore P: 0.9368\n",
        "Episode: 9 Total reward: -78.67872619628906 Training loss: 13.1729 Explore P: 0.9292\n",
        "Episode: 10 Total reward: -114.67742919921875 Training loss: 30.3574 Explore P: 0.9192\n",
        "Model Saved\n",
        "Episode: 11 Total reward: -96.40922546386719 Training loss: 1.3338 Explore P: 0.9128\n",
        "Episode: 12 Total reward: -55.94306945800781 Training loss: 1.0281 Explore P: 0.9030\n",
        "Episode: 13 Total reward: -90.61083984375 Training loss: 17.3349 Explore P: 0.8908\n",
        "Episode: 14 Total reward: -110.54817199707031 Training loss: 11.5495 Explore P: 0.8845\n",
        "Episode: 15 Total reward: -86.7437744140625 Training loss: 8.5952 Explore P: 0.8773\n",
        "Model Saved\n",
        "Episode: 16 Total reward: -108.89964294433594 Training loss: 1.6884 Explore P: 0.8730\n",
        "Episode: 17 Total reward: -59.06085205078125 Training loss: 1.4325 Explore P: 0.8659\n",
        "Episode: 18 Total reward: -102.50300598144531 Training loss: 1.0175 Explore P: 0.8593\n",
        "Episode: 19 Total reward: -100.752685546875 Training loss: 7.5007 Explore P: 0.8556\n",
        "Episode: 20 Total reward: -111.81524658203125 Training loss: 5.0652 Explore P: 0.8487\n",
        "Model Saved\n",
        "Episode: 21 Total reward: -104.21478271484375 Training loss: 1.5522 Explore P: 0.8395\n",
        "Episode: 22 Total reward: -112.78564453125 Training loss: 5.5151 Explore P: 0.8359\n",
        "Episode: 23 Total reward: -82.05340576171875 Training loss: 1.2886 Explore P: 0.8292\n",
        "Episode: 24 Total reward: -111.15492248535156 Training loss: 0.5443 Explore P: 0.8225\n",
        "Episode: 25 Total reward: -73.76707458496094 Training loss: 1.5218 Explore P: 0.8156\n",
        "Model Saved\n",
        "Episode: 26 Total reward: -93.35487365722656 Training loss: 0.9257 Explore P: 0.8088\n",
        "Episode: 27 Total reward: -104.47758483886719 Training loss: 2.1761 Explore P: 0.8023\n",
        "Episode: 28 Total reward: -81.60890197753906 Training loss: 0.5991 Explore P: 0.7958\n",
        "Episode: 29 Total reward: -100.63589477539062 Training loss: 1.7395 Explore P: 0.7894\n",
        "Episode: 30 Total reward: -88.62884521484375 Training loss: 8.3556 Explore P: 0.7830\n",
        "Model Saved\n",
        "Episode: 31 Total reward: -105.23612976074219 Training loss: 0.8298 Explore P: 0.7767\n",
        "Episode: 32 Total reward: -111.5128173828125 Training loss: 0.6878 Explore P: 0.7711\n",
        "Episode: 33 Total reward: -107.63644409179688 Training loss: 0.7860 Explore P: 0.7651\n",
        "Episode: 34 Total reward: -99.78999328613281 Training loss: 1.9388 Explore P: 0.7567\n",
        "Episode: 35 Total reward: -107.68731689453125 Training loss: 3.5948 Explore P: 0.7481\n",
        "Model Saved\n",
        "Episode: 36 Total reward: -112.137451171875 Training loss: 0.4563 Explore P: 0.7421\n",
        "Episode: 37 Total reward: -50.57890319824219 Training loss: 0.5308 Explore P: 0.7361\n",
        "Episode: 38 Total reward: -73.00382995605469 Training loss: 1.9759 Explore P: 0.7302\n",
        "Episode: 39 Total reward: -80.82208251953125 Training loss: 0.2969 Explore P: 0.7243\n",
        "Episode: 40 Total reward: -97.41578674316406 Training loss: 16.1484 Explore P: 0.7185\n",
        "Model Saved\n",
        "Episode: 41 Total reward: -77.568115234375 Training loss: 0.2420 Explore P: 0.7128\n",
        "Episode: 42 Total reward: -103.93637084960938 Training loss: 0.1838 Explore P: 0.7026\n",
        "Episode: 43 Total reward: -81.61286926269531 Training loss: 0.3259 Explore P: 0.6948\n",
        "Episode: 44 Total reward: -91.02716064453125 Training loss: 0.3337 Explore P: 0.6859\n",
        "Episode: 45 Total reward: -98.70729064941406 Training loss: 2.1673 Explore P: 0.6804\n",
        "Model Saved\n",
        "Episode: 46 Total reward: -115.98574829101562 Training loss: 14.9863 Explore P: 0.6726\n",
        "Episode: 47 Total reward: -100.81024169921875 Training loss: 2.0342 Explore P: 0.6654\n",
        "Episode: 48 Total reward: -60.25152587890625 Training loss: 0.2753 Explore P: 0.6569\n",
        "Episode: 49 Total reward: -67.41098022460938 Training loss: 0.3018 Explore P: 0.6486\n",
        "Episode: 50 Total reward: -105.46267700195312 Training loss: 1.0995 Explore P: 0.6413\n",
        "Model Saved\n",
        "Episode: 51 Total reward: -73.07460021972656 Training loss: 0.1813 Explore P: 0.6362\n",
        "Episode: 52 Total reward: -96.30844116210938 Training loss: 0.2939 Explore P: 0.6310\n",
        "Episode: 53 Total reward: -94.21073913574219 Training loss: 0.4776 Explore P: 0.6284\n",
        "Episode: 54 Total reward: -65.328125 Training loss: 0.2104 Explore P: 0.6233\n",
        "Episode: 55 Total reward: -66.21479797363281 Training loss: 3.2012 Explore P: 0.6183\n",
        "Model Saved\n",
        "Episode: 56 Total reward: -94.83515930175781 Training loss: 0.5179 Explore P: 0.6136\n",
        "Episode: 57 Total reward: -92.63566589355469 Training loss: 7.6108 Explore P: 0.6068\n",
        "Episode: 58 Total reward: -114.22836303710938 Training loss: 0.1981 Explore P: 0.5979\n",
        "Episode: 59 Total reward: -109.301025390625 Training loss: 0.1633 Explore P: 0.5931\n",
        "Episode: 60 Total reward: -69.18382263183594 Training loss: 0.3027 Explore P: 0.5883\n",
        "Model Saved\n",
        "Episode: 61 Total reward: -96.5882568359375 Training loss: 0.2388 Explore P: 0.5856\n",
        "Episode: 62 Total reward: -115.95585632324219 Training loss: 0.2598 Explore P: 0.5815\n",
        "Episode: 63 Total reward: -91.42893981933594 Training loss: 3.1792 Explore P: 0.5768\n",
        "Episode: 64 Total reward: -78.47196960449219 Training loss: 0.1737 Explore P: 0.5722\n",
        "Episode: 65 Total reward: -33.51860046386719 Training loss: 16.5782 Explore P: 0.5676\n",
        "Model Saved\n",
        "Episode: 66 Total reward: -52.46026611328125 Training loss: 0.7277 Explore P: 0.5630\n",
        "Episode: 67 Total reward: -104.60054016113281 Training loss: 0.1622 Explore P: 0.5585\n",
        "Episode: 68 Total reward: -77.99497985839844 Training loss: 2.5138 Explore P: 0.5540\n",
        "Episode: 69 Total reward: -54.47041320800781 Training loss: 0.1590 Explore P: 0.5496\n",
        "Episode: 70 Total reward: -63.22991943359375 Training loss: 0.1965 Explore P: 0.5452\n",
        "Model Saved\n",
        "Episode: 71 Total reward: -87.78546142578125 Training loss: 0.3122 Explore P: 0.5375\n",
        "Episode: 72 Total reward: -96.14764404296875 Training loss: 0.1515 Explore P: 0.5351\n",
        "Episode: 73 Total reward: -69.32623291015625 Training loss: 2.8430 Explore P: 0.5308\n",
        "Episode: 74 Total reward: -13.840484619140625 Training loss: 0.2721 Explore P: 0.5266\n",
        "Episode: 75 Total reward: -89.6734619140625 Training loss: 0.1506 Explore P: 0.5213\n",
        "Model Saved\n",
        "Episode: 76 Total reward: -64.53419494628906 Training loss: 1.8367 Explore P: 0.5171\n",
        "Episode: 77 Total reward: -106.41300964355469 Training loss: 0.3183 Explore P: 0.5072\n",
        "Episode: 78 Total reward: -50.4837646484375 Training loss: 0.2255 Explore P: 0.5033\n",
        "Episode: 79 Total reward: -34.91241455078125 Training loss: 0.1923 Explore P: 0.4976\n",
        "Episode: 80 Total reward: -115.21119689941406 Training loss: 0.1336 Explore P: 0.4950\n",
        "Model Saved\n",
        "Episode: 81 Total reward: -73.21771240234375 Training loss: 0.1376 Explore P: 0.4911\n",
        "Episode: 82 Total reward: -62.74360656738281 Training loss: 0.6687 Explore P: 0.4871\n",
        "Episode: 83 Total reward: -15.30194091796875 Training loss: 0.1503 Explore P: 0.4778\n",
        "Episode: 84 Total reward: -74.79470825195312 Training loss: 0.1727 Explore P: 0.4740\n",
        "Episode: 85 Total reward: -54.167205810546875 Training loss: 0.1432 Explore P: 0.4702\n",
        "Model Saved\n",
        "Episode: 86 Total reward: -62.83433532714844 Training loss: 0.1632 Explore P: 0.4665\n",
        "Episode: 87 Total reward: -82.97991943359375 Training loss: 0.1923 Explore P: 0.4644\n",
        "Episode: 88 Total reward: -72.07733154296875 Training loss: 0.2274 Explore P: 0.4607\n",
        "Episode: 89 Total reward: -55.19401550292969 Training loss: 0.1261 Explore P: 0.4570\n",
        "Episode: 90 Total reward: -76.98689270019531 Training loss: 0.7601 Explore P: 0.4505\n",
        "Model Saved\n",
        "Episode: 91 Total reward: -65.32528686523438 Training loss: 0.3138 Explore P: 0.4469\n",
        "Episode: 92 Total reward: -50.588714599609375 Training loss: 0.2203 Explore P: 0.4435\n",
        "Episode: 93 Total reward: -70.39730834960938 Training loss: 1.2486 Explore P: 0.4415\n",
        "\n",
        "Episode: 94 Total reward: 70.74258422851562 Training loss: 0.4045 Explore P: 0.4366\n",
        "Episode: 95 Total reward: -11.190460205078125 Training loss: 0.2244 Explore P: 0.4331\n",
        "Model Saved\n",
        "Episode: 96 Total reward: -22.803070068359375 Training loss: 0.4332 Explore P: 0.4297\n",
        "Episode: 97 Total reward: -43.600616455078125 Training loss: 2.4079 Explore P: 0.4265\n",
        "Episode: 98 Total reward: -74.661376953125 Training loss: 0.3113 Explore P: 0.4246\n",
        "Episode: 99 Total reward: -32.23060607910156 Training loss: 0.1899 Explore P: 0.4212\n",
        "Episode: 100 Total reward: -66.32485961914062 Training loss: 0.1400 Explore P: 0.4167\n",
        "Model Saved\n",
        "Episode: 101 Total reward: -15.644882202148438 Training loss: 0.0826 Explore P: 0.4134\n",
        "Episode: 102 Total reward: 44.1182861328125 Training loss: 0.1348 Explore P: 0.4101\n",
        "Episode: 103 Total reward: -61.74578857421875 Training loss: 0.6734 Explore P: 0.4058\n",
        "Episode: 104 Total reward: -87.16415405273438 Training loss: 0.2358 Explore P: 0.4026\n",
        "Episode: 105 Total reward: -90.69143676757812 Training loss: 0.4390 Explore P: 0.3939\n",
        "Model Saved\n",
        "Episode: 106 Total reward: -56.23359680175781 Training loss: 0.1456 Explore P: 0.3908\n",
        "Episode: 107 Total reward: -41.05461120605469 Training loss: 0.9647 Explore P: 0.3877\n",
        "Episode: 108 Total reward: -1.7525482177734375 Training loss: 0.4109 Explore P: 0.3846\n",
        "Episode: 109 Total reward: -37.95100402832031 Training loss: 0.2784 Explore P: 0.3815\n",
        "Episode: 110 Total reward: -71.89024353027344 Training loss: 0.1012 Explore P: 0.3786\n",
        "Model Saved\n",
        "Episode: 111 Total reward: -72.90853881835938 Training loss: 1.4025 Explore P: 0.3756\n",
        "Model updated\n",
        "Episode: 112 Total reward: -56.199127197265625 Training loss: 7.5684 Explore P: 0.3727\n",
        "Episode: 113 Total reward: -77.53300476074219 Training loss: 3.6123 Explore P: 0.3698\n",
        "Episode: 114 Total reward: -50.253692626953125 Training loss: 6.0007 Explore P: 0.3668\n",
        "Episode: 115 Total reward: 18.208023071289062 Training loss: 6.2701 Explore P: 0.3639\n",
        "Model Saved\n",
        "Episode: 116 Total reward: -74.686767578125 Training loss: 7.9382 Explore P: 0.3610\n",
        "Episode: 117 Total reward: -76.70317077636719 Training loss: 3.9754 Explore P: 0.3593\n",
        "Episode: 118 Total reward: 18.843551635742188 Training loss: 1.0298 Explore P: 0.3554\n",
        "Episode: 119 Total reward: 1.3499298095703125 Training loss: 1.5573 Explore P: 0.3525\n",
        "Episode: 120 Total reward: -0.566131591796875 Training loss: 0.4084 Explore P: 0.3497\n",
        "Model Saved\n",
        "Episode: 121 Total reward: 20.053070068359375 Training loss: 0.6762 Explore P: 0.3470\n",
        "Episode: 122 Total reward: -79.74948120117188 Training loss: 0.5085 Explore P: 0.3443\n",
        "Episode: 123 Total reward: -68.07794189453125 Training loss: 0.6844 Explore P: 0.3416\n",
        "Episode: 124 Total reward: 20.166915893554688 Training loss: 0.2775 Explore P: 0.3389\n",
        "Episode: 125 Total reward: -87.4755859375 Training loss: 0.3127 Explore P: 0.3364\n",
        "Model Saved\n",
        "Episode: 126 Total reward: -17.0537109375 Training loss: 0.3796 Explore P: 0.3337\n",
        "Episode: 127 Total reward: 5.201812744140625 Training loss: 0.6150 Explore P: 0.3311\n",
        "Episode: 128 Total reward: -32.572784423828125 Training loss: 0.2595 Explore P: 0.3285\n",
        "Episode: 129 Total reward: -43.18853759765625 Training loss: 0.4992 Explore P: 0.3259\n",
        "Episode: 130 Total reward: -84.01849365234375 Training loss: 0.3338 Explore P: 0.3226\n",
        "Model Saved\n",
        "Episode: 131 Total reward: -99.23286437988281 Training loss: 1.2294 Explore P: 0.3200\n",
        "Episode: 132 Total reward: -27.938064575195312 Training loss: 0.9042 Explore P: 0.3175\n",
        "Episode: 133 Total reward: 2.96868896484375 Training loss: 0.3110 Explore P: 0.3151\n",
        "Episode: 134 Total reward: -49.97503662109375 Training loss: 0.4291 Explore P: 0.3119\n",
        "Episode: 135 Total reward: 8.848037719726562 Training loss: 0.9113 Explore P: 0.3095\n",
        "Model Saved\n",
        "Episode: 136 Total reward: -78.30146789550781 Training loss: 1.1113 Explore P: 0.3064\n",
        "Episode: 137 Total reward: -35.61848449707031 Training loss: 0.2758 Explore P: 0.3039\n",
        "Episode: 138 Total reward: -80.23164367675781 Training loss: 1.1325 Explore P: 0.3015\n",
        "Episode: 139 Total reward: -41.44696044921875 Training loss: 0.2293 Explore P: 0.2993\n",
        "Episode: 140 Total reward: -63.55998229980469 Training loss: 0.5988 Explore P: 0.2969\n",
        "Model Saved\n",
        "Episode: 141 Total reward: -74.58718872070312 Training loss: 0.3622 Explore P: 0.2956\n",
        "Episode: 142 Total reward: -44.1854248046875 Training loss: 0.8818 Explore P: 0.2933\n",
        "Episode: 143 Total reward: -43.17417907714844 Training loss: 0.6441 Explore P: 0.2918\n",
        "Episode: 144 Total reward: -35.05082702636719 Training loss: 0.1932 Explore P: 0.2885\n",
        "Episode: 145 Total reward: 2.6080322265625 Training loss: 0.2974 Explore P: 0.2857\n",
        "Model Saved\n",
        "Episode: 146 Total reward: -75.66334533691406 Training loss: 0.2797 Explore P: 0.2828\n",
        "Episode: 147 Total reward: -79.89767456054688 Training loss: 14.5457 Explore P: 0.2805\n",
        "Episode: 148 Total reward: -65.21456909179688 Training loss: 0.7638 Explore P: 0.2783\n",
        "Episode: 149 Total reward: 13.195510864257812 Training loss: 0.3936 Explore P: 0.2761\n",
        "Episode: 150 Total reward: 60.77146911621094 Training loss: 1.1485 Explore P: 0.2739\n",
        "Model Saved\n",
        "Episode: 151 Total reward: -67.01502990722656 Training loss: 1.1541 Explore P: 0.2710\n",
        "Episode: 152 Total reward: 7.119903564453125 Training loss: 0.4257 Explore P: 0.2689\n",
        "Episode: 153 Total reward: 13.754486083984375 Training loss: 0.4931 Explore P: 0.2639\n",
        "Episode: 154 Total reward: -67.7314453125 Training loss: 0.5301 Explore P: 0.2618\n",
        "Episode: 155 Total reward: -61.25654602050781 Training loss: 0.3877 Explore P: 0.2599\n",
        "Model Saved\n",
        "Episode: 156 Total reward: -1.2131805419921875 Training loss: 0.3397 Explore P: 0.2579\n",
        "Episode: 157 Total reward: -26.2254638671875 Training loss: 0.1870 Explore P: 0.2558\n",
        "Episode: 158 Total reward: 71.63455200195312 Training loss: 0.3283 Explore P: 0.2538\n",
        "Episode: 159 Total reward: -41.72747802734375 Training loss: 0.6035 Explore P: 0.2520\n",
        "Episode: 160 Total reward: -75.83839416503906 Training loss: 0.5253 Explore P: 0.2488\n",
        "Model Saved\n",
        "Episode: 161 Total reward: 3.0420074462890625 Training loss: 0.8875 Explore P: 0.2468\n",
        "Episode: 162 Total reward: -21.011383056640625 Training loss: 0.2739 Explore P: 0.2449\n",
        "Episode: 163 Total reward: -19.587127685546875 Training loss: 1.2479 Explore P: 0.2431\n",
        "Episode: 164 Total reward: -53.40458679199219 Training loss: 1.2350 Explore P: 0.2413\n",
        "Episode: 165 Total reward: -59.686767578125 Training loss: 0.4527 Explore P: 0.2395\n",
        "Model Saved\n",
        "Episode: 166 Total reward: -53.43865966796875 Training loss: 12.8202 Explore P: 0.2384\n",
        "Episode: 167 Total reward: 4.73968505859375 Training loss: 0.2532 Explore P: 0.2366\n",
        "Episode: 168 Total reward: -42.3804931640625 Training loss: 0.6826 Explore P: 0.2347\n",
        "Episode: 169 Total reward: -1.4572296142578125 Training loss: 0.5197 Explore P: 0.2329\n",
        "Episode: 170 Total reward: -39.27558898925781 Training loss: 11.5407 Explore P: 0.2311\n",
        "Model Saved\n",
        "Episode: 171 Total reward: 8.362579345703125 Training loss: 0.2713 Explore P: 0.2295\n",
        "Episode: 172 Total reward: 14.519943237304688 Training loss: 7.7963 Explore P: 0.2277\n",
        "Episode: 173 Total reward: -58.884429931640625 Training loss: 0.3072 Explore P: 0.2259\n",
        "Episode: 174 Total reward: -93.07179260253906 Training loss: 0.6735 Explore P: 0.2235\n",
        "Episode: 175 Total reward: -60.440277099609375 Training loss: 0.6426 Explore P: 0.2218\n",
        "Model Saved\n",
        "Episode: 176 Total reward: 24.163375854492188 Training loss: 0.5932 Explore P: 0.2201\n",
        "Episode: 177 Total reward: -74.15121459960938 Training loss: 0.1940 Explore P: 0.2191\n",
        "Episode: 178 Total reward: -47.54103088378906 Training loss: 0.9826 Explore P: 0.2174\n",
        "Episode: 179 Total reward: -88.96371459960938 Training loss: 0.6407 Explore P: 0.2157\n",
        "Episode: 180 Total reward: 86.02571105957031 Training loss: 0.3157 Explore P: 0.2134\n",
        "Model Saved\n",
        "Episode: 181 Total reward: -8.269500732421875 Training loss: 1.0492 Explore P: 0.2118\n",
        "Episode: 182 Total reward: 37.916839599609375 Training loss: 0.3531 Explore P: 0.2102\n",
        "Episode: 183 Total reward: 28.824462890625 Training loss: 0.3685 Explore P: 0.2086\n",
        "Episode: 184 Total reward: -103.504150390625 Training loss: 0.8678 Explore P: 0.2077\n",
        "Episode: 185 Total reward: -33.638336181640625 Training loss: 0.5436 Explore P: 0.2062\n",
        "Model Saved\n",
        "Episode: 186 Total reward: -46.80809020996094 Training loss: 0.8421 Explore P: 0.2046\n",
        "\n",
        "Episode: 187 Total reward: 4.5064849853515625 Training loss: 0.2865 Explore P: 0.2030\n",
        "Episode: 188 Total reward: -10.029891967773438 Training loss: 0.4644 Explore P: 0.2014\n",
        "Episode: 189 Total reward: -35.31138610839844 Training loss: 0.3323 Explore P: 0.1999\n",
        "Episode: 190 Total reward: 22.30352783203125 Training loss: 0.6971 Explore P: 0.1984\n",
        "Model Saved\n",
        "Episode: 191 Total reward: -54.252655029296875 Training loss: 0.7283 Explore P: 0.1968\n",
        "Episode: 192 Total reward: -94.67848205566406 Training loss: 1.4658 Explore P: 0.1959\n",
        "Episode: 193 Total reward: -38.33479309082031 Training loss: 0.2945 Explore P: 0.1944\n",
        "Episode: 194 Total reward: -96.05851745605469 Training loss: 0.2530 Explore P: 0.1929\n",
        "Episode: 195 Total reward: -16.951339721679688 Training loss: 0.8220 Explore P: 0.1914\n",
        "Model Saved\n",
        "Episode: 196 Total reward: -104.72447204589844 Training loss: 0.4501 Explore P: 0.1900\n",
        "Episode: 197 Total reward: -3.453094482421875 Training loss: 0.4974 Explore P: 0.1886\n",
        "Episode: 198 Total reward: -26.187362670898438 Training loss: 0.2195 Explore P: 0.1872\n",
        "Episode: 199 Total reward: -98.55648803710938 Training loss: 0.2501 Explore P: 0.1864\n",
        "Episode: 200 Total reward: -16.166595458984375 Training loss: 0.3163 Explore P: 0.1850\n",
        "Model Saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAhPzuV0zoRb",
        "colab_type": "text"
      },
      "source": [
        "## Step 9: Watch our Agent play üëÄ\n",
        "Now that we trained our agent, we can test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xmGQICzzoRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}