{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole: REINFORCE Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent <b>that plays Cartpole </b>\n",
    "\n",
    "<img src=\"http://neuro-educator.com/wp-content/uploads/2017/09/DQN.gif\" alt=\"Cartpole gif\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Policy gradients [Article](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/81/2924b60e2befc192c7e43279ff388c68104515228b865506f13b4f18c338/gym-0.17.0.tar.gz (1.6MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.6MB 13.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gym) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gym) (1.16.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gym) (1.11.0)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.0MB 45.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle~=1.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/0b/189cd3c19faf362ff2df5f301456c6cf8571ef6684644cfdfdbff293825c/cloudpickle-1.3.0-py2.py3-none-any.whl\n",
      "Collecting future\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829kB 51.7MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.0-cp36-none-any.whl size=1648739 sha256=0cca182ea4de7f047b88de3066551fc61acfb94f8057922b16becbc821f09583\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/eb/9b/33/416e7f0b999c7136a464bb6e71c41d8208b43bf63fc01bee39\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491095 sha256=d7d831c2ddaa5c29a030db3b610dfff7367a0e2ad68b5c725ba108bc7f8c3009\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, pyglet, cloudpickle, gym\n",
      "  Found existing installation: cloudpickle 0.5.3\n",
      "    Uninstalling cloudpickle-0.5.3:\n",
      "      Successfully uninstalled cloudpickle-0.5.3\n",
      "Successfully installed cloudpickle-1.3.0 future-0.18.2 gym-0.17.0 pyglet-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "This time we use <a href=\"https://gym.openai.com/\">OpenAI Gym</a> which has a lot of great environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up our hyperparameters ‚öóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## TRAINING\n",
    "max_episodes = 301\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Define the preprocessing functions ‚öôÔ∏è\n",
    "This function takes <b>the rewards and perform discounting.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    \n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "        \n",
    "    #print(discounted_episode_rewards)\n",
    "        \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/Policy%20Gradients/Cartpole/assets/catpole.png\">\n",
    "\n",
    "The idea is simple:\n",
    "- Our state which is an array of 4 values will be used as an input.\n",
    "- Our NN is 3 fully connected layers.\n",
    "- Our output activation function is softmax that squashes the outputs to a probability distribution (for instance if we have 4, 2, 6 --> softmax --> (0.4, 0.2, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name='input_')\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name='actions')\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name='discounted_episode_rewards')\n",
    "    \n",
    "    # Placeholder for variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32, name='mean_reward')\n",
    "    \n",
    "    with tf.name_scope('fc1'):\n",
    "        fc1 = tf.contrib.layers.fully_connected(\n",
    "            inputs = input_,\n",
    "            num_outputs = 10,\n",
    "            activation_fn = tf.nn.relu,\n",
    "            weights_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "        )\n",
    "        \n",
    "    with tf.name_scope('fc2'):\n",
    "        fc2 = tf.contrib.layers.fully_connected(\n",
    "            inputs = fc1,\n",
    "            num_outputs = action_size,\n",
    "            activation_fn = tf.nn.relu,\n",
    "            weights_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "        )\n",
    "        \n",
    "    with tf.name_scope('fc3'):\n",
    "        fc3 = tf.contrib.layers.fully_connected(\n",
    "            inputs = fc2,\n",
    "            num_outputs = action_size,\n",
    "            activation_fn = None,\n",
    "            weights_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "        )\n",
    "        \n",
    "    with tf.name_scope('softmax'):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        # cross-entropy of the selected action (input as 1-hot) \n",
    "        # when the network puts fc3 into the softmax at the end \n",
    "        # (the \"logits\" go into softmax)\n",
    "        # cross-entropy = - log(probability of selected action)\n",
    "        # this will be larger for lower-probability actions\n",
    "        # (intuitively: want to bump up the probabilities of those\n",
    "        # in particular (if the reward was large) ? )\n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits = fc3,\n",
    "            labels = actions,\n",
    "        )\n",
    "        # Reducing this mean drives\n",
    "        # parameters towards larger rewards (because of neg_)\n",
    "        # bigger updates when rewards are bigger\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_)\n",
    "        \n",
    "    with tf.name_scope('train'):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('fw') #'/tensorboard/pg/fw')\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "\n",
    "tf.summary.scalar('Reward_mean', mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create the NN \n",
    "\n",
    "maxReward = 0 # Keep track of maximum reward \n",
    "\n",
    "For episode in range(max_episodes): \n",
    "    episode + 1 \n",
    "    reset environment \n",
    "    reset stores (states, actions, rewards) \n",
    "    \n",
    "    For each step: \n",
    "        Choose action a \n",
    "        Perform action a \n",
    "        Store s, a, r \n",
    "    \n",
    "        If done: \n",
    "            Calculate sum reward \n",
    "            Calculate gamma \n",
    "            Gt Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "Episode:  0\n",
      "Reward:  15.0\n",
      "Mean reward:  15.0\n",
      "Max reward so far:  15.0\n",
      "Model saved\n",
      "============================\n",
      "Episode:  1\n",
      "Reward:  12.0\n",
      "Mean reward:  13.5\n",
      "Max reward so far:  15.0\n",
      "============================\n",
      "Episode:  2\n",
      "Reward:  15.0\n",
      "Mean reward:  14.0\n",
      "Max reward so far:  15.0\n",
      "============================\n",
      "Episode:  3\n",
      "Reward:  14.0\n",
      "Mean reward:  14.0\n",
      "Max reward so far:  15.0\n",
      "============================\n",
      "Episode:  4\n",
      "Reward:  43.0\n",
      "Mean reward:  19.8\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  5\n",
      "Reward:  26.0\n",
      "Mean reward:  20.833333333333332\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  6\n",
      "Reward:  35.0\n",
      "Mean reward:  22.857142857142858\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  7\n",
      "Reward:  25.0\n",
      "Mean reward:  23.125\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  8\n",
      "Reward:  15.0\n",
      "Mean reward:  22.22222222222222\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  9\n",
      "Reward:  22.0\n",
      "Mean reward:  22.2\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  10\n",
      "Reward:  16.0\n",
      "Mean reward:  21.636363636363637\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  11\n",
      "Reward:  43.0\n",
      "Mean reward:  23.416666666666668\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  12\n",
      "Reward:  18.0\n",
      "Mean reward:  23.0\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  13\n",
      "Reward:  12.0\n",
      "Mean reward:  22.214285714285715\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  14\n",
      "Reward:  22.0\n",
      "Mean reward:  22.2\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  15\n",
      "Reward:  21.0\n",
      "Mean reward:  22.125\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  16\n",
      "Reward:  11.0\n",
      "Mean reward:  21.470588235294116\n",
      "Max reward so far:  43.0\n",
      "============================\n",
      "Episode:  17\n",
      "Reward:  64.0\n",
      "Mean reward:  23.833333333333332\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  18\n",
      "Reward:  37.0\n",
      "Mean reward:  24.526315789473685\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  19\n",
      "Reward:  17.0\n",
      "Mean reward:  24.15\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  20\n",
      "Reward:  23.0\n",
      "Mean reward:  24.095238095238095\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  21\n",
      "Reward:  55.0\n",
      "Mean reward:  25.5\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  22\n",
      "Reward:  14.0\n",
      "Mean reward:  25.0\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  23\n",
      "Reward:  15.0\n",
      "Mean reward:  24.583333333333332\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  24\n",
      "Reward:  23.0\n",
      "Mean reward:  24.52\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  25\n",
      "Reward:  26.0\n",
      "Mean reward:  24.576923076923077\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  26\n",
      "Reward:  16.0\n",
      "Mean reward:  24.25925925925926\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  27\n",
      "Reward:  24.0\n",
      "Mean reward:  24.25\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  28\n",
      "Reward:  19.0\n",
      "Mean reward:  24.06896551724138\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  29\n",
      "Reward:  11.0\n",
      "Mean reward:  23.633333333333333\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  30\n",
      "Reward:  44.0\n",
      "Mean reward:  24.29032258064516\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  31\n",
      "Reward:  12.0\n",
      "Mean reward:  23.90625\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  32\n",
      "Reward:  14.0\n",
      "Mean reward:  23.606060606060606\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  33\n",
      "Reward:  57.0\n",
      "Mean reward:  24.58823529411765\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  34\n",
      "Reward:  22.0\n",
      "Mean reward:  24.514285714285716\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  35\n",
      "Reward:  32.0\n",
      "Mean reward:  24.72222222222222\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  36\n",
      "Reward:  39.0\n",
      "Mean reward:  25.10810810810811\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  37\n",
      "Reward:  28.0\n",
      "Mean reward:  25.18421052631579\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  38\n",
      "Reward:  41.0\n",
      "Mean reward:  25.58974358974359\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  39\n",
      "Reward:  28.0\n",
      "Mean reward:  25.65\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  40\n",
      "Reward:  18.0\n",
      "Mean reward:  25.463414634146343\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  41\n",
      "Reward:  30.0\n",
      "Mean reward:  25.571428571428573\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  42\n",
      "Reward:  16.0\n",
      "Mean reward:  25.348837209302324\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  43\n",
      "Reward:  14.0\n",
      "Mean reward:  25.09090909090909\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  44\n",
      "Reward:  31.0\n",
      "Mean reward:  25.22222222222222\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  45\n",
      "Reward:  42.0\n",
      "Mean reward:  25.58695652173913\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  46\n",
      "Reward:  24.0\n",
      "Mean reward:  25.5531914893617\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  47\n",
      "Reward:  15.0\n",
      "Mean reward:  25.333333333333332\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  48\n",
      "Reward:  15.0\n",
      "Mean reward:  25.122448979591837\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  49\n",
      "Reward:  16.0\n",
      "Mean reward:  24.94\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  50\n",
      "Reward:  12.0\n",
      "Mean reward:  24.686274509803923\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  51\n",
      "Reward:  21.0\n",
      "Mean reward:  24.615384615384617\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  52\n",
      "Reward:  27.0\n",
      "Mean reward:  24.660377358490567\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  53\n",
      "Reward:  28.0\n",
      "Mean reward:  24.72222222222222\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  54\n",
      "Reward:  14.0\n",
      "Mean reward:  24.527272727272727\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  55\n",
      "Reward:  12.0\n",
      "Mean reward:  24.303571428571427\n",
      "Max reward so far:  64.0\n",
      "============================\n",
      "Episode:  56\n",
      "Reward:  79.0\n",
      "Mean reward:  25.263157894736842\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  57\n",
      "Reward:  19.0\n",
      "Mean reward:  25.155172413793103\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  58\n",
      "Reward:  12.0\n",
      "Mean reward:  24.93220338983051\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  59\n",
      "Reward:  25.0\n",
      "Mean reward:  24.933333333333334\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  60\n",
      "Reward:  13.0\n",
      "Mean reward:  24.737704918032787\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  61\n",
      "Reward:  13.0\n",
      "Mean reward:  24.548387096774192\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  62\n",
      "Reward:  70.0\n",
      "Mean reward:  25.26984126984127\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  63\n",
      "Reward:  19.0\n",
      "Mean reward:  25.171875\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  64\n",
      "Reward:  31.0\n",
      "Mean reward:  25.26153846153846\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  65\n",
      "Reward:  20.0\n",
      "Mean reward:  25.181818181818183\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  66\n",
      "Reward:  21.0\n",
      "Mean reward:  25.119402985074625\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  67\n",
      "Reward:  28.0\n",
      "Mean reward:  25.16176470588235\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  68\n",
      "Reward:  23.0\n",
      "Mean reward:  25.130434782608695\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  69\n",
      "Reward:  30.0\n",
      "Mean reward:  25.2\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  70\n",
      "Reward:  15.0\n",
      "Mean reward:  25.056338028169016\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  71\n",
      "Reward:  36.0\n",
      "Mean reward:  25.208333333333332\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  72\n",
      "Reward:  25.0\n",
      "Mean reward:  25.205479452054796\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  73\n",
      "Reward:  14.0\n",
      "Mean reward:  25.054054054054053\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  74\n",
      "Reward:  17.0\n",
      "Mean reward:  24.946666666666665\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  75\n",
      "Reward:  16.0\n",
      "Mean reward:  24.82894736842105\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  76\n",
      "Reward:  34.0\n",
      "Mean reward:  24.948051948051948\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  77\n",
      "Reward:  27.0\n",
      "Mean reward:  24.974358974358974\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  78\n",
      "Reward:  22.0\n",
      "Mean reward:  24.936708860759495\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  79\n",
      "Reward:  13.0\n",
      "Mean reward:  24.7875\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  80\n",
      "Reward:  38.0\n",
      "Mean reward:  24.950617283950617\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  81\n",
      "Reward:  31.0\n",
      "Mean reward:  25.024390243902438\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  82\n",
      "Reward:  16.0\n",
      "Mean reward:  24.91566265060241\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  83\n",
      "Reward:  11.0\n",
      "Mean reward:  24.75\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  84\n",
      "Reward:  21.0\n",
      "Mean reward:  24.705882352941178\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  85\n",
      "Reward:  31.0\n",
      "Mean reward:  24.77906976744186\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  86\n",
      "Reward:  26.0\n",
      "Mean reward:  24.79310344827586\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  87\n",
      "Reward:  13.0\n",
      "Mean reward:  24.65909090909091\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  88\n",
      "Reward:  10.0\n",
      "Mean reward:  24.49438202247191\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  89\n",
      "Reward:  12.0\n",
      "Mean reward:  24.355555555555554\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  90\n",
      "Reward:  16.0\n",
      "Mean reward:  24.263736263736263\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  91\n",
      "Reward:  29.0\n",
      "Mean reward:  24.315217391304348\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  92\n",
      "Reward:  29.0\n",
      "Mean reward:  24.365591397849464\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  93\n",
      "Reward:  40.0\n",
      "Mean reward:  24.53191489361702\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  94\n",
      "Reward:  15.0\n",
      "Mean reward:  24.431578947368422\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  95\n",
      "Reward:  65.0\n",
      "Mean reward:  24.854166666666668\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  96\n",
      "Reward:  41.0\n",
      "Mean reward:  25.02061855670103\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  97\n",
      "Reward:  26.0\n",
      "Mean reward:  25.03061224489796\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  98\n",
      "Reward:  65.0\n",
      "Mean reward:  25.434343434343436\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  99\n",
      "Reward:  16.0\n",
      "Mean reward:  25.34\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  100\n",
      "Reward:  20.0\n",
      "Mean reward:  25.287128712871286\n",
      "Max reward so far:  79.0\n",
      "Model saved\n",
      "============================\n",
      "Episode:  101\n",
      "Reward:  13.0\n",
      "Mean reward:  25.166666666666668\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  102\n",
      "Reward:  23.0\n",
      "Mean reward:  25.145631067961165\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  103\n",
      "Reward:  20.0\n",
      "Mean reward:  25.096153846153847\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  104\n",
      "Reward:  21.0\n",
      "Mean reward:  25.057142857142857\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  105\n",
      "Reward:  19.0\n",
      "Mean reward:  25.0\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  106\n",
      "Reward:  19.0\n",
      "Mean reward:  24.94392523364486\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  107\n",
      "Reward:  27.0\n",
      "Mean reward:  24.962962962962962\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  108\n",
      "Reward:  34.0\n",
      "Mean reward:  25.045871559633028\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  109\n",
      "Reward:  18.0\n",
      "Mean reward:  24.98181818181818\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  110\n",
      "Reward:  28.0\n",
      "Mean reward:  25.00900900900901\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  111\n",
      "Reward:  24.0\n",
      "Mean reward:  25.0\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  112\n",
      "Reward:  21.0\n",
      "Mean reward:  24.964601769911503\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  113\n",
      "Reward:  24.0\n",
      "Mean reward:  24.95614035087719\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  114\n",
      "Reward:  41.0\n",
      "Mean reward:  25.095652173913045\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  115\n",
      "Reward:  44.0\n",
      "Mean reward:  25.25862068965517\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  116\n",
      "Reward:  19.0\n",
      "Mean reward:  25.205128205128204\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  117\n",
      "Reward:  30.0\n",
      "Mean reward:  25.24576271186441\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  118\n",
      "Reward:  34.0\n",
      "Mean reward:  25.319327731092436\n",
      "Max reward so far:  79.0\n",
      "============================\n",
      "Episode:  119\n",
      "Reward:  85.0\n",
      "Mean reward:  25.816666666666666\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  120\n",
      "Reward:  42.0\n",
      "Mean reward:  25.950413223140497\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  121\n",
      "Reward:  67.0\n",
      "Mean reward:  26.28688524590164\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  122\n",
      "Reward:  13.0\n",
      "Mean reward:  26.178861788617887\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  123\n",
      "Reward:  78.0\n",
      "Mean reward:  26.596774193548388\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  124\n",
      "Reward:  12.0\n",
      "Mean reward:  26.48\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  125\n",
      "Reward:  36.0\n",
      "Mean reward:  26.555555555555557\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  126\n",
      "Reward:  39.0\n",
      "Mean reward:  26.653543307086615\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  127\n",
      "Reward:  14.0\n",
      "Mean reward:  26.5546875\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  128\n",
      "Reward:  52.0\n",
      "Mean reward:  26.751937984496124\n",
      "Max reward so far:  85.0\n",
      "============================\n",
      "Episode:  129\n",
      "Reward:  110.0\n",
      "Mean reward:  27.392307692307693\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  130\n",
      "Reward:  26.0\n",
      "Mean reward:  27.38167938931298\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  131\n",
      "Reward:  66.0\n",
      "Mean reward:  27.674242424242426\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  132\n",
      "Reward:  15.0\n",
      "Mean reward:  27.57894736842105\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  133\n",
      "Reward:  26.0\n",
      "Mean reward:  27.567164179104477\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  134\n",
      "Reward:  26.0\n",
      "Mean reward:  27.555555555555557\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  135\n",
      "Reward:  29.0\n",
      "Mean reward:  27.566176470588236\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  136\n",
      "Reward:  77.0\n",
      "Mean reward:  27.927007299270073\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  137\n",
      "Reward:  39.0\n",
      "Mean reward:  28.007246376811594\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  138\n",
      "Reward:  84.0\n",
      "Mean reward:  28.41007194244604\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  139\n",
      "Reward:  31.0\n",
      "Mean reward:  28.428571428571427\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  140\n",
      "Reward:  55.0\n",
      "Mean reward:  28.617021276595743\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  141\n",
      "Reward:  45.0\n",
      "Mean reward:  28.732394366197184\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  142\n",
      "Reward:  20.0\n",
      "Mean reward:  28.67132867132867\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  143\n",
      "Reward:  30.0\n",
      "Mean reward:  28.680555555555557\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  144\n",
      "Reward:  26.0\n",
      "Mean reward:  28.662068965517243\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  145\n",
      "Reward:  35.0\n",
      "Mean reward:  28.705479452054796\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  146\n",
      "Reward:  44.0\n",
      "Mean reward:  28.80952380952381\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  147\n",
      "Reward:  30.0\n",
      "Mean reward:  28.81756756756757\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  148\n",
      "Reward:  43.0\n",
      "Mean reward:  28.91275167785235\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  149\n",
      "Reward:  25.0\n",
      "Mean reward:  28.886666666666667\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  150\n",
      "Reward:  41.0\n",
      "Mean reward:  28.966887417218544\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  151\n",
      "Reward:  62.0\n",
      "Mean reward:  29.18421052631579\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  152\n",
      "Reward:  77.0\n",
      "Mean reward:  29.49673202614379\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  153\n",
      "Reward:  14.0\n",
      "Mean reward:  29.396103896103895\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  154\n",
      "Reward:  50.0\n",
      "Mean reward:  29.529032258064515\n",
      "Max reward so far:  110.0\n",
      "============================\n",
      "Episode:  155\n",
      "Reward:  124.0\n",
      "Mean reward:  30.134615384615383\n",
      "Max reward so far:  124.0\n",
      "============================\n",
      "Episode:  156\n",
      "Reward:  55.0\n",
      "Mean reward:  30.29299363057325\n",
      "Max reward so far:  124.0\n",
      "============================\n",
      "Episode:  157\n",
      "Reward:  47.0\n",
      "Mean reward:  30.39873417721519\n",
      "Max reward so far:  124.0\n",
      "============================\n",
      "Episode:  158\n",
      "Reward:  25.0\n",
      "Mean reward:  30.364779874213838\n",
      "Max reward so far:  124.0\n",
      "============================\n",
      "Episode:  159\n",
      "Reward:  62.0\n",
      "Mean reward:  30.5625\n",
      "Max reward so far:  124.0\n",
      "============================\n",
      "Episode:  160\n",
      "Reward:  24.0\n",
      "Mean reward:  30.52173913043478\n",
      "Max reward so far:  124.0\n",
      "============================\n",
      "Episode:  161\n",
      "Reward:  142.0\n",
      "Mean reward:  31.209876543209877\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  162\n",
      "Reward:  98.0\n",
      "Mean reward:  31.619631901840492\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  163\n",
      "Reward:  18.0\n",
      "Mean reward:  31.536585365853657\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  164\n",
      "Reward:  43.0\n",
      "Mean reward:  31.606060606060606\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  165\n",
      "Reward:  58.0\n",
      "Mean reward:  31.765060240963855\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  166\n",
      "Reward:  113.0\n",
      "Mean reward:  32.25149700598802\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  167\n",
      "Reward:  11.0\n",
      "Mean reward:  32.125\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  168\n",
      "Reward:  76.0\n",
      "Mean reward:  32.38461538461539\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  169\n",
      "Reward:  25.0\n",
      "Mean reward:  32.34117647058824\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  170\n",
      "Reward:  111.0\n",
      "Mean reward:  32.801169590643276\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  171\n",
      "Reward:  73.0\n",
      "Mean reward:  33.03488372093023\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  172\n",
      "Reward:  51.0\n",
      "Mean reward:  33.138728323699425\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  173\n",
      "Reward:  13.0\n",
      "Mean reward:  33.02298850574713\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  174\n",
      "Reward:  75.0\n",
      "Mean reward:  33.26285714285714\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  175\n",
      "Reward:  118.0\n",
      "Mean reward:  33.74431818181818\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  176\n",
      "Reward:  24.0\n",
      "Mean reward:  33.68926553672316\n",
      "Max reward so far:  142.0\n",
      "============================\n",
      "Episode:  177\n",
      "Reward:  161.0\n",
      "Mean reward:  34.40449438202247\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  178\n",
      "Reward:  92.0\n",
      "Mean reward:  34.726256983240226\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  179\n",
      "Reward:  62.0\n",
      "Mean reward:  34.87777777777778\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  180\n",
      "Reward:  50.0\n",
      "Mean reward:  34.96132596685083\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  181\n",
      "Reward:  12.0\n",
      "Mean reward:  34.83516483516483\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  182\n",
      "Reward:  30.0\n",
      "Mean reward:  34.80874316939891\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  183\n",
      "Reward:  47.0\n",
      "Mean reward:  34.875\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  184\n",
      "Reward:  31.0\n",
      "Mean reward:  34.85405405405405\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  185\n",
      "Reward:  51.0\n",
      "Mean reward:  34.94086021505376\n",
      "Max reward so far:  161.0\n",
      "============================\n",
      "Episode:  186\n",
      "Reward:  177.0\n",
      "Mean reward:  35.70053475935829\n",
      "Max reward so far:  177.0\n",
      "============================\n",
      "Episode:  187\n",
      "Reward:  50.0\n",
      "Mean reward:  35.776595744680854\n",
      "Max reward so far:  177.0\n",
      "============================\n",
      "Episode:  188\n",
      "Reward:  22.0\n",
      "Mean reward:  35.7037037037037\n",
      "Max reward so far:  177.0\n",
      "============================\n",
      "Episode:  189\n",
      "Reward:  20.0\n",
      "Mean reward:  35.62105263157895\n",
      "Max reward so far:  177.0\n",
      "============================\n",
      "Episode:  190\n",
      "Reward:  254.0\n",
      "Mean reward:  36.76439790575916\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  191\n",
      "Reward:  34.0\n",
      "Mean reward:  36.75\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  192\n",
      "Reward:  153.0\n",
      "Mean reward:  37.35233160621762\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  193\n",
      "Reward:  66.0\n",
      "Mean reward:  37.5\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  194\n",
      "Reward:  15.0\n",
      "Mean reward:  37.38461538461539\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  195\n",
      "Reward:  82.0\n",
      "Mean reward:  37.61224489795919\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  196\n",
      "Reward:  117.0\n",
      "Mean reward:  38.015228426395936\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  197\n",
      "Reward:  92.0\n",
      "Mean reward:  38.28787878787879\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  198\n",
      "Reward:  14.0\n",
      "Mean reward:  38.165829145728644\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  199\n",
      "Reward:  101.0\n",
      "Mean reward:  38.48\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  200\n",
      "Reward:  118.0\n",
      "Mean reward:  38.875621890547265\n",
      "Max reward so far:  254.0\n",
      "Model saved\n",
      "============================\n",
      "Episode:  201\n",
      "Reward:  32.0\n",
      "Mean reward:  38.84158415841584\n",
      "Max reward so far:  254.0\n",
      "============================\n",
      "Episode:  202\n",
      "Reward:  265.0\n",
      "Mean reward:  39.95566502463054\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  203\n",
      "Reward:  238.0\n",
      "Mean reward:  40.9264705882353\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  204\n",
      "Reward:  104.0\n",
      "Mean reward:  41.234146341463415\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  205\n",
      "Reward:  29.0\n",
      "Mean reward:  41.1747572815534\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  206\n",
      "Reward:  30.0\n",
      "Mean reward:  41.1207729468599\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  207\n",
      "Reward:  18.0\n",
      "Mean reward:  41.00961538461539\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  208\n",
      "Reward:  185.0\n",
      "Mean reward:  41.698564593301434\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  209\n",
      "Reward:  66.0\n",
      "Mean reward:  41.81428571428572\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  210\n",
      "Reward:  216.0\n",
      "Mean reward:  42.639810426540286\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  211\n",
      "Reward:  167.0\n",
      "Mean reward:  43.22641509433962\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  212\n",
      "Reward:  190.0\n",
      "Mean reward:  43.91549295774648\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  213\n",
      "Reward:  62.0\n",
      "Mean reward:  44.0\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  214\n",
      "Reward:  69.0\n",
      "Mean reward:  44.116279069767444\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  215\n",
      "Reward:  78.0\n",
      "Mean reward:  44.273148148148145\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  216\n",
      "Reward:  120.0\n",
      "Mean reward:  44.6221198156682\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  217\n",
      "Reward:  153.0\n",
      "Mean reward:  45.11926605504587\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  218\n",
      "Reward:  166.0\n",
      "Mean reward:  45.67123287671233\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  219\n",
      "Reward:  89.0\n",
      "Mean reward:  45.86818181818182\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  220\n",
      "Reward:  113.0\n",
      "Mean reward:  46.171945701357465\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  221\n",
      "Reward:  257.0\n",
      "Mean reward:  47.12162162162162\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  222\n",
      "Reward:  41.0\n",
      "Mean reward:  47.09417040358744\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  223\n",
      "Reward:  160.0\n",
      "Mean reward:  47.598214285714285\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  224\n",
      "Reward:  263.0\n",
      "Mean reward:  48.55555555555556\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  225\n",
      "Reward:  199.0\n",
      "Mean reward:  49.2212389380531\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  226\n",
      "Reward:  180.0\n",
      "Mean reward:  49.797356828193834\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  227\n",
      "Reward:  229.0\n",
      "Mean reward:  50.583333333333336\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  228\n",
      "Reward:  124.0\n",
      "Mean reward:  50.903930131004365\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  229\n",
      "Reward:  28.0\n",
      "Mean reward:  50.80434782608695\n",
      "Max reward so far:  265.0\n",
      "============================\n",
      "Episode:  230\n",
      "Reward:  335.0\n",
      "Mean reward:  52.03463203463203\n",
      "Max reward so far:  335.0\n",
      "============================\n",
      "Episode:  231\n",
      "Reward:  248.0\n",
      "Mean reward:  52.87931034482759\n",
      "Max reward so far:  335.0\n",
      "============================\n",
      "Episode:  232\n",
      "Reward:  563.0\n",
      "Mean reward:  55.06866952789699\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  233\n",
      "Reward:  149.0\n",
      "Mean reward:  55.47008547008547\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  234\n",
      "Reward:  201.0\n",
      "Mean reward:  56.08936170212766\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  235\n",
      "Reward:  159.0\n",
      "Mean reward:  56.52542372881356\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  236\n",
      "Reward:  375.0\n",
      "Mean reward:  57.869198312236286\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  237\n",
      "Reward:  444.0\n",
      "Mean reward:  59.49159663865546\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  238\n",
      "Reward:  307.0\n",
      "Mean reward:  60.52719665271967\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  239\n",
      "Reward:  132.0\n",
      "Mean reward:  60.825\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  240\n",
      "Reward:  134.0\n",
      "Mean reward:  61.128630705394194\n",
      "Max reward so far:  563.0\n",
      "============================\n",
      "Episode:  241\n",
      "Reward:  644.0\n",
      "Mean reward:  63.53719008264463\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  242\n",
      "Reward:  106.0\n",
      "Mean reward:  63.711934156378604\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  243\n",
      "Reward:  272.0\n",
      "Mean reward:  64.56557377049181\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  244\n",
      "Reward:  23.0\n",
      "Mean reward:  64.39591836734694\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  245\n",
      "Reward:  341.0\n",
      "Mean reward:  65.52032520325203\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  246\n",
      "Reward:  302.0\n",
      "Mean reward:  66.47773279352226\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  247\n",
      "Reward:  618.0\n",
      "Mean reward:  68.70161290322581\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  248\n",
      "Reward:  194.0\n",
      "Mean reward:  69.20481927710843\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  249\n",
      "Reward:  628.0\n",
      "Mean reward:  71.44\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  250\n",
      "Reward:  76.0\n",
      "Mean reward:  71.45816733067728\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  251\n",
      "Reward:  221.0\n",
      "Mean reward:  72.0515873015873\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  252\n",
      "Reward:  464.0\n",
      "Mean reward:  73.60079051383399\n",
      "Max reward so far:  644.0\n",
      "============================\n",
      "Episode:  253\n",
      "Reward:  1210.0\n",
      "Mean reward:  78.0748031496063\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  254\n",
      "Reward:  333.0\n",
      "Mean reward:  79.07450980392157\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  255\n",
      "Reward:  702.0\n",
      "Mean reward:  81.5078125\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  256\n",
      "Reward:  303.0\n",
      "Mean reward:  82.36964980544747\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  257\n",
      "Reward:  23.0\n",
      "Mean reward:  82.13953488372093\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  258\n",
      "Reward:  296.0\n",
      "Mean reward:  82.96525096525096\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  259\n",
      "Reward:  220.0\n",
      "Mean reward:  83.49230769230769\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  260\n",
      "Reward:  94.0\n",
      "Mean reward:  83.53256704980843\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  261\n",
      "Reward:  519.0\n",
      "Mean reward:  85.19465648854961\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  262\n",
      "Reward:  358.0\n",
      "Mean reward:  86.2319391634981\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  263\n",
      "Reward:  341.0\n",
      "Mean reward:  87.1969696969697\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  264\n",
      "Reward:  163.0\n",
      "Mean reward:  87.48301886792453\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  265\n",
      "Reward:  154.0\n",
      "Mean reward:  87.73308270676692\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  266\n",
      "Reward:  169.0\n",
      "Mean reward:  88.0374531835206\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  267\n",
      "Reward:  206.0\n",
      "Mean reward:  88.4776119402985\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  268\n",
      "Reward:  180.0\n",
      "Mean reward:  88.817843866171\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  269\n",
      "Reward:  152.0\n",
      "Mean reward:  89.05185185185185\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  270\n",
      "Reward:  121.0\n",
      "Mean reward:  89.16974169741698\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  271\n",
      "Reward:  210.0\n",
      "Mean reward:  89.61397058823529\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  272\n",
      "Reward:  225.0\n",
      "Mean reward:  90.10989010989012\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  273\n",
      "Reward:  142.0\n",
      "Mean reward:  90.2992700729927\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  274\n",
      "Reward:  129.0\n",
      "Mean reward:  90.44\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  275\n",
      "Reward:  141.0\n",
      "Mean reward:  90.6231884057971\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  276\n",
      "Reward:  149.0\n",
      "Mean reward:  90.83393501805054\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  277\n",
      "Reward:  165.0\n",
      "Mean reward:  91.10071942446044\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  278\n",
      "Reward:  124.0\n",
      "Mean reward:  91.21863799283155\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  279\n",
      "Reward:  134.0\n",
      "Mean reward:  91.37142857142857\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  280\n",
      "Reward:  97.0\n",
      "Mean reward:  91.39145907473309\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  281\n",
      "Reward:  187.0\n",
      "Mean reward:  91.73049645390071\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  282\n",
      "Reward:  121.0\n",
      "Mean reward:  91.8339222614841\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  283\n",
      "Reward:  112.0\n",
      "Mean reward:  91.90492957746478\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  284\n",
      "Reward:  171.0\n",
      "Mean reward:  92.18245614035088\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  285\n",
      "Reward:  146.0\n",
      "Mean reward:  92.37062937062937\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  286\n",
      "Reward:  186.0\n",
      "Mean reward:  92.69686411149826\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  287\n",
      "Reward:  179.0\n",
      "Mean reward:  92.99652777777777\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  288\n",
      "Reward:  132.0\n",
      "Mean reward:  93.13148788927336\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  289\n",
      "Reward:  163.0\n",
      "Mean reward:  93.37241379310345\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  290\n",
      "Reward:  136.0\n",
      "Mean reward:  93.51890034364261\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  291\n",
      "Reward:  186.0\n",
      "Mean reward:  93.83561643835617\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  292\n",
      "Reward:  149.0\n",
      "Mean reward:  94.02389078498294\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  293\n",
      "Reward:  118.0\n",
      "Mean reward:  94.10544217687075\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  294\n",
      "Reward:  233.0\n",
      "Mean reward:  94.57627118644068\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  295\n",
      "Reward:  115.0\n",
      "Mean reward:  94.64527027027027\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  296\n",
      "Reward:  100.0\n",
      "Mean reward:  94.66329966329967\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  297\n",
      "Reward:  198.0\n",
      "Mean reward:  95.01006711409396\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  298\n",
      "Reward:  187.0\n",
      "Mean reward:  95.31772575250837\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  299\n",
      "Reward:  122.0\n",
      "Mean reward:  95.40666666666667\n",
      "Max reward so far:  1210.0\n",
      "============================\n",
      "Episode:  300\n",
      "Reward:  101.0\n",
      "Mean reward:  95.42524916943522\n",
      "Max reward so far:  1210.0\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [], [], []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "        \n",
    "        # Launch game\n",
    "        state = env.reset()\n",
    "        \n",
    "        #env.render()\n",
    "        \n",
    "        while True:\n",
    "            # Choose a w/ our stochastic policy\n",
    "            action_probability_distribution = sess.run(\n",
    "                action_distribution, \n",
    "                feed_dict={input_: state.reshape([1,4])}\n",
    "            )\n",
    "            \n",
    "            action = np.random.choice(\n",
    "                range(action_probability_distribution.shape[1]),\n",
    "                p=action_probability_distribution.ravel()\n",
    "            )\n",
    "            \n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "            \n",
    "            # One-hot the action\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print('============================')\n",
    "                print('Episode: ', episode)\n",
    "                print('Reward: ', episode_rewards_sum)\n",
    "                print('Mean reward: ', mean_reward)\n",
    "                print('Max reward so far: ', maximumRewardRecorded)\n",
    "                \n",
    "                # calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run(\n",
    "                    [loss, train_opt],\n",
    "                    feed_dict={\n",
    "                        input_: np.vstack(np.array(episode_states)),\n",
    "                        actions: np.vstack(np.array(episode_actions)),\n",
    "                        discounted_episode_rewards_: discounted_episode_rewards\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Write TF summaries\n",
    "                summary = sess.run(\n",
    "                    write_op,\n",
    "                    feed_dict={\n",
    "                        input_: np.vstack(np.array(episode_states)),\n",
    "                        actions: np.vstack(np.array(episode_actions)),\n",
    "                        discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                        mean_reward_: mean_reward\n",
    "                    }\n",
    "                )\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                # Reset transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [], [], []\n",
    "                \n",
    "                break\n",
    "                \n",
    "            state = new_state\n",
    "            \n",
    "        # Save model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, 'model.ckpt')\n",
    "            print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "****************************\n",
      "EPISODE  0\n",
      "Score  98.0\n",
      "****************************\n",
      "EPISODE  1\n",
      "Score  140.0\n",
      "****************************\n",
      "EPISODE  2\n",
      "Score  235.0\n",
      "****************************\n",
      "EPISODE  3\n",
      "Score  98.0\n",
      "****************************\n",
      "EPISODE  4\n",
      "Score  142.0\n",
      "****************************\n",
      "EPISODE  5\n",
      "Score  145.0\n",
      "****************************\n",
      "EPISODE  6\n",
      "Score  260.0\n",
      "****************************\n",
      "EPISODE  7\n",
      "Score  183.0\n",
      "****************************\n",
      "EPISODE  8\n",
      "Score  131.0\n",
      "****************************\n",
      "EPISODE  9\n",
      "Score  102.0\n",
      "Score over time:  153.4\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, 'model.ckpt')\n",
    "    \n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print('****************************')\n",
    "        print('EPISODE ', episode)\n",
    "        \n",
    "        while True:\n",
    "            action_probability_distribution = sess.run(\n",
    "                action_distribution,\n",
    "                feed_dict={input_: state.reshape([1,4])}\n",
    "            )\n",
    "            action = np.random.choice(\n",
    "                range(action_probability_distribution.shape[1]),\n",
    "                p=action_probability_distribution.ravel()\n",
    "            )\n",
    "            \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print('Score ', total_rewards)\n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            \n",
    "    env.close()\n",
    "    print('Score over time: ', str(sum(rewards)/10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
