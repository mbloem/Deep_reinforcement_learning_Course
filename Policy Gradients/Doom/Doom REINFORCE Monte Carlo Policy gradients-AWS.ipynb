{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doom-Health: REINFORCE Monte Carlo Policy gradients üïπÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent <b>that try to survive in Doom environment by using a Policy Gradient architecture.</b> <br>\n",
    "Our agent playing Doom:\n",
    "\n",
    "<img src=\"assets/projectw4.gif\" style=\"max-width: 600px;\" alt=\"Policy Gradient with Doom\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can follow this notebook with this video tutorial üìπ that will helps you to understand each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wLTQRuizVyE?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Policy gradients [Article](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)\n",
    "- We made a [tutorial video](https://youtu.be/wLTQRuizVyE) where we implement a Policy Gradient agent with Tensorflow that learns to play Doom üëπüî´ in a Deathmatch environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sudo: apt-get: command not found\n",
      "sudo: apt-get: command not found\n",
      "sudo: apt-get: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install deps from \n",
    "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
    "\n",
    "sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
    "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
    "libopenal-dev timidity libwildmidi-dev unzip\n",
    "\n",
    "# Boost libraries\n",
    "sudo apt-get install libboost-all-dev\n",
    "\n",
    "# Lua binding dependencies\n",
    "sudo apt-get install liblua5.1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sudo yum install gcc gcc-c++ make\n",
    "sudo yum install boost-devel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vizdoom\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/6c/23565c09387173423883e7881fce53541ff89b5209ca0904c67e577dd6ac/vizdoom-1.1.7.tar.gz\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from vizdoom) (1.16.4)\n",
      "Building wheels for collected packages: vizdoom\n",
      "  Building wheel for vizdoom (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-rs_cqn3h/vizdoom/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-rs_cqn3h/vizdoom/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-vju0rjq4 --python-tag cp36\n",
      "       cwd: /tmp/pip-install-rs_cqn3h/vizdoom/\n",
      "  Complete output (30 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  CMake Error: No source or binary directory provided\n",
      "  \u001b[1m\n",
      "  Installation failed, you may be missing some dependencies.\n",
      "  Please check https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md for details\n",
      "  \n",
      "  \u001b[0mTraceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-rs_cqn3h/vizdoom/setup.py\", line 119, in <module>\n",
      "      keywords=['vizdoom', 'doom', 'ai', 'deep learning', 'reinforcement learning', 'research']\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/setuptools/__init__.py\", line 145, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/dist.py\", line 955, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/wheel/bdist_wheel.py\", line 202, in run\n",
      "      self.run_command('build')\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/tmp/pip-install-rs_cqn3h/vizdoom/setup.py\", line 81, in run\n",
      "      subprocess.check_call(['cmake'] + cmake_arg_list)\n",
      "    File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/subprocess.py\", line 291, in check_call\n",
      "      raise CalledProcessError(retcode, cmd)\n",
      "  subprocess.CalledProcessError: Command '['cmake', '-DCMAKE_BUILD_TYPE=Release', '-DBUILD_PYTHON=ON', '-DPYTHON_EXECUTABLE=/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python3.6', '-DPYTHON_LIBRARY=/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/libpython3.6m.so', '-DPYTHON_INCLUDE_DIR=/home/ec2-user/anaconda3/envs/tensorflow_p36/include/python3.6m', '-DNUMPY_INCLUDES=/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/include', '-DBUILD_PYTHON3=ON']' returned non-zero exit status 1.\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for vizdoom\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for vizdoom\n",
      "Failed to build vizdoom\n",
      "Installing collected packages: vizdoom\n",
      "    Running setup.py install for vizdoom ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-rs_cqn3h/vizdoom/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-rs_cqn3h/vizdoom/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-_bcc0_5w/install-record.txt --single-version-externally-managed --compile\n",
      "         cwd: /tmp/pip-install-rs_cqn3h/vizdoom/\n",
      "    Complete output (32 lines):\n",
      "    running install\n",
      "    running build\n",
      "    CMake Error: No source or binary directory provided\n",
      "    \u001b[1m\n",
      "    Installation failed, you may be missing some dependencies.\n",
      "    Please check https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md for details\n",
      "    \n",
      "    \u001b[0mTraceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-rs_cqn3h/vizdoom/setup.py\", line 119, in <module>\n",
      "        keywords=['vizdoom', 'doom', 'ai', 'deep learning', 'reinforcement learning', 'research']\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/setuptools/__init__.py\", line 145, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/dist.py\", line 955, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/setuptools/command/install.py\", line 61, in run\n",
      "        return orig.install.run(self)\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/command/install.py\", line 545, in run\n",
      "        self.run_command('build')\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/tmp/pip-install-rs_cqn3h/vizdoom/setup.py\", line 81, in run\n",
      "        subprocess.check_call(['cmake'] + cmake_arg_list)\n",
      "      File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/subprocess.py\", line 291, in check_call\n",
      "        raise CalledProcessError(retcode, cmd)\n",
      "    subprocess.CalledProcessError: Command '['cmake', '-DCMAKE_BUILD_TYPE=Release', '-DBUILD_PYTHON=ON', '-DPYTHON_EXECUTABLE=/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python3.6', '-DPYTHON_LIBRARY=/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/libpython3.6m.so', '-DPYTHON_INCLUDE_DIR=/home/ec2-user/anaconda3/envs/tensorflow_p36/include/python3.6m', '-DNUMPY_INCLUDES=/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/include', '-DBUILD_PYTHON3=ON']' returned non-zero exit status 1.\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-rs_cqn3h/vizdoom/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-rs_cqn3h/vizdoom/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-_bcc0_5w/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'vizdoom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2e181d663650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m      \u001b[0;31m# Deep Learning library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m           \u001b[0;31m# Handle matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvizdoom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m        \u001b[0;31m# Doom Environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m                \u001b[0;31m# Handling random number generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m                  \u001b[0;31m# Handling time calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'vizdoom'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/health_doom.jpg\" style=\"max-width:500px;\" alt=\"Doom health\"/>\n",
    "\n",
    "The purpose of this scenario is to teach the agent **how to survive without knowing what makes him survive.** Agent know only that life is precious and death is bad so he must learn what prolongs his existence and that his health is connected with it.\n",
    "\n",
    "Map is a rectangle with green, acidic floor which hurts the player periodically. Initially there are some medkits spread uniformly over the map. A new medkit falls from the skies every now and then. **Medkits heal some portions of player's health - to survive agent needs to pick them up. Episode finishes after player's death or on timeout.**\n",
    "\n",
    "Further configuration:\n",
    "\n",
    "- living_reward = 1\n",
    "- 3 available buttons: turn left, turn right, move forward\n",
    "- 1 available game variable: HEALTH\n",
    "- death penalty = 100\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame üñºÔ∏è\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discount_and_normalize_rewards üí∞\n",
    "This function is important, because we are in a Monte Carlo situation. <br>\n",
    "\n",
    "We need to **discount the rewards at the end of the episode**. This function takes, the reward discount it, and **then normalize them** (to avoid a big variability in rewards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note: Policy gradient methods like reinforce **are on-policy method which can not be updated from experience replay.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/doomPG.png\" alt=\"Doom PG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create batches.<br>\n",
    "These batches contains episodes **(their number depends on how many rewards we collect**: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\n",
    "<br>\n",
    "* Make a batch\n",
    "    * For each step:\n",
    "        * Choose action a\n",
    "        * Perform action a\n",
    "        * Store s, a, r\n",
    "        * **If** done:\n",
    "            * Calculate sum reward\n",
    "            * Calculate gamma Gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the Neural Network\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* maxReward = 0 # Keep track of maximum reward\n",
    "* **For** epochs in range(num_epochs):\n",
    "    * Get batches\n",
    "    * Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
